{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Machine Translation\n",
    "\n",
    "Welcome to your first programming assignment for this week! \n",
    "\n",
    "You will build a Neural Machine Translation (NMT) model to translate human readable dates (\"25th of June, 2009\") into machine readable dates (\"2009-06-25\"). You will do this using an attention model, one of the most sophisticated sequence to sequence models. \n",
    "\n",
    "This notebook was produced together with NVIDIA's Deep Learning Institute. \n",
    "\n",
    "Let's load all the packages you will need for this assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 适用于 tensorflow < 2.0 , 此时 tensorflow 和 keras 是分开的  \n",
    "\n",
    "from keras.layers import Bidirectional, Concatenate, Permute, Dot, Input, CuDNNLSTM, Multiply\n",
    "from keras.layers import RepeatVector, Dense, Activation,Lambda,Softmax,Reshape\n",
    "\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import load_model, Model\n",
    "import keras.backend as K\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "from faker import Faker\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from babel.dates import format_date\n",
    "from nmt_utils import *\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  适用于 tensorflow >= 2.0 keras 被直接集成到 tensorflow 的内部\n",
    "#  ref: https://keras.io/about/\n",
    "\n",
    "from tensorflow.keras.layers import Bidirectional, Concatenate, Permute, Dot, Input, LSTM, Multiply\n",
    "from tensorflow.keras.layers import RepeatVector, Dense, Activation,Lambda,Softmax,Reshape\n",
    "\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import load_model, Model\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "from faker import Faker\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from babel.dates import format_date\n",
    "from lib.nmt_utils import *\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  - Translating human readable dates into machine readable dates\n",
    "\n",
    "The model you will build here could be used to translate from one language to another, such as translating from English to Hindi. However, language translation requires massive datasets and usually takes days of training on GPUs. To give you a place to experiment with these models even without using massive datasets, we will instead use a simpler \"date translation\" task. \n",
    "\n",
    "The network will input a date written in a variety of possible formats (*e.g. \"the 29th of August 1958\", \"03/30/1968\", \"24 JUNE 1987\"*) and translate them into standardized, machine readable dates (*e.g. \"1958-08-29\", \"1968-03-30\", \"1987-06-24\"*). We will have the network learn to output dates in the common machine-readable format YYYY-MM-DD. \n",
    "\n",
    "\n",
    "\n",
    "<!-- \n",
    "Take a look at [nmt_utils.py](./nmt_utils.py) to see all the formatting. Count and figure out how the formats work, you will need this knowledge later. !--> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  - Dataset\n",
    "\n",
    "We will train the model on a dataset of 10000 human readable dates and their equivalent, standardized, machine readable dates. Let's run the following cells to load the dataset and print some examples. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 10000/10000 [00:00<00:00, 20964.08it/s]\n"
     ]
    }
   ],
   "source": [
    "m = 10000\n",
    "dataset, human_vocab, machine_vocab, inv_machine_vocab = load_dataset(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('9 may 1998', '1998-05-09'),\n",
       " ('10.11.19', '2019-11-10'),\n",
       " ('9/10/70', '1970-09-10'),\n",
       " ('saturday april 28 1990', '1990-04-28'),\n",
       " ('thursday january 26 1995', '1995-01-26'),\n",
       " ('monday march 7 1983', '1983-03-07'),\n",
       " ('sunday may 22 1988', '1988-05-22'),\n",
       " ('08 jul 2008', '2008-07-08'),\n",
       " ('8 sep 1999', '1999-09-08'),\n",
       " ('thursday january 1 1981', '1981-01-01')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You've loaded:\n",
    "- `dataset`: a list of tuples of (human readable date, machine readable date)\n",
    "- `human_vocab`: a python dictionary mapping all characters used in the human readable dates to an integer-valued index \n",
    "- `machine_vocab`: a python dictionary mapping all characters used in machine readable dates to an integer-valued index. These indices are not necessarily consistent with `human_vocab`. \n",
    "- `inv_machine_vocab`: the inverse dictionary of `machine_vocab`, mapping from indices back to characters. \n",
    "\n",
    "Let's preprocess the data and map the raw text data into the index values. We will also use Tx=30 (which we assume is the maximum length of the human readable date; if we get a longer input, we would have to truncate it) and Ty=10 (since \"YYYY-MM-DD\" is 10 characters long). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "human_vocab\n",
    "machine_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X.shape: (10000, 30)\n",
      "Y.shape: (10000, 10)\n",
      "Xoh.shape: (10000, 30, 37)\n",
      "Yoh.shape: (10000, 10, 11)\n"
     ]
    }
   ],
   "source": [
    "Tx = 30\n",
    "Ty = 10\n",
    "X, Y, Xoh, Yoh = preprocess_data(dataset, human_vocab, machine_vocab, Tx, Ty)\n",
    "\n",
    "print(\"X.shape:\", X.shape)\n",
    "print(\"Y.shape:\", Y.shape)\n",
    "print(\"Xoh.shape:\", Xoh.shape)\n",
    "print(\"Yoh.shape:\", Yoh.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X[0] #'<pad>': 36"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You now have:\n",
    "- `X`: a processed version of the human readable dates in the training set, where each character is replaced by an index mapped to the character via `human_vocab`. Each date is further padded to $T_x$ values with a special character (< pad >). `X.shape = (m, Tx)`\n",
    "- `Y`: a processed version of the machine readable dates in the training set, where each character is replaced by the index it is mapped to in `machine_vocab`. You should have `Y.shape = (m, Ty)`. \n",
    "- `Xoh`: one-hot version of `X`, the \"1\" entry's index is mapped to the character thanks to `human_vocab`. `Xoh.shape = (m, Tx, len(human_vocab))`\n",
    "- `Yoh`: one-hot version of `Y`, the \"1\" entry's index is mapped to the character thanks to `machine_vocab`. `Yoh.shape = (m, Tx, len(machine_vocab))`. Here, `len(machine_vocab) = 11` since there are 11 characters ('-' as well as 0-9). \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets also look at some examples of preprocessed training examples. Feel free to play with `index` in the cell below to navigate the dataset and see how source/target dates are preprocessed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 0\n",
    "print(\"Source date:\", dataset[index][0])\n",
    "print(\"Target date:\", dataset[index][1])\n",
    "print()\n",
    "print(\"Source after preprocessing (indices):\", X[index][0])\n",
    "print(\"Target after preprocessing (indices):\", Y[index][0])\n",
    "print()\n",
    "print(\"Source after preprocessing (one-hot):\", Xoh[index][0])\n",
    "print(\"Target after preprocessing (one-hot):\", Yoh[index][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Neural machine translation with attention\n",
    "\n",
    "If you had to translate a book's paragraph from French to English, you would not read the whole paragraph, then close the book and translate. Even during the translation process, you would read/re-read and focus on the parts of the French paragraph corresponding to the parts of the English you are writing down. \n",
    "\n",
    "The attention mechanism tells a Neural Machine Translation model where it should pay attention to at any step. \n",
    "\n",
    "\n",
    "###  Attention mechanism\n",
    "\n",
    "In this part, you will implement the attention mechanism presented in the lecture videos. Here is a figure to remind you how the model works. The diagram on the left shows the attention model. The diagram on the right shows what one \"Attention\" step does to calculate the attention variables $\\alpha^{\\langle t, t' \\rangle}$, which are used to compute the context variable $context^{\\langle t \\rangle}$ for each timestep in the output ($t=1, \\ldots, T_y$). \n",
    "\n",
    "<table>\n",
    "<td> \n",
    "<img src=\"images/attn_model.png\" style=\"width:500;height:500px;\"> <br>\n",
    "</td> \n",
    "<td> \n",
    "<img src=\"images/attn_mechanism.png\" style=\"width:500;height:500px;\"> <br>\n",
    "</td> \n",
    "</table>\n",
    "<caption><center> **Figure 1**: Neural machine translation with attention</center></caption>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Here are some properties of the model that you may notice: \n",
    "\n",
    "- There are two separate LSTMs in this model (see diagram on the left). Because the one at the bottom of the picture is a Bi-directional LSTM and comes *before* the attention mechanism, we will call it *pre-attention* Bi-LSTM. The LSTM at the top of the diagram comes *after* the attention mechanism, so we will call it the *post-attention* LSTM. The pre-attention Bi-LSTM goes through $T_x$ time steps; the post-attention LSTM goes through $T_y$ time steps. \n",
    "\n",
    "- The post-attention LSTM passes $s^{\\langle t \\rangle}, c^{\\langle t \\rangle}$ from one time step to the next. In the lecture videos, we were using only a basic RNN for the post-activation sequence model, so the state captured by the RNN output activations $s^{\\langle t\\rangle}$. But since we are using an LSTM here, the LSTM has both the output activation $s^{\\langle t\\rangle}$ and the hidden cell state $c^{\\langle t\\rangle}$. However, unlike previous text generation examples (such as Dinosaurus in week 1), in this model the post-activation LSTM at time $t$ does will not take the specific generated $y^{\\langle t-1 \\rangle}$ as input; it only takes $s^{\\langle t\\rangle}$ and $c^{\\langle t\\rangle}$ as input. We have designed the model this way, because (unlike language generation where adjacent characters are highly correlated) there isn't as strong a dependency between the previous character and the next character in a YYYY-MM-DD date. \n",
    "\n",
    "- We use $a^{\\langle t \\rangle} = [\\overrightarrow{a}^{\\langle t \\rangle}; \\overleftarrow{a}^{\\langle t \\rangle}]$ to represent the concatenation of the activations of both the forward-direction and backward-directions of the pre-attention Bi-LSTM. \n",
    "\n",
    "- The diagram on the right uses a `RepeatVector` node to copy $s^{\\langle t-1 \\rangle}$'s value $T_x$ times, and then `Concatenation` to concatenate $s^{\\langle t-1 \\rangle}$ and $a^{\\langle t \\rangle}$ to compute $e^{\\langle t, t'}$, which is then passed through a softmax to compute $\\alpha^{\\langle t, t' \\rangle}$. We'll explain how to use `RepeatVector` and `Concatenation` in Keras below. \n",
    "\n",
    "Lets implement this model. You will start by implementing two functions: `one_step_attention()` and `model()`.\n",
    "\n",
    "**1) `one_step_attention()`**: At step $t$, given all the hidden states of the Bi-LSTM ($[a^{<1>},a^{<2>}, ..., a^{<T_x>}]$) and the previous hidden state of the second LSTM ($s^{<t-1>}$), `one_step_attention()` will compute the attention weights ($[\\alpha^{<t,1>},\\alpha^{<t,2>}, ..., \\alpha^{<t,T_x>}]$) and output the context vector (see Figure  1 (right) for details):\n",
    "$$context^{<t>} = \\sum_{t' = 0}^{T_x} \\alpha^{<t,t'>}a^{<t'>}\\tag{1}$$ \n",
    "\n",
    "Note that we are denoting the attention in this notebook $context^{\\langle t \\rangle}$. In the lecture videos, the context was denoted $c^{\\langle t \\rangle}$, but here we are calling it $context^{\\langle t \\rangle}$ to avoid confusion with the (post-attention) LSTM's internal memory cell variable, which is sometimes also denoted $c^{\\langle t \\rangle}$. \n",
    "  \n",
    "**2) `model()`**: Implements the entire model. It first runs the input through a Bi-LSTM to get back $[a^{<1>},a^{<2>}, ..., a^{<T_x>}]$. Then, it calls `one_step_attention()` $T_y$ times (`for` loop). At each iteration of this loop, it gives the computed context vector $c^{<t>}$ to the second LSTM, and runs the output of the LSTM through a dense layer with softmax activation to generate a prediction $\\hat{y}^{<t>}$. \n",
    "\n",
    "\n",
    "\n",
    "**Exercise**: Implement `one_step_attention()`. The function `model()` will call the layers in `one_step_attention()` $T_y$ using a for-loop, and it is important that all $T_y$ copies have the same weights. I.e., it should not re-initiaiize the weights every time. In other words, all $T_y$ steps should have shared weights. Here's how you can implement layers with shareable weights in Keras:\n",
    "1. Define the layer objects (as global variables for examples).\n",
    "2. Call these objects when propagating the input.\n",
    "\n",
    "We have defined the layers you need as global variables. Please run the following cells to create them. Please check the Keras documentation to make sure you understand what these layers are: [RepeatVector()](https://keras.io/layers/core/#repeatvector), [Concatenate()](https://keras.io/layers/merge/#concatenate), [Dense()](https://keras.io/layers/core/#dense), [Activation()](https://keras.io/layers/core/#activation), [Dot()](https://keras.io/layers/merge/#dot)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can use these layers to implement `one_step_attention()`. In order to propagate a Keras tensor object X through one of these layers, use `layer(X)` (or `layer([X,Y])` if it requires multiple inputs.), e.g. `densor(X)` will propagate X through the `Dense(1)` layer defined above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# M1: 使用 全局的 层对象，以在多个 model 中共享他们的权重\n",
    "\n",
    "# GRADED FUNCTION: one_step_attention\n",
    "# Defined shared layers as global variables\n",
    "repeator = RepeatVector(Tx)  \n",
    "#concatenator = Concatenate(axis=-1)\n",
    "concatenator = Concatenate(axis=2)\n",
    "densor = Dense(1, activation = \"relu\")\n",
    "#activator = Activation(softmax, name='attention_weights') # We are using a custom softmax(axis = 1) loaded in this notebook\n",
    "activator=Softmax(axis=1)\n",
    "dotor = Dot(axes = 1)\n",
    "\n",
    "\n",
    "def one_step_attention(a, s_prev): #与RNN 类似，是一个 循环结构\n",
    "    \"\"\"\n",
    "    Performs one step of attention: Outputs a context vector computed as a dot product of the attention weights\n",
    "    \"alphas\" and the hidden states \"a\" of the Bi-LSTM.\n",
    "    \n",
    "    Arguments:\n",
    "    a -- hidden state output of the Bi-LSTM, numpy-array of shape (m, Tx, 2*n_a)\n",
    "    s_prev -- previous hidden state of the (post-attention) LSTM, numpy-array of shape (m, n_s)\n",
    "    \n",
    "    Returns:\n",
    "    context -- context vector, input of the next (post-attetion) LSTM cell\n",
    "    \"\"\"\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "    # Use repeator to repeat s_prev to be of shape (m, Tx, n_s) so that you can concatenate it with all hidden states \"a\" (≈ 1 line)\n",
    "    s_prev = repeator(s_prev) \n",
    "    \n",
    "    # Use concatenator to concatenate a and s_prev on the last axis (≈ 1 line) \n",
    "    concat =concatenator([a,s_prev]) #shape: (m, Tx, 2*n_a+n_s)\n",
    "                                      \n",
    "    # Use densor to propagate concat through a small fully-connected neural network to compute the \"energies\" variable e. (≈1 lines)\n",
    "    e = densor(concat) #  shape: (m, Tx, 1)\n",
    "                    \n",
    "    # Use activator and e to compute the attention weights \"alphas\" (≈ 1 line)\n",
    "    alphas = activator(e) # shape:  (m, Tx, 1)\n",
    "    \n",
    "    # Use dotor together with \"alphas\" and \"a\" to compute the context vector to be given to the next (post-attention) LSTM-cell (≈ 1 line)\n",
    "    context = dotor([alphas,a]) #  shape: (m, 1, 2*n_a)\n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# By XRH in 2019.12.16\n",
    "# M2: 把层 layer 包装为 model ,并通过重新定义 model 的输入的方式 来共享 layer 的权重 \n",
    "\n",
    "n_a = 64\n",
    "n_s = 128 \n",
    "\n",
    "\n",
    "def one_step_attention_model(Tx, n_a, n_s): \n",
    "\n",
    "    \"\"\" \n",
    "    Arguments:\n",
    "    a -- hidden state output of the Bi-LSTM, numpy-array of shape (m, Tx, 2*n_a)\n",
    "    s_prev -- previous hidden state of the (post-attention) LSTM, numpy-array of shape (m, n_s)\n",
    "    \n",
    "    Returns:\n",
    "    context -- context vector, input of the next (post-attetion) LSTM cell\n",
    "    \"\"\"\n",
    "    \n",
    "    repeator = RepeatVector(Tx)  \n",
    "    concatenator = Concatenate(axis=2)\n",
    "    densor = Dense(1, activation = \"relu\")\n",
    "    activator=Softmax(axis=1)\n",
    "    dotor = Dot(axes = 1)\n",
    "    \n",
    "    \n",
    "    a0=Input(shape=(Tx, 2*n_a), name='a')\n",
    "    s_prev0=Input(shape=(n_s,), name='s_prev')\n",
    "    \n",
    "    a=a0 # 否则报错 ： ValueError: Graph disconnected: cannot obtain value for tensor Tensor .... The following previous layers were accessed without issue: []\n",
    "    s_prev=s_prev0\n",
    "    \n",
    "    s_prev = repeator(s_prev) \n",
    "    \n",
    "    concat =concatenator([a,s_prev]) #shape: (m, Tx, 2*n_a+n_s)\n",
    "                                      \n",
    "    e = densor(concat) #  shape: (m, Tx, 1)\n",
    "                    \n",
    "    alphas = activator(e) # shape:  (m, Tx, 1)\n",
    "    \n",
    "    context = dotor([alphas,a]) #  shape: (m, 1, 2*n_a)\n",
    "    \n",
    "    model=Model(inputs=[a0, s_prev0] ,outputs=context)\n",
    "    \n",
    "    return model\n",
    "\n",
    "model_one_step_attention = one_step_attention_model(Tx, n_a, n_s)\n",
    "\n",
    "def one_step_attention_M2(a, s_prev): \n",
    "        \n",
    "    context=model_one_step_attention([a, s_prev])\n",
    "    \n",
    "    return context\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will be able to check the expected output of `one_step_attention()` after you've coded the `model()` function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**: Implement `model()` as explained in figure 2 and the text above. Again, we have defined global layers that will share weights to be used in `model()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 适用于 tensorflow < 2.0 , 此时 tensorflow 和 keras 是分开的 \n",
    "\n",
    "n_a = 64\n",
    "n_s = 128\n",
    "\n",
    "# n_s = 64\n",
    "\n",
    "pre_activation_LSTM_cell=Bidirectional(CuDNNLSTM(n_a, return_sequences=True,return_state = True))\n",
    "\n",
    "post_activation_LSTM_cell = CuDNNLSTM(n_s, return_state = True)\n",
    "\n",
    "output_layer = Dense(len(machine_vocab), activation=softmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  适用于 tensorflow >= 2.0 keras 被直接集成到 tensorflow 的内部\n",
    "\n",
    "n_a = 64\n",
    "n_s = 128\n",
    "\n",
    "# n_s = 64\n",
    "\n",
    "pre_activation_LSTM_cell=Bidirectional(LSTM(n_a, return_sequences=True,return_state = True))\n",
    "\n",
    "post_activation_LSTM_cell = LSTM(n_s, return_state = True)\n",
    "\n",
    "output_layer = Dense(len(machine_vocab), activation=softmax)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can use these layers $T_y$ times in a `for` loop to generate the outputs, and their parameters will not be reinitialized. You will have to carry out the following steps: \n",
    "\n",
    "1. Propagate the input into a [Bidirectional](https://keras.io/layers/wrappers/#bidirectional) [LSTM](https://keras.io/layers/recurrent/#lstm)\n",
    "2. Iterate for $t = 0, \\dots, T_y-1$: \n",
    "    1. Call `one_step_attention()` on $[\\alpha^{<t,1>},\\alpha^{<t,2>}, ..., \\alpha^{<t,T_x>}]$ and $s^{<t-1>}$ to get the context vector $context^{<t>}$.\n",
    "    2. Give $context^{<t>}$ to the post-attention LSTM cell. Remember pass in the previous hidden-state $s^{\\langle t-1\\rangle}$ and cell-states $c^{\\langle t-1\\rangle}$ of this LSTM using `initial_state= [previous hidden state, previous cell state]`. Get back the new hidden state $s^{<t>}$ and the new cell state $c^{<t>}$.\n",
    "    3. Apply a softmax layer to $s^{<t>}$, get the output. \n",
    "    4. Save the output by adding it to the list of outputs.\n",
    "\n",
    "3. Create your Keras model instance, it should have three inputs (\"inputs\", $s^{<0>}$ and $c^{<0>}$) and output the list of \"outputs\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: model \n",
    "def model(Tx, Ty, n_a, n_s, human_vocab_size, machine_vocab_size,m):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    Tx -- length of the input sequence\n",
    "    Ty -- length of the output sequence\n",
    "    n_a -- hidden state size of the Bi-LSTM\n",
    "    n_s -- hidden state size of the post-attention LSTM\n",
    "    human_vocab_size -- size of the python dictionary \"human_vocab\"\n",
    "    machine_vocab_size -- size of the python dictionary \"machine_vocab\"\n",
    "\n",
    "    Returns:\n",
    "    model -- Keras model instance\n",
    "    \"\"\"\n",
    "    \n",
    "    # Define the inputs of your model with a shape (Tx,)\n",
    "\n",
    "    \n",
    "    X = Input(shape=(Tx, human_vocab_size)) # shape: (m,Tx,human_vocab_size)\n",
    "    \n",
    "    \n",
    "    # Define s0 and c0, initial hidden state for the decoder LSTM of shape (n_s,)\n",
    "    s0 = Input(shape=(n_s,), name='s0')  # shape of s:  (m, 64)\n",
    "    c0 = Input(shape=(n_s,), name='c0')  # shape of c:  (m, 64)\n",
    "    s = s0\n",
    "    c = c0\n",
    "\n",
    "#     m=X.shape[0] # m=None \n",
    "#     s=K.zeros(shape=(m, n_s))\n",
    "#     c=K.zeros(shape=(m, n_s))\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Initialize empty list of outputs\n",
    "    outputs = []\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "    \n",
    "    # Step 1: Define your pre-attention Bi-LSTM. Remember to use return_sequences=True. (≈ 1 line)\n",
    "    a, forward_h, forward_c, backward_h, backward_c= pre_activation_LSTM_cell(inputs=X) #  shape of a : (m,Tx, 2*n_a)\n",
    "\n",
    "    # Step 2: Iterate for Ty steps\n",
    "    for t in range(Ty):\n",
    "    \n",
    "        # Step 2.A: Perform one step of the attention mechanism to get back the context vector at step t (≈ 1 line)\n",
    "        # a.shape()\n",
    "        context = one_step_attention(a, s) # shape of s:  (m, 64)\n",
    "        \n",
    "        # Step 2.B: Apply the post-attention LSTM cell to the \"context\" vector.\n",
    "        # Don't forget to pass: initial_state = [hidden state, cell state] (≈ 1 line)\n",
    "        \n",
    "        s, _, c = post_activation_LSTM_cell(inputs=context,initial_state=[s, c])#输入只有一个时间步\n",
    "\n",
    "        \n",
    "        # Step 2.C: Apply Dense layer to the hidden state output of the post-attention LSTM (≈ 1 line)\n",
    "        out = output_layer(s)\n",
    "        \n",
    "        # Step 2.D: Append \"out\" to the \"outputs\" list (≈ 1 line)\n",
    "        outputs.append(out) # shape of out : ( m ,machine_vocab) \n",
    "    \n",
    "    #\n",
    "    # Step 3: Create model instance taking three inputs and returning the list of outputs. (≈ 1 line)\n",
    "    model =  Model(inputs=[X,s0, c0], outputs=outputs)\n",
    "    #shape of outs : ( Ty ,m ,machine_vocab) \n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#   By XRH in 2019.12.16\n",
    "#  decoder 中，修改 LSTM 的初始 隐藏状态的输入 ，由原来的 0 向量，改为 encoder 中 LSTM 最后一个时间步的隐状态（注意进行拼接）    \n",
    "#\n",
    "def model1(Tx, Ty, n_a, n_s, human_vocab_size, machine_vocab_size):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    Tx -- length of the input sequence\n",
    "    Ty -- length of the output sequence\n",
    "    n_a -- hidden state size of the Bi-LSTM\n",
    "    n_s -- hidden state size of the post-attention LSTM\n",
    "    human_vocab_size -- size of the python dictionary \"human_vocab\"\n",
    "    machine_vocab_size -- size of the python dictionary \"machine_vocab\"\n",
    "\n",
    "    Returns:\n",
    "    model -- Keras model instance\n",
    "    \"\"\"\n",
    "    \n",
    "    # Define the inputs of your model with a shape (Tx,)\n",
    "\n",
    "    \n",
    "    X = Input(shape=(Tx, human_vocab_size)) # shape: (m,Tx,human_vocab_size)\n",
    "    \n",
    "    \n",
    "    # Initialize empty list of outputs\n",
    "    outputs = []\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "    \n",
    "    # Step 1: Define your pre-attention Bi-LSTM. Remember to use return_sequences=True. (≈ 1 line)\n",
    "    a, forward_h, forward_c, backward_h, backward_c= pre_activation_LSTM_cell(inputs=X) #  shape of a : (m,Tx, 2*n_a)\n",
    "    s = Concatenate()([forward_h, backward_h]) # shape of s:  (m, 64)\n",
    "    c = Concatenate()([forward_c, backward_c]) # shape of c:  (m, 64)\n",
    "\n",
    "\n",
    "    # Step 2: Iterate for Ty steps\n",
    "    for t in range(Ty):\n",
    "    \n",
    "        # Step 2.A: Perform one step of the attention mechanism to get back the context vector at step t (≈ 1 line)\n",
    "        # a.shape()\n",
    "        context = one_step_attention(a, s) # shape of s:  (m, 64)\n",
    "        \n",
    "        # Step 2.B: Apply the post-attention LSTM cell to the \"context\" vector.\n",
    "        # Don't forget to pass: initial_state = [hidden state, cell state] (≈ 1 line)\n",
    "        \n",
    "        s, _, c = post_activation_LSTM_cell(inputs=context,initial_state=[s, c])#输入只有一个时间步\n",
    "\n",
    "        \n",
    "        # Step 2.C: Apply Dense layer to the hidden state output of the post-attention LSTM (≈ 1 line)\n",
    "        out = output_layer(s)\n",
    "        \n",
    "        # Step 2.D: Append \"out\" to the \"outputs\" list (≈ 1 line)\n",
    "        outputs.append(out) # shape of out : ( m ,machine_vocab) \n",
    "    \n",
    "    #\n",
    "    # Step 3: Create model instance taking three inputs and returning the list of outputs. (≈ 1 line)\n",
    "    model =  Model(inputs=[X], outputs=outputs)\n",
    "    #shape of outs : ( Ty ,m ,machine_vocab) \n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# testing0\n",
    "def testing_tensors():\n",
    "    a=K.zeros(5) \n",
    "    print(a)\n",
    "    print(K.eval(a))\n",
    "\n",
    "    b=K.constant([1, 2, 3],dtype='uint8')\n",
    "    print(b)\n",
    "    print(K.eval(b))\n",
    "\n",
    "    val = np.array([[1, 2], [3, 4]])\n",
    "    kvar = K.variable(value=val, dtype='int32', name='example_var')\n",
    "    print(kvar)\n",
    "    print(K.eval(kvar))\n",
    "    \n",
    "    keras_placeholder = K.placeholder(shape=(2, 4, 5))\n",
    "    print(keras_placeholder)\n",
    "\n",
    "\n",
    "# testing_tensors()\n",
    "\n",
    "# testing1 \n",
    "def convert_to_one_hot(y, C):\n",
    "    return np.eye(C)[y.reshape(-1)].T\n",
    "\n",
    "# y = np.array([1,2,3,4])\n",
    "# y=np.argmax(y, axis = -1)\n",
    "# convert_to_one_hot(y,len(machine_vocab))\n",
    "\n",
    "# testing2\n",
    "# y=np.array([1,2,3,4])\n",
    "# y\n",
    "# y.reshape(2,2)\n",
    "# y\n",
    "\n",
    "# testing3   \n",
    "#array的尊卑关系： numpy -> backend tensor -> layer tensor\n",
    "def tensor_test():\n",
    "    \n",
    "#0.\n",
    "    x=np.zeros((1,len(machine_vocab)))\n",
    "    print(x.shape)\n",
    "    pred0=K.reshape(x, (1,len(machine_vocab)))\n",
    "    print(pred0)\n",
    "#     Tensor(\"Reshape_2:0\", shape=(1, 11), dtype=float64)\n",
    "\n",
    "#1.    \n",
    "#     pred0=np.zeros((1,len(machine_vocab))) \n",
    "#     pred0=Reshape(target_shape=(1,len(machine_vocab)))(pred0)\n",
    "#   ValueError: Layer reshape_42 was called with an input that isn't a symbolic tensor. Received type: <class 'numpy.ndarray'>. Full input: [array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])]. All inputs to the layer should be tensors.     \n",
    "\n",
    "#2.   \n",
    "#     pred0=K.zeros((1,len(machine_vocab))) \n",
    "#     pred0=Reshape(target_shape=(1,len(machine_vocab)))(pred0)\n",
    "#     print(pred0)\n",
    "#  Tensor(\"reshape_91/Reshape:0\", shape=(1, 1, 11), dtype=float32)\n",
    "\n",
    "# tensor_test()\n",
    "    \n",
    "\n",
    "def reshape_tensor(x, shape):\n",
    "    return K.reshape(x, shape);\n",
    "\n",
    "def argmax_tensor(x, axis):\n",
    "    return K.argmax(x, axis);\n",
    "\n",
    "def one_hot_tensor(x, num_classes):\n",
    "    #by # https://fdalvi.github.io/blog/2018-04-07-keras-sequential-onehot/\n",
    "    return K.one_hot(K.cast(x, 'uint8'), num_classes);\n",
    "\n",
    "\n",
    "\n",
    "# testing4\n",
    "def tensor_test2():\n",
    "    pred=Input(shape=(len(machine_vocab),), name='pred')  \n",
    "    print (pred)\n",
    "\n",
    "#   以下两个 reshape 效果相同 都是输出了 layer tensor\n",
    "    x = Lambda(reshape_tensor, arguments={'shape': (1, len(machine_vocab))})(pred)\n",
    "    print (x)\n",
    "    \n",
    "    y=Reshape(target_shape=(1,len(machine_vocab)))(pred)\n",
    "    print (y)\n",
    "\n",
    "tensor_test2()\n",
    "    \n",
    "def tensor_test3():\n",
    "    pred=Input(shape=(len(machine_vocab),), name='pred')  \n",
    "    print (pred)\n",
    "\n",
    "    pred=Lambda(argmax_tensor, arguments={'axis': -1 })(pred)\n",
    "    print(pred)\n",
    "    pred=Lambda(one_hot_tensor, arguments={'num_classes': len(machine_vocab) })(pred)  \n",
    "    print(pred)\n",
    "\n",
    "tensor_test3()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import losses\n",
    "def base_model(m):\n",
    "\n",
    "    # 建立模型第一层的 keras Tensor，只能是Input 或者从 别的地方传来的 tensor from keras.layer\n",
    "    X = Input(shape=(32,), name='X') # 此时 shape(None,32) ；真正训练时为 shape(m,32)\n",
    "\n",
    "    init_state = Input(shape=(32,), name='init_state')\n",
    "\n",
    "    #1.\n",
    "    # b= K.zeros((100,32))\n",
    "    # b = Input( name='input_b',tensor=K.zeros((100,32)))  #<tf.Variable 'Variable:0' shape=(100, 32) dtype=float32>\n",
    "    # 喂入模型的一个 batch的样本的数目 m 此时是未知的，因此为None ，\n",
    "    # 所以模型的第一层中，所有和 m 相关的tensor 都要使用 Input，相当于tensorflow的一个占位符，其他方法不work:\n",
    "    #\n",
    "    # 反例1:\n",
    "    # b=K.zeros((init_state.shape[0],32)) # init_state.shape[0]=None，K.zeros 的shape参数 中不能出现None\n",
    "    #\n",
    "    # 反例2:\n",
    "    # b= K.zeros((m,32)) # m 由参数传入，m=100\n",
    "    # b = Input( name='input_b',tensor=K.zeros((m,32))) # <tf.Variable 'Variable:0' shape=(100, 32) dtype=float32>\n",
    "    # 虽然成功建立的 tensor 但是和 来源于 keras.layer 的a (shape: (None,32)) 无法进行 axis=1的拼接\n",
    "    # 即报错： ValueError: A `Concatenate` layer requires inputs with matching shapes except for the concat axis. Got inputs shapes: [(None, 32), (100, 32)]\n",
    "\n",
    "    #2.\n",
    "    # 连接的所有tensor 必须来源于 keras.layer（血统纯正），否则无法生成计算图\n",
    "    # 下面两种方法都报 AttributeError: 'NoneType' object has no attribute '_inbound_nodes'\n",
    "    #M1\n",
    "    # init_state_2 = K.constant(np.zeros((1,64)), dtype='float32') \n",
    "    #M2\n",
    "    # init_state_2=K.ones((1,64))\n",
    "    \n",
    "    #M3 使用 keras.layer.Reshape 转换，但是维度不对,会自动的在全面加上一维\n",
    "    \n",
    "\n",
    "    concat = Concatenate(axis=1)([X, init_state])\n",
    "\n",
    "#     concat2= Concatenate(axis=1)([concat, b])\n",
    "\n",
    "#     concat2 = Concatenate(axis=0)([concat, init_state_2])\n",
    "\n",
    "    dense = Dense(1, name='dense_1')\n",
    "\n",
    "    res=dense(concat)\n",
    "\n",
    "    print(res)\n",
    "    model= Model(inputs=[ X , init_state ], outputs=res)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "M=100\n",
    "model = base_model(M)\n",
    "\n",
    "X=np.random.randint(0,10,size=[M,32])\n",
    "init_state=np.zeros((M,32))\n",
    "# b_train=K.zeros((M,32)) #ValueError: If your data is in the form of symbolic tensors, you should specify the `steps_per_epoch` argument (instead of the `batch_size` argument, because symbolic tensors are expected to produce batches of input data).\n",
    "\n",
    "x_train = [X, init_state]\n",
    "y_train=np.random.randint(low=0,high=2,size=[M,1])\n",
    "\n",
    "\n",
    "model.compile(loss=losses.binary_crossentropy, optimizer='sgd')\n",
    "\n",
    "batch_size=10\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=5\n",
    "          )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#   By XRH in 2019.12.28\n",
    "# 使用 keras.layer 搭建神经网络 不太灵活：\n",
    "# （1）keras.layer 提供的结构太少，并且只能用这些 层来 搭建\n",
    "# （2）要使用 后端提供的丰富的函数 还需要使用 Lambda层进行包装，较繁琐\n",
    "# 因此，我们 使用 后端的函数来 定义属于自己的层\n",
    "\n",
    "from keras.engine.topology import Layer\n",
    "\n",
    "class ConcatDense(Layer):\n",
    "\n",
    "    def __init__(self, output_dim, **kwargs):\n",
    "        self.output_dim = output_dim\n",
    "        super(ConcatDense, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert isinstance(input_shape, list)\n",
    "        # 为该层创建一个可训练的权重\n",
    "        self.kernel = self.add_weight(name='kernel',\n",
    "                                      shape=(input_shape[0][1]+input_shape[1][1], self.output_dim),\n",
    "                                      initializer='uniform',\n",
    "                                      trainable=True)\n",
    "        \n",
    "        self.bias = self.add_weight(shape=(input_shape[0][1]+input_shape[1][1],),\n",
    "                                        initializer='zeros',\n",
    "                                        trainable=False)\n",
    "        \n",
    "        self.input_shapes=input_shape\n",
    "        \n",
    "        super(ConcatDense, self).build(input_shape)  # 一定要在最后调用它\n",
    "\n",
    "    def call(self, inputs):\n",
    "        assert isinstance(inputs, list)\n",
    "        \n",
    "        X, init_state = inputs # X shape: (None,32)  init_state shape: (None,32)\n",
    "        \n",
    "        print( self.input_shapes)\n",
    "        print(X)\n",
    "        print(init_state)\n",
    "        \n",
    "#       a=K.ones(self.input_shapes[0])        \n",
    "        concat=K.concatenate([X,init_state],axis=1)\n",
    "#         concat2=K.concatenate([concat,K.ones((1,64))],axis=0)\n",
    "\n",
    "        outputs= K.bias_add(concat, self.bias, data_format='channels_last')\n",
    "        \n",
    "        outputs=outputs+concat\n",
    "\n",
    "        res=K.dot(outputs, self.kernel)\n",
    "        \n",
    "        return res\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        assert isinstance(input_shape, list)\n",
    "        shape_X, shape_init_state = input_shape\n",
    "        return (shape_X[0], self.output_dim)\n",
    "\n",
    "\n",
    "    \n",
    "from keras import losses\n",
    "def base_model(m):\n",
    "\n",
    "    # 建立模型第一层的 keras Tensor，只能是Input 或者从 别的地方传来的 tensor from keras.layer\n",
    "    X = Input(shape=(32,), name='X') # 此时 shape(None,32) ；真正训练时为 shape(m,32)\n",
    "\n",
    "    init_state = Input(shape=(32,), name='init_state')\n",
    "\n",
    "    res=ConcatDense(1)([ X , init_state ])\n",
    "    print(res)\n",
    "    model= Model(inputs=[ X , init_state ], outputs=res)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "M=100\n",
    "model = base_model(M)\n",
    "\n",
    "X=np.random.randint(0,10,size=[M,32])\n",
    "init_state=np.zeros((M,32))\n",
    "\n",
    "x_train = [X, init_state]\n",
    "y_train=np.random.randint(low=0,high=2,size=[M,1])\n",
    "\n",
    "\n",
    "model.compile(loss=losses.binary_crossentropy, optimizer='sgd')\n",
    "\n",
    "batch_size=10\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=5\n",
    "          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#   By XRH in 2019.9.10\n",
    "#   改进attention：解码时加入上一个时刻的输出单词（eg. 当前词是'-' 下一个词必须为数字）\n",
    "#  (1)在decoder， 经过softmax 输出后 取最大的 那一个 machine_vocab 的one-hot \n",
    "#     向量 与 context 拼接后输入 post_activation_LSTM_cell ，\n",
    "#  (2)无需更改 lstm 的输出维度 ，仍然保持 n_s=128\n",
    "# （3）把所有的 layer object 声明为全局的，以便 后面重构 decoder 可以使用训练好的网络结构\n",
    "\n",
    "\n",
    "def argmax_tensor(x, axis):\n",
    "    return K.argmax(x, axis);\n",
    "\n",
    "def argmin_tensor(x, axis):\n",
    "    return K.argmin(x, axis);\n",
    "\n",
    "lambda_argmin=Lambda(argmin_tensor, arguments={'axis': -1 },name='argmin_tensor')\n",
    "\n",
    "\n",
    "def one_hot_tensor(x, num_classes):\n",
    "    #by : https://fdalvi.github.io/blog/2018-04-07-keras-sequential-onehot/\n",
    "    return K.one_hot(K.cast(x, 'uint8'), num_classes);\n",
    "\n",
    "n_a = 64\n",
    "n_s = 128 \n",
    "\n",
    "\n",
    "pre_activation_LSTM_cell=Bidirectional(LSTM(n_a, return_sequences=True,return_state = True),name='encoder_lstm')\n",
    "\n",
    "concatenate_s=Concatenate(name='concatenate_s')\n",
    "concatenate_c=Concatenate(name='concatenate_c')\n",
    "\n",
    "concatenate_context=Concatenate()\n",
    "\n",
    "\n",
    "\n",
    "post_activation_LSTM_cell = LSTM(n_s, return_state = True,name='decoder_lstm') \n",
    "output_layer = Dense(len(machine_vocab), activation=softmax,name='decoder_output')\n",
    "\n",
    "\n",
    "lambda_argmax=Lambda(argmax_tensor, arguments={'axis': -1 },name='argmax_tensor')\n",
    "lambda_one_hot=Lambda(one_hot_tensor, arguments={'num_classes': len(machine_vocab) },name='one_hot_tensor')  \n",
    "\n",
    "reshape=Reshape(target_shape=(1,len(machine_vocab)))\n",
    "\n",
    "\n",
    "def model2(Tx, Ty, n_a, n_s, human_vocab_size, machine_vocab_size):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    Tx -- length of the input sequence\n",
    "    Ty -- length of the output sequence\n",
    "    n_a -- hidden state size of the Bi-LSTM\n",
    "    n_s -- hidden state size of the post-attention LSTM\n",
    "    human_vocab_size -- size of the python dictionary \"human_vocab\"\n",
    "    machine_vocab_size -- size of the python dictionary \"machine_vocab\"\n",
    "\n",
    "    Returns:\n",
    "    model -- Keras model instance\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    X = Input(shape=(Tx, human_vocab_size)) # shape: (m,Tx,human_vocab_size)\n",
    "\n",
    "    \n",
    "    pred0=Input(shape=(1,len(machine_vocab)), name='pred0')  # shape of pred0 (m ,1, 11)\n",
    "\n",
    "\n",
    "    pred=pred0\n",
    "    \n",
    "    print('pred: after Input',pred)\n",
    "\n",
    "    outputs = []\n",
    "    \n",
    "  \n",
    "    a, forward_h, forward_c, backward_h, backward_c= pre_activation_LSTM_cell(inputs=X) #  shape of a : (m,Tx, 2*n_a)\n",
    "\n",
    "\n",
    "    s = concatenate_s([forward_h, backward_h]) # shape of s:  (m, 64+64=128)\n",
    "    c = concatenate_c([forward_c, backward_c])\n",
    "\n",
    "\n",
    "    for t in range(Ty):\n",
    "    \n",
    "      \n",
    "#         context = one_step_attention(a, s) # shape of context :  (m, 1, 128)\n",
    "\n",
    "        context=one_step_attention_M2(a, s)\n",
    "        print('context after one_step_attention: ',context)\n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "        context=concatenate_context([context,pred])# shape of context: (m,128+11=139) \n",
    "       \n",
    "        print('context after Concatenate:  ',context)\n",
    "    \n",
    "        s, _, c = post_activation_LSTM_cell(inputs=context,initial_state=[s, c])#输入只有一个时间步\n",
    "\n",
    "    \n",
    "        out = output_layer(s)      \n",
    "       \n",
    " # 1. model 必须全部用 keras 包里的tensor 计算，而不能使用 numpy中的函数，因为都是带上 m个样本 的并行计算（利用GPU加速）      \n",
    "#         pred= np.argmax(out, axis = -1) \n",
    "#         pred=convert_to_one_hot(pred,len(machine_vocab))\n",
    "        \n",
    "\n",
    "#2.也不能 直接用keras.backend 中的函数，它的返回仅仅是tensor \n",
    "#         pred= K.argmax(out, axis = -1)\n",
    "#         pred=K.one_hot(pred,len(machine_vocab))\n",
    "# AttributeError: 'NoneType' object has no attribute '_inbound_nodes'\n",
    "\n",
    "#3.必须用 keras.layers 中的 它的输入输出 自动会考虑 一个batch 的计算 ；注意对比两个 reshape\n",
    "# keras.layers.Reshape(target_shape) 输出为 (batch_size,) + target_shape\n",
    "# keras.backend.reshape(x, shape) 输出为 shape\n",
    "\n",
    "\n",
    "        pred=lambda_argmax(out)\n",
    "        pred=lambda_one_hot(pred)\n",
    "        \n",
    "        print(pred)\n",
    "\n",
    "#         pred=RepeatVector(1)(pred)\n",
    "\n",
    "        pred=reshape(pred)\n",
    "        print(pred)\n",
    "          \n",
    "        outputs.append(out) # shape of out : ( m ,machine_vocab) \n",
    "    \n",
    "    #1.\n",
    "    #model =  Model(inputs=[X, s0, c0], outputs=outputs) \n",
    "    # 未加上 新增的pred0\n",
    "    #ValueError: Graph disconnected: cannot obtain value for tensor Tensor\n",
    "    \n",
    "    #2.\n",
    "\n",
    "    model =  Model(inputs=[X ,pred0], outputs=outputs) \n",
    "    \n",
    "    #shape of outputs : ( Ty ,m ,machine_vocab) \n",
    "    \n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following cell to create your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model(Tx, Ty, n_a, n_s, len(human_vocab), len(machine_vocab),m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = model1(Tx, Ty, n_a, n_s, len(human_vocab), len(machine_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred: after Input Tensor(\"pred0:0\", shape=(None, 1, 11), dtype=float32)\n",
      "context after one_step_attention:  Tensor(\"model/Identity:0\", shape=(None, 1, 128), dtype=float32)\n",
      "context after Concatenate:   Tensor(\"concatenate_1/Identity:0\", shape=(None, 1, 139), dtype=float32)\n",
      "Tensor(\"one_hot_tensor/Identity:0\", shape=(None, 11), dtype=float32)\n",
      "Tensor(\"reshape/Identity:0\", shape=(None, 1, 11), dtype=float32)\n",
      "context after one_step_attention:  Tensor(\"model_1/Identity:0\", shape=(None, 1, 128), dtype=float32)\n",
      "context after Concatenate:   Tensor(\"concatenate_1_1/Identity:0\", shape=(None, 1, 139), dtype=float32)\n",
      "Tensor(\"one_hot_tensor_1/Identity:0\", shape=(None, 11), dtype=float32)\n",
      "Tensor(\"reshape_1/Identity:0\", shape=(None, 1, 11), dtype=float32)\n",
      "context after one_step_attention:  Tensor(\"model_2/Identity:0\", shape=(None, 1, 128), dtype=float32)\n",
      "context after Concatenate:   Tensor(\"concatenate_1_2/Identity:0\", shape=(None, 1, 139), dtype=float32)\n",
      "Tensor(\"one_hot_tensor_2/Identity:0\", shape=(None, 11), dtype=float32)\n",
      "Tensor(\"reshape_2/Identity:0\", shape=(None, 1, 11), dtype=float32)\n",
      "context after one_step_attention:  Tensor(\"model_3/Identity:0\", shape=(None, 1, 128), dtype=float32)\n",
      "context after Concatenate:   Tensor(\"concatenate_1_3/Identity:0\", shape=(None, 1, 139), dtype=float32)\n",
      "Tensor(\"one_hot_tensor_3/Identity:0\", shape=(None, 11), dtype=float32)\n",
      "Tensor(\"reshape_3/Identity:0\", shape=(None, 1, 11), dtype=float32)\n",
      "context after one_step_attention:  Tensor(\"model_4/Identity:0\", shape=(None, 1, 128), dtype=float32)\n",
      "context after Concatenate:   Tensor(\"concatenate_1_4/Identity:0\", shape=(None, 1, 139), dtype=float32)\n",
      "Tensor(\"one_hot_tensor_4/Identity:0\", shape=(None, 11), dtype=float32)\n",
      "Tensor(\"reshape_4/Identity:0\", shape=(None, 1, 11), dtype=float32)\n",
      "context after one_step_attention:  Tensor(\"model_5/Identity:0\", shape=(None, 1, 128), dtype=float32)\n",
      "context after Concatenate:   Tensor(\"concatenate_1_5/Identity:0\", shape=(None, 1, 139), dtype=float32)\n",
      "Tensor(\"one_hot_tensor_5/Identity:0\", shape=(None, 11), dtype=float32)\n",
      "Tensor(\"reshape_5/Identity:0\", shape=(None, 1, 11), dtype=float32)\n",
      "context after one_step_attention:  Tensor(\"model_6/Identity:0\", shape=(None, 1, 128), dtype=float32)\n",
      "context after Concatenate:   Tensor(\"concatenate_1_6/Identity:0\", shape=(None, 1, 139), dtype=float32)\n",
      "Tensor(\"one_hot_tensor_6/Identity:0\", shape=(None, 11), dtype=float32)\n",
      "Tensor(\"reshape_6/Identity:0\", shape=(None, 1, 11), dtype=float32)\n",
      "context after one_step_attention:  Tensor(\"model_7/Identity:0\", shape=(None, 1, 128), dtype=float32)\n",
      "context after Concatenate:   Tensor(\"concatenate_1_7/Identity:0\", shape=(None, 1, 139), dtype=float32)\n",
      "Tensor(\"one_hot_tensor_7/Identity:0\", shape=(None, 11), dtype=float32)\n",
      "Tensor(\"reshape_7/Identity:0\", shape=(None, 1, 11), dtype=float32)\n",
      "context after one_step_attention:  Tensor(\"model_8/Identity:0\", shape=(None, 1, 128), dtype=float32)\n",
      "context after Concatenate:   Tensor(\"concatenate_1_8/Identity:0\", shape=(None, 1, 139), dtype=float32)\n",
      "Tensor(\"one_hot_tensor_8/Identity:0\", shape=(None, 11), dtype=float32)\n",
      "Tensor(\"reshape_8/Identity:0\", shape=(None, 1, 11), dtype=float32)\n",
      "context after one_step_attention:  Tensor(\"model_9/Identity:0\", shape=(None, 1, 128), dtype=float32)\n",
      "context after Concatenate:   Tensor(\"concatenate_1_9/Identity:0\", shape=(None, 1, 139), dtype=float32)\n",
      "Tensor(\"one_hot_tensor_9/Identity:0\", shape=(None, 11), dtype=float32)\n",
      "Tensor(\"reshape_9/Identity:0\", shape=(None, 1, 11), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "model2 = model2(Tx, Ty, n_a, n_s, len(human_vocab), len(machine_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import plot_model \n",
    "\n",
    "plot_model(model, to_file='model.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import plot_model \n",
    "\n",
    "plot_model(model2, to_file='model2.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get a summary of the model to check if it matches the expected output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**:\n",
    "\n",
    "Here is the summary you should see\n",
    "<table>\n",
    "    <tr>\n",
    "        <td>\n",
    "            **Total params:**\n",
    "        </td>\n",
    "        <td>\n",
    "         185,484\n",
    "        </td>\n",
    "    </tr>\n",
    "        <tr>\n",
    "        <td>\n",
    "            **Trainable params:**\n",
    "        </td>\n",
    "        <td>\n",
    "         185,484\n",
    "        </td>\n",
    "    </tr>\n",
    "            <tr>\n",
    "        <td>\n",
    "            **Non-trainable params:**\n",
    "        </td>\n",
    "        <td>\n",
    "         0\n",
    "        </td>\n",
    "    </tr>\n",
    "                    <tr>\n",
    "        <td>\n",
    "            **bidirectional_1's output shape **\n",
    "        </td>\n",
    "        <td>\n",
    "         (None, 30, 128)  \n",
    "        </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>\n",
    "            **repeat_vector_1's output shape **\n",
    "        </td>\n",
    "        <td>\n",
    "         (None, 30, 128)  \n",
    "        </td>\n",
    "    </tr>\n",
    "                <tr>\n",
    "        <td>\n",
    "            **concatenate_1's output shape **\n",
    "        </td>\n",
    "        <td>\n",
    "         (None, 30, 256) \n",
    "        </td>\n",
    "    </tr>\n",
    "            <tr>\n",
    "        <td>\n",
    "            **attention_weights's output shape **\n",
    "        </td>\n",
    "        <td>\n",
    "         (None, 30, 1)  \n",
    "        </td>\n",
    "    </tr>\n",
    "        <tr>\n",
    "        <td>\n",
    "            **dot_1's output shape **\n",
    "        </td>\n",
    "        <td>\n",
    "         (None, 1, 128) \n",
    "        </td>\n",
    "    </tr>\n",
    "           <tr>\n",
    "        <td>\n",
    "            **dense_2's output shape **\n",
    "        </td>\n",
    "        <td>\n",
    "         (None, 11) \n",
    "        </td>\n",
    "    </tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As usual, after creating your model in Keras, you need to compile it and define what loss, optimizer and metrics your are want to use. Compile your model using `categorical_crossentropy` loss, a custom [Adam](https://keras.io/optimizers/#adam) [optimizer](https://keras.io/optimizers/#usage-of-optimizers) (`learning rate = 0.005`, $\\beta_1 = 0.9$, $\\beta_2 = 0.999$, `decay = 0.01`)  and `['accuracy']` metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### START CODE HERE ### (≈2 lines)\n",
    "opt = Adam(lr = 0.005, beta_1=0.9, beta_2=0.999, decay = 0.01)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "\n",
    "### END CODE HERE ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = Adam(lr = 0.005, beta_1=0.9, beta_2=0.999, decay = 0.01)\n",
    "model1.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "opt = Adam(lr = 0.005, beta_1=0.9, beta_2=0.999, decay = 0.01)\n",
    "model2.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last step is to define all your inputs and outputs to fit the model:\n",
    "- You already have X of shape $(m = 10000, T_x = 30)$ containing the training examples.\n",
    "- You need to create `s0` and `c0` to initialize your `post_activation_LSTM_cell` with 0s.\n",
    "- Given the `model()` you coded, you need the \"outputs\" to be a list of T_y elements of shape (m, 11). So that: `outputs[i][0], ..., outputs[i][Ty]` represent the true labels (characters) corresponding to the $i^{th}$ training example (`X[i]`). More generally, `outputs[i][j]` is the true label of the $j^{th}$ character in the $i^{th}$ training example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tips : \n",
    "# n_a = 64\n",
    "# n_s = 128 \n",
    "# m = 10000\n",
    "\n",
    "\n",
    "outputs = list(Yoh.swapaxes(0,1)) #Yoh.swapaxes(0,1) 第0维度 和 第1 维度交换，原来为(m,T_y,11) 变换后 为：(T_y,m,11)\n",
    "\n",
    "# for model\n",
    "s0 = np.zeros((m, n_s)) \n",
    "c0 = np.zeros((m, n_s))\n",
    "\n",
    "\n",
    "# for model2\n",
    "pred0=np.zeros((m,1,len(machine_vocab)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now fit the model and run it for one epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "history=model.fit([Xoh, s0, c0], outputs, epochs=40, batch_size=2048,validation_split=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for model1\n",
    "history=model.fit([Xoh], outputs, epochs=40, batch_size=2048,validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/120\n",
      "5/5 [==============================] - 6s 1s/step - loss: 22.3426 - decoder_output_loss: 2.1691 - decoder_output_1_loss: 2.2395 - decoder_output_2_loss: 2.3700 - decoder_output_3_loss: 2.5951 - decoder_output_4_loss: 1.7753 - decoder_output_5_loss: 1.8924 - decoder_output_6_loss: 2.6737 - decoder_output_7_loss: 1.6705 - decoder_output_8_loss: 2.1644 - decoder_output_9_loss: 2.7925 - decoder_output_accuracy: 0.2679 - decoder_output_1_accuracy: 0.0200 - decoder_output_2_accuracy: 0.0346 - decoder_output_3_accuracy: 0.0311 - decoder_output_4_accuracy: 0.6804 - decoder_output_5_accuracy: 0.1017 - decoder_output_6_accuracy: 0.0280 - decoder_output_7_accuracy: 0.6878 - decoder_output_8_accuracy: 0.0573 - decoder_output_9_accuracy: 0.0278 - val_loss: 20.7618 - val_decoder_output_loss: 1.8105 - val_decoder_output_1_loss: 1.8432 - val_decoder_output_2_loss: 2.2650 - val_decoder_output_3_loss: 2.7048 - val_decoder_output_4_loss: 1.5890 - val_decoder_output_5_loss: 1.7396 - val_decoder_output_6_loss: 2.6116 - val_decoder_output_7_loss: 1.5541 - val_decoder_output_8_loss: 2.0019 - val_decoder_output_9_loss: 2.6420 - val_decoder_output_accuracy: 0.5930 - val_decoder_output_1_accuracy: 0.3930 - val_decoder_output_2_accuracy: 0.1820 - val_decoder_output_3_accuracy: 0.1170 - val_decoder_output_4_accuracy: 0.5620 - val_decoder_output_5_accuracy: 0.0000e+00 - val_decoder_output_6_accuracy: 0.0000e+00 - val_decoder_output_7_accuracy: 1.0000 - val_decoder_output_8_accuracy: 0.0000e+00 - val_decoder_output_9_accuracy: 0.0000e+00\n",
      "Epoch 2/120\n",
      "5/5 [==============================] - 1s 215ms/step - loss: 20.0985 - decoder_output_loss: 1.6146 - decoder_output_1_loss: 1.4935 - decoder_output_2_loss: 2.2222 - decoder_output_3_loss: 2.9096 - decoder_output_4_loss: 1.3248 - decoder_output_5_loss: 1.6896 - decoder_output_6_loss: 2.7579 - decoder_output_7_loss: 1.2557 - decoder_output_8_loss: 2.1325 - decoder_output_9_loss: 2.6980 - decoder_output_accuracy: 0.4892 - decoder_output_1_accuracy: 0.5367 - decoder_output_2_accuracy: 0.2098 - decoder_output_3_accuracy: 0.1204 - decoder_output_4_accuracy: 0.6839 - decoder_output_5_accuracy: 0.0000e+00 - decoder_output_6_accuracy: 0.0000e+00 - decoder_output_7_accuracy: 1.0000 - decoder_output_8_accuracy: 0.0040 - decoder_output_9_accuracy: 0.0111 - val_loss: 19.0921 - val_decoder_output_loss: 1.0965 - val_decoder_output_1_loss: 1.3537 - val_decoder_output_2_loss: 2.1058 - val_decoder_output_3_loss: 3.0199 - val_decoder_output_4_loss: 1.0899 - val_decoder_output_5_loss: 1.6497 - val_decoder_output_6_loss: 2.7207 - val_decoder_output_7_loss: 1.5714 - val_decoder_output_8_loss: 1.8576 - val_decoder_output_9_loss: 2.6270 - val_decoder_output_accuracy: 0.5930 - val_decoder_output_1_accuracy: 0.5930 - val_decoder_output_2_accuracy: 0.1820 - val_decoder_output_3_accuracy: 0.1200 - val_decoder_output_4_accuracy: 1.0000 - val_decoder_output_5_accuracy: 0.0000e+00 - val_decoder_output_6_accuracy: 0.0000e+00 - val_decoder_output_7_accuracy: 0.9920 - val_decoder_output_8_accuracy: 0.3450 - val_decoder_output_9_accuracy: 0.1190\n",
      "Epoch 3/120\n",
      "5/5 [==============================] - 1s 164ms/step - loss: 18.6615 - decoder_output_loss: 0.9725 - decoder_output_1_loss: 1.2798 - decoder_output_2_loss: 1.9903 - decoder_output_3_loss: 2.9925 - decoder_output_4_loss: 0.9867 - decoder_output_5_loss: 1.7076 - decoder_output_6_loss: 2.8071 - decoder_output_7_loss: 1.3821 - decoder_output_8_loss: 1.8396 - decoder_output_9_loss: 2.7032 - decoder_output_accuracy: 0.6047 - decoder_output_1_accuracy: 0.5840 - decoder_output_2_accuracy: 0.2008 - decoder_output_3_accuracy: 0.0692 - decoder_output_4_accuracy: 1.0000 - decoder_output_5_accuracy: 0.0000e+00 - decoder_output_6_accuracy: 0.0000e+00 - decoder_output_7_accuracy: 0.9859 - decoder_output_8_accuracy: 0.1798 - decoder_output_9_accuracy: 0.1000 - val_loss: 17.9283 - val_decoder_output_loss: 0.8046 - val_decoder_output_1_loss: 1.0352 - val_decoder_output_2_loss: 1.9106 - val_decoder_output_3_loss: 2.8750 - val_decoder_output_4_loss: 0.9453 - val_decoder_output_5_loss: 1.5709 - val_decoder_output_6_loss: 2.8658 - val_decoder_output_7_loss: 1.5056 - val_decoder_output_8_loss: 1.7415 - val_decoder_output_9_loss: 2.6737 - val_decoder_output_accuracy: 0.5930 - val_decoder_output_1_accuracy: 0.5930 - val_decoder_output_2_accuracy: 0.2870 - val_decoder_output_3_accuracy: 0.0000e+00 - val_decoder_output_4_accuracy: 1.0000 - val_decoder_output_5_accuracy: 0.0000e+00 - val_decoder_output_6_accuracy: 0.0000e+00 - val_decoder_output_7_accuracy: 0.9980 - val_decoder_output_8_accuracy: 0.3450 - val_decoder_output_9_accuracy: 0.1190\n",
      "Epoch 4/120\n",
      "5/5 [==============================] - 1s 201ms/step - loss: 17.7215 - decoder_output_loss: 0.7425 - decoder_output_1_loss: 0.9510 - decoder_output_2_loss: 1.9196 - decoder_output_3_loss: 2.8516 - decoder_output_4_loss: 0.8801 - decoder_output_5_loss: 1.6497 - decoder_output_6_loss: 2.8934 - decoder_output_7_loss: 1.3591 - decoder_output_8_loss: 1.8169 - decoder_output_9_loss: 2.6576 - decoder_output_accuracy: 0.6159 - decoder_output_1_accuracy: 0.5981 - decoder_output_2_accuracy: 0.2477 - decoder_output_3_accuracy: 0.0710 - decoder_output_4_accuracy: 0.9780 - decoder_output_5_accuracy: 0.0000e+00 - decoder_output_6_accuracy: 0.0030 - decoder_output_7_accuracy: 0.9164 - decoder_output_8_accuracy: 0.2151 - decoder_output_9_accuracy: 0.1189 - val_loss: 17.2563 - val_decoder_output_loss: 0.7266 - val_decoder_output_1_loss: 0.8694 - val_decoder_output_2_loss: 1.7745 - val_decoder_output_3_loss: 2.6831 - val_decoder_output_4_loss: 0.8770 - val_decoder_output_5_loss: 1.6102 - val_decoder_output_6_loss: 3.0066 - val_decoder_output_7_loss: 1.1706 - val_decoder_output_8_loss: 1.9457 - val_decoder_output_9_loss: 2.5927 - val_decoder_output_accuracy: 0.6090 - val_decoder_output_1_accuracy: 0.6100 - val_decoder_output_2_accuracy: 0.2380 - val_decoder_output_3_accuracy: 0.0000e+00 - val_decoder_output_4_accuracy: 1.0000 - val_decoder_output_5_accuracy: 0.0000e+00 - val_decoder_output_6_accuracy: 0.0000e+00 - val_decoder_output_7_accuracy: 1.0000 - val_decoder_output_8_accuracy: 0.0000e+00 - val_decoder_output_9_accuracy: 0.1380\n",
      "Epoch 5/120\n",
      "5/5 [==============================] - 1s 168ms/step - loss: 17.1136 - decoder_output_loss: 0.6402 - decoder_output_1_loss: 0.7765 - decoder_output_2_loss: 1.7900 - decoder_output_3_loss: 2.8819 - decoder_output_4_loss: 0.7572 - decoder_output_5_loss: 1.4946 - decoder_output_6_loss: 3.1141 - decoder_output_7_loss: 1.2190 - decoder_output_8_loss: 1.8258 - decoder_output_9_loss: 2.6143 - decoder_output_accuracy: 0.7387 - decoder_output_1_accuracy: 0.7004 - decoder_output_2_accuracy: 0.2126 - decoder_output_3_accuracy: 0.0463 - decoder_output_4_accuracy: 1.0000 - decoder_output_5_accuracy: 0.0000e+00 - decoder_output_6_accuracy: 0.0000e+00 - decoder_output_7_accuracy: 1.0000 - decoder_output_8_accuracy: 0.1880 - decoder_output_9_accuracy: 0.1047 - val_loss: 16.6247 - val_decoder_output_loss: 0.5923 - val_decoder_output_1_loss: 0.6909 - val_decoder_output_2_loss: 1.7473 - val_decoder_output_3_loss: 2.5666 - val_decoder_output_4_loss: 0.6891 - val_decoder_output_5_loss: 1.6342 - val_decoder_output_6_loss: 3.1336 - val_decoder_output_7_loss: 1.3405 - val_decoder_output_8_loss: 1.7483 - val_decoder_output_9_loss: 2.4819 - val_decoder_output_accuracy: 0.7200 - val_decoder_output_1_accuracy: 0.6350 - val_decoder_output_2_accuracy: 0.2130 - val_decoder_output_3_accuracy: 0.1280 - val_decoder_output_4_accuracy: 1.0000 - val_decoder_output_5_accuracy: 0.0000e+00 - val_decoder_output_6_accuracy: 0.0000e+00 - val_decoder_output_7_accuracy: 0.9160 - val_decoder_output_8_accuracy: 0.3450 - val_decoder_output_9_accuracy: 0.0960\n",
      "Epoch 6/120\n",
      "5/5 [==============================] - 1s 177ms/step - loss: 16.1872 - decoder_output_loss: 0.5211 - decoder_output_1_loss: 0.6252 - decoder_output_2_loss: 1.6784 - decoder_output_3_loss: 2.5761 - decoder_output_4_loss: 0.6646 - decoder_output_5_loss: 1.4159 - decoder_output_6_loss: 3.0559 - decoder_output_7_loss: 1.3611 - decoder_output_8_loss: 1.8164 - decoder_output_9_loss: 2.4727 - decoder_output_accuracy: 0.8213 - decoder_output_1_accuracy: 0.7597 - decoder_output_2_accuracy: 0.2904 - decoder_output_3_accuracy: 0.0577 - decoder_output_4_accuracy: 1.0000 - decoder_output_5_accuracy: 0.1110 - decoder_output_6_accuracy: 0.0010 - decoder_output_7_accuracy: 0.6922 - decoder_output_8_accuracy: 0.2417 - decoder_output_9_accuracy: 0.1071 - val_loss: 15.4461 - val_decoder_output_loss: 0.4213 - val_decoder_output_1_loss: 0.4491 - val_decoder_output_2_loss: 1.5322 - val_decoder_output_3_loss: 2.5840 - val_decoder_output_4_loss: 0.5147 - val_decoder_output_5_loss: 1.2451 - val_decoder_output_6_loss: 3.1855 - val_decoder_output_7_loss: 1.0975 - val_decoder_output_8_loss: 1.9424 - val_decoder_output_9_loss: 2.4743 - val_decoder_output_accuracy: 0.8250 - val_decoder_output_1_accuracy: 0.8660 - val_decoder_output_2_accuracy: 0.3310 - val_decoder_output_3_accuracy: 0.0530 - val_decoder_output_4_accuracy: 1.0000 - val_decoder_output_5_accuracy: 0.6460 - val_decoder_output_6_accuracy: 0.0000e+00 - val_decoder_output_7_accuracy: 1.0000 - val_decoder_output_8_accuracy: 0.0040 - val_decoder_output_9_accuracy: 0.0960\n",
      "Epoch 7/120\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 14.9577 - decoder_output_loss: 0.3511 - decoder_output_1_loss: 0.3780 - decoder_output_2_loss: 1.4656 - decoder_output_3_loss: 2.5257 - decoder_output_4_loss: 0.4577 - decoder_output_5_loss: 1.1962 - decoder_output_6_loss: 3.0836 - decoder_output_7_loss: 1.2361 - decoder_output_8_loss: 1.8393 - decoder_output_9_loss: 2.4245 - decoder_output_accuracy: 0.8797 - decoder_output_1_accuracy: 0.8943 - decoder_output_2_accuracy: 0.3540 - decoder_output_3_accuracy: 0.0953 - decoder_output_4_accuracy: 1.0000 - decoder_output_5_accuracy: 0.7216 - decoder_output_6_accuracy: 7.7778e-04 - decoder_output_7_accuracy: 0.9271 - decoder_output_8_accuracy: 0.2344 - decoder_output_9_accuracy: 0.1002 - val_loss: 14.2186 - val_decoder_output_loss: 0.2712 - val_decoder_output_1_loss: 0.2816 - val_decoder_output_2_loss: 1.3667 - val_decoder_output_3_loss: 2.4939 - val_decoder_output_4_loss: 0.2835 - val_decoder_output_5_loss: 1.0198 - val_decoder_output_6_loss: 3.1177 - val_decoder_output_7_loss: 1.1833 - val_decoder_output_8_loss: 1.7449 - val_decoder_output_9_loss: 2.4559 - val_decoder_output_accuracy: 0.8990 - val_decoder_output_1_accuracy: 0.8990 - val_decoder_output_2_accuracy: 0.3900 - val_decoder_output_3_accuracy: 0.1250 - val_decoder_output_4_accuracy: 1.0000 - val_decoder_output_5_accuracy: 0.7540 - val_decoder_output_6_accuracy: 0.0000e+00 - val_decoder_output_7_accuracy: 0.8050 - val_decoder_output_8_accuracy: 0.3450 - val_decoder_output_9_accuracy: 0.1190\n",
      "Epoch 8/120\n",
      "5/5 [==============================] - 1s 163ms/step - loss: 13.6548 - decoder_output_loss: 0.2174 - decoder_output_1_loss: 0.2194 - decoder_output_2_loss: 1.2998 - decoder_output_3_loss: 2.4584 - decoder_output_4_loss: 0.2583 - decoder_output_5_loss: 0.9158 - decoder_output_6_loss: 3.0400 - decoder_output_7_loss: 1.0273 - decoder_output_8_loss: 1.7997 - decoder_output_9_loss: 2.4185 - decoder_output_accuracy: 0.9291 - decoder_output_1_accuracy: 0.9291 - decoder_output_2_accuracy: 0.4084 - decoder_output_3_accuracy: 0.1274 - decoder_output_4_accuracy: 1.0000 - decoder_output_5_accuracy: 0.7479 - decoder_output_6_accuracy: 0.0000e+00 - decoder_output_7_accuracy: 0.9531 - decoder_output_8_accuracy: 0.2473 - decoder_output_9_accuracy: 0.1134 - val_loss: 12.6624 - val_decoder_output_loss: 0.1685 - val_decoder_output_1_loss: 0.1677 - val_decoder_output_2_loss: 1.2364 - val_decoder_output_3_loss: 2.3617 - val_decoder_output_4_loss: 0.1775 - val_decoder_output_5_loss: 0.7523 - val_decoder_output_6_loss: 2.9063 - val_decoder_output_7_loss: 0.7380 - val_decoder_output_8_loss: 1.8072 - val_decoder_output_9_loss: 2.3467 - val_decoder_output_accuracy: 0.9420 - val_decoder_output_1_accuracy: 0.9420 - val_decoder_output_2_accuracy: 0.4130 - val_decoder_output_3_accuracy: 0.1490 - val_decoder_output_4_accuracy: 1.0000 - val_decoder_output_5_accuracy: 0.7540 - val_decoder_output_6_accuracy: 0.0000e+00 - val_decoder_output_7_accuracy: 1.0000 - val_decoder_output_8_accuracy: 0.2710 - val_decoder_output_9_accuracy: 0.1110\n",
      "Epoch 9/120\n",
      "5/5 [==============================] - 1s 182ms/step - loss: 11.9975 - decoder_output_loss: 0.1548 - decoder_output_1_loss: 0.1591 - decoder_output_2_loss: 1.2285 - decoder_output_3_loss: 2.3815 - decoder_output_4_loss: 0.1356 - decoder_output_5_loss: 0.6996 - decoder_output_6_loss: 2.7441 - decoder_output_7_loss: 0.5152 - decoder_output_8_loss: 1.6296 - decoder_output_9_loss: 2.3497 - decoder_output_accuracy: 0.9482 - decoder_output_1_accuracy: 0.9480 - decoder_output_2_accuracy: 0.4228 - decoder_output_3_accuracy: 0.1406 - decoder_output_4_accuracy: 1.0000 - decoder_output_5_accuracy: 0.7479 - decoder_output_6_accuracy: 0.0127 - decoder_output_7_accuracy: 1.0000 - decoder_output_8_accuracy: 0.3444 - decoder_output_9_accuracy: 0.1187 - val_loss: 10.8717 - val_decoder_output_loss: 0.1567 - val_decoder_output_1_loss: 0.1925 - val_decoder_output_2_loss: 1.2086 - val_decoder_output_3_loss: 2.3172 - val_decoder_output_4_loss: 0.0752 - val_decoder_output_5_loss: 0.6038 - val_decoder_output_6_loss: 2.4650 - val_decoder_output_7_loss: 0.1387 - val_decoder_output_8_loss: 1.3950 - val_decoder_output_9_loss: 2.3189 - val_decoder_output_accuracy: 0.9330 - val_decoder_output_1_accuracy: 0.9330 - val_decoder_output_2_accuracy: 0.4230 - val_decoder_output_3_accuracy: 0.1480 - val_decoder_output_4_accuracy: 1.0000 - val_decoder_output_5_accuracy: 0.7540 - val_decoder_output_6_accuracy: 0.0970 - val_decoder_output_7_accuracy: 1.0000 - val_decoder_output_8_accuracy: 0.3940 - val_decoder_output_9_accuracy: 0.1120\n",
      "Epoch 10/120\n",
      "5/5 [==============================] - 1s 163ms/step - loss: 10.5566 - decoder_output_loss: 0.1339 - decoder_output_1_loss: 0.1470 - decoder_output_2_loss: 1.1899 - decoder_output_3_loss: 2.3389 - decoder_output_4_loss: 0.0609 - decoder_output_5_loss: 0.5914 - decoder_output_6_loss: 2.3592 - decoder_output_7_loss: 0.0917 - decoder_output_8_loss: 1.3314 - decoder_output_9_loss: 2.3123 - decoder_output_accuracy: 0.9499 - decoder_output_1_accuracy: 0.9504 - decoder_output_2_accuracy: 0.4442 - decoder_output_3_accuracy: 0.1339 - decoder_output_4_accuracy: 1.0000 - decoder_output_5_accuracy: 0.7479 - decoder_output_6_accuracy: 0.1578 - decoder_output_7_accuracy: 1.0000 - decoder_output_8_accuracy: 0.4039 - decoder_output_9_accuracy: 0.1139 - val_loss: 10.2512 - val_decoder_output_loss: 0.1213 - val_decoder_output_1_loss: 0.1304 - val_decoder_output_2_loss: 1.1655 - val_decoder_output_3_loss: 2.3235 - val_decoder_output_4_loss: 0.0369 - val_decoder_output_5_loss: 0.5589 - val_decoder_output_6_loss: 2.2681 - val_decoder_output_7_loss: 0.0346 - val_decoder_output_8_loss: 1.3076 - val_decoder_output_9_loss: 2.3044 - val_decoder_output_accuracy: 0.9580 - val_decoder_output_1_accuracy: 0.9560 - val_decoder_output_2_accuracy: 0.4190 - val_decoder_output_3_accuracy: 0.1410 - val_decoder_output_4_accuracy: 1.0000 - val_decoder_output_5_accuracy: 0.7540 - val_decoder_output_6_accuracy: 0.1840 - val_decoder_output_7_accuracy: 1.0000 - val_decoder_output_8_accuracy: 0.4310 - val_decoder_output_9_accuracy: 0.1430\n",
      "Epoch 11/120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 1s 166ms/step - loss: 10.0881 - decoder_output_loss: 0.1092 - decoder_output_1_loss: 0.1177 - decoder_output_2_loss: 1.1443 - decoder_output_3_loss: 2.3131 - decoder_output_4_loss: 0.0275 - decoder_output_5_loss: 0.5353 - decoder_output_6_loss: 2.2269 - decoder_output_7_loss: 0.0268 - decoder_output_8_loss: 1.2899 - decoder_output_9_loss: 2.2974 - decoder_output_accuracy: 0.9607 - decoder_output_1_accuracy: 0.9609 - decoder_output_2_accuracy: 0.4514 - decoder_output_3_accuracy: 0.1442 - decoder_output_4_accuracy: 1.0000 - decoder_output_5_accuracy: 0.7479 - decoder_output_6_accuracy: 0.1850 - decoder_output_7_accuracy: 1.0000 - decoder_output_8_accuracy: 0.3712 - decoder_output_9_accuracy: 0.1473 - val_loss: 9.9245 - val_decoder_output_loss: 0.1123 - val_decoder_output_1_loss: 0.1184 - val_decoder_output_2_loss: 1.1148 - val_decoder_output_3_loss: 2.2856 - val_decoder_output_4_loss: 0.0194 - val_decoder_output_5_loss: 0.5020 - val_decoder_output_6_loss: 2.1947 - val_decoder_output_7_loss: 0.0154 - val_decoder_output_8_loss: 1.2865 - val_decoder_output_9_loss: 2.2754 - val_decoder_output_accuracy: 0.9550 - val_decoder_output_1_accuracy: 0.9580 - val_decoder_output_2_accuracy: 0.4560 - val_decoder_output_3_accuracy: 0.1300 - val_decoder_output_4_accuracy: 1.0000 - val_decoder_output_5_accuracy: 0.7600 - val_decoder_output_6_accuracy: 0.1600 - val_decoder_output_7_accuracy: 1.0000 - val_decoder_output_8_accuracy: 0.3910 - val_decoder_output_9_accuracy: 0.1440\n",
      "Epoch 12/120\n",
      "5/5 [==============================] - 1s 176ms/step - loss: 9.8113 - decoder_output_loss: 0.1001 - decoder_output_1_loss: 0.1060 - decoder_output_2_loss: 1.1167 - decoder_output_3_loss: 2.2890 - decoder_output_4_loss: 0.0181 - decoder_output_5_loss: 0.4635 - decoder_output_6_loss: 2.1640 - decoder_output_7_loss: 0.0122 - decoder_output_8_loss: 1.2541 - decoder_output_9_loss: 2.2876 - decoder_output_accuracy: 0.9629 - decoder_output_1_accuracy: 0.9631 - decoder_output_2_accuracy: 0.4542 - decoder_output_3_accuracy: 0.1393 - decoder_output_4_accuracy: 1.0000 - decoder_output_5_accuracy: 0.7744 - decoder_output_6_accuracy: 0.1927 - decoder_output_7_accuracy: 1.0000 - decoder_output_8_accuracy: 0.4327 - decoder_output_9_accuracy: 0.1347 - val_loss: 9.6611 - val_decoder_output_loss: 0.1032 - val_decoder_output_1_loss: 0.1031 - val_decoder_output_2_loss: 1.0967 - val_decoder_output_3_loss: 2.2792 - val_decoder_output_4_loss: 0.0123 - val_decoder_output_5_loss: 0.4143 - val_decoder_output_6_loss: 2.1387 - val_decoder_output_7_loss: 0.0082 - val_decoder_output_8_loss: 1.2345 - val_decoder_output_9_loss: 2.2709 - val_decoder_output_accuracy: 0.9610 - val_decoder_output_1_accuracy: 0.9600 - val_decoder_output_2_accuracy: 0.4810 - val_decoder_output_3_accuracy: 0.1320 - val_decoder_output_4_accuracy: 1.0000 - val_decoder_output_5_accuracy: 0.8150 - val_decoder_output_6_accuracy: 0.1840 - val_decoder_output_7_accuracy: 1.0000 - val_decoder_output_8_accuracy: 0.4800 - val_decoder_output_9_accuracy: 0.1500\n",
      "Epoch 13/120\n",
      "5/5 [==============================] - 1s 174ms/step - loss: 9.5467 - decoder_output_loss: 0.0948 - decoder_output_1_loss: 0.0926 - decoder_output_2_loss: 1.0973 - decoder_output_3_loss: 2.2739 - decoder_output_4_loss: 0.0112 - decoder_output_5_loss: 0.3829 - decoder_output_6_loss: 2.1033 - decoder_output_7_loss: 0.0069 - decoder_output_8_loss: 1.2112 - decoder_output_9_loss: 2.2727 - decoder_output_accuracy: 0.9659 - decoder_output_1_accuracy: 0.9658 - decoder_output_2_accuracy: 0.4817 - decoder_output_3_accuracy: 0.1472 - decoder_output_4_accuracy: 1.0000 - decoder_output_5_accuracy: 0.8416 - decoder_output_6_accuracy: 0.2067 - decoder_output_7_accuracy: 1.0000 - decoder_output_8_accuracy: 0.4492 - decoder_output_9_accuracy: 0.1449 - val_loss: 9.5565 - val_decoder_output_loss: 0.0994 - val_decoder_output_1_loss: 0.1018 - val_decoder_output_2_loss: 1.0730 - val_decoder_output_3_loss: 2.2891 - val_decoder_output_4_loss: 0.0116 - val_decoder_output_5_loss: 0.4071 - val_decoder_output_6_loss: 2.1013 - val_decoder_output_7_loss: 0.0048 - val_decoder_output_8_loss: 1.2095 - val_decoder_output_9_loss: 2.2589 - val_decoder_output_accuracy: 0.9570 - val_decoder_output_1_accuracy: 0.9570 - val_decoder_output_2_accuracy: 0.5020 - val_decoder_output_3_accuracy: 0.1470 - val_decoder_output_4_accuracy: 1.0000 - val_decoder_output_5_accuracy: 0.8210 - val_decoder_output_6_accuracy: 0.1560 - val_decoder_output_7_accuracy: 1.0000 - val_decoder_output_8_accuracy: 0.4930 - val_decoder_output_9_accuracy: 0.1640\n",
      "Epoch 14/120\n",
      "5/5 [==============================] - 1s 164ms/step - loss: 9.3670 - decoder_output_loss: 0.0867 - decoder_output_1_loss: 0.0881 - decoder_output_2_loss: 1.0771 - decoder_output_3_loss: 2.2479 - decoder_output_4_loss: 0.0109 - decoder_output_5_loss: 0.3443 - decoder_output_6_loss: 2.0682 - decoder_output_7_loss: 0.0040 - decoder_output_8_loss: 1.1763 - decoder_output_9_loss: 2.2635 - decoder_output_accuracy: 0.9672 - decoder_output_1_accuracy: 0.9676 - decoder_output_2_accuracy: 0.4803 - decoder_output_3_accuracy: 0.1598 - decoder_output_4_accuracy: 1.0000 - decoder_output_5_accuracy: 0.8640 - decoder_output_6_accuracy: 0.2270 - decoder_output_7_accuracy: 1.0000 - decoder_output_8_accuracy: 0.4879 - decoder_output_9_accuracy: 0.1543 - val_loss: 9.2145 - val_decoder_output_loss: 0.0874 - val_decoder_output_1_loss: 0.0876 - val_decoder_output_2_loss: 1.0572 - val_decoder_output_3_loss: 2.2302 - val_decoder_output_4_loss: 0.0080 - val_decoder_output_5_loss: 0.2891 - val_decoder_output_6_loss: 2.0498 - val_decoder_output_7_loss: 0.0034 - val_decoder_output_8_loss: 1.1638 - val_decoder_output_9_loss: 2.2380 - val_decoder_output_accuracy: 0.9630 - val_decoder_output_1_accuracy: 0.9630 - val_decoder_output_2_accuracy: 0.5050 - val_decoder_output_3_accuracy: 0.1710 - val_decoder_output_4_accuracy: 1.0000 - val_decoder_output_5_accuracy: 0.8790 - val_decoder_output_6_accuracy: 0.2910 - val_decoder_output_7_accuracy: 1.0000 - val_decoder_output_8_accuracy: 0.4990 - val_decoder_output_9_accuracy: 0.1650\n",
      "Epoch 15/120\n",
      "5/5 [==============================] - 1s 169ms/step - loss: 9.0819 - decoder_output_loss: 0.0840 - decoder_output_1_loss: 0.0868 - decoder_output_2_loss: 1.0699 - decoder_output_3_loss: 2.2199 - decoder_output_4_loss: 0.0102 - decoder_output_5_loss: 0.2591 - decoder_output_6_loss: 1.9798 - decoder_output_7_loss: 0.0053 - decoder_output_8_loss: 1.1263 - decoder_output_9_loss: 2.2405 - decoder_output_accuracy: 0.9683 - decoder_output_1_accuracy: 0.9682 - decoder_output_2_accuracy: 0.4872 - decoder_output_3_accuracy: 0.1672 - decoder_output_4_accuracy: 0.9999 - decoder_output_5_accuracy: 0.9098 - decoder_output_6_accuracy: 0.2760 - decoder_output_7_accuracy: 0.9998 - decoder_output_8_accuracy: 0.5021 - decoder_output_9_accuracy: 0.1660 - val_loss: 8.9535 - val_decoder_output_loss: 0.0834 - val_decoder_output_1_loss: 0.0854 - val_decoder_output_2_loss: 1.0423 - val_decoder_output_3_loss: 2.2205 - val_decoder_output_4_loss: 0.0113 - val_decoder_output_5_loss: 0.2202 - val_decoder_output_6_loss: 1.9549 - val_decoder_output_7_loss: 0.0052 - val_decoder_output_8_loss: 1.1112 - val_decoder_output_9_loss: 2.2192 - val_decoder_output_accuracy: 0.9690 - val_decoder_output_1_accuracy: 0.9690 - val_decoder_output_2_accuracy: 0.5130 - val_decoder_output_3_accuracy: 0.1710 - val_decoder_output_4_accuracy: 1.0000 - val_decoder_output_5_accuracy: 0.9250 - val_decoder_output_6_accuracy: 0.2360 - val_decoder_output_7_accuracy: 1.0000 - val_decoder_output_8_accuracy: 0.5250 - val_decoder_output_9_accuracy: 0.1780\n",
      "Epoch 16/120\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 8.8780 - decoder_output_loss: 0.0826 - decoder_output_1_loss: 0.0853 - decoder_output_2_loss: 1.0490 - decoder_output_3_loss: 2.1928 - decoder_output_4_loss: 0.0107 - decoder_output_5_loss: 0.2080 - decoder_output_6_loss: 1.9204 - decoder_output_7_loss: 0.0063 - decoder_output_8_loss: 1.0991 - decoder_output_9_loss: 2.2240 - decoder_output_accuracy: 0.9679 - decoder_output_1_accuracy: 0.9684 - decoder_output_2_accuracy: 0.5044 - decoder_output_3_accuracy: 0.1899 - decoder_output_4_accuracy: 1.0000 - decoder_output_5_accuracy: 0.9302 - decoder_output_6_accuracy: 0.2649 - decoder_output_7_accuracy: 0.9997 - decoder_output_8_accuracy: 0.5161 - decoder_output_9_accuracy: 0.1696 - val_loss: 8.7989 - val_decoder_output_loss: 0.0834 - val_decoder_output_1_loss: 0.0872 - val_decoder_output_2_loss: 1.0257 - val_decoder_output_3_loss: 2.1651 - val_decoder_output_4_loss: 0.0079 - val_decoder_output_5_loss: 0.2176 - val_decoder_output_6_loss: 1.9140 - val_decoder_output_7_loss: 0.0053 - val_decoder_output_8_loss: 1.0845 - val_decoder_output_9_loss: 2.2081 - val_decoder_output_accuracy: 0.9680 - val_decoder_output_1_accuracy: 0.9700 - val_decoder_output_2_accuracy: 0.5170 - val_decoder_output_3_accuracy: 0.2060 - val_decoder_output_4_accuracy: 1.0000 - val_decoder_output_5_accuracy: 0.9200 - val_decoder_output_6_accuracy: 0.2780 - val_decoder_output_7_accuracy: 1.0000 - val_decoder_output_8_accuracy: 0.5400 - val_decoder_output_9_accuracy: 0.1900\n",
      "Epoch 17/120\n",
      "5/5 [==============================] - 1s 177ms/step - loss: 8.6958 - decoder_output_loss: 0.0797 - decoder_output_1_loss: 0.0834 - decoder_output_2_loss: 1.0297 - decoder_output_3_loss: 2.1437 - decoder_output_4_loss: 0.0084 - decoder_output_5_loss: 0.1967 - decoder_output_6_loss: 1.8703 - decoder_output_7_loss: 0.0056 - decoder_output_8_loss: 1.0709 - decoder_output_9_loss: 2.2073 - decoder_output_accuracy: 0.9693 - decoder_output_1_accuracy: 0.9691 - decoder_output_2_accuracy: 0.5189 - decoder_output_3_accuracy: 0.2161 - decoder_output_4_accuracy: 1.0000 - decoder_output_5_accuracy: 0.9379 - decoder_output_6_accuracy: 0.3181 - decoder_output_7_accuracy: 0.9999 - decoder_output_8_accuracy: 0.5243 - decoder_output_9_accuracy: 0.1682 - val_loss: 8.5960 - val_decoder_output_loss: 0.0805 - val_decoder_output_1_loss: 0.0852 - val_decoder_output_2_loss: 0.9969 - val_decoder_output_3_loss: 2.1369 - val_decoder_output_4_loss: 0.0084 - val_decoder_output_5_loss: 0.1821 - val_decoder_output_6_loss: 1.8619 - val_decoder_output_7_loss: 0.0044 - val_decoder_output_8_loss: 1.0600 - val_decoder_output_9_loss: 2.1797 - val_decoder_output_accuracy: 0.9640 - val_decoder_output_1_accuracy: 0.9650 - val_decoder_output_2_accuracy: 0.5550 - val_decoder_output_3_accuracy: 0.2100 - val_decoder_output_4_accuracy: 1.0000 - val_decoder_output_5_accuracy: 0.9350 - val_decoder_output_6_accuracy: 0.2900 - val_decoder_output_7_accuracy: 1.0000 - val_decoder_output_8_accuracy: 0.5380 - val_decoder_output_9_accuracy: 0.1850\n",
      "Epoch 18/120\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 8.5046 - decoder_output_loss: 0.0751 - decoder_output_1_loss: 0.0779 - decoder_output_2_loss: 1.0096 - decoder_output_3_loss: 2.1103 - decoder_output_4_loss: 0.0088 - decoder_output_5_loss: 0.1698 - decoder_output_6_loss: 1.8174 - decoder_output_7_loss: 0.0061 - decoder_output_8_loss: 1.0418 - decoder_output_9_loss: 2.1879 - decoder_output_accuracy: 0.9716 - decoder_output_1_accuracy: 0.9714 - decoder_output_2_accuracy: 0.5438 - decoder_output_3_accuracy: 0.2261 - decoder_output_4_accuracy: 1.0000 - decoder_output_5_accuracy: 0.9422 - decoder_output_6_accuracy: 0.3178 - decoder_output_7_accuracy: 0.9998 - decoder_output_8_accuracy: 0.5398 - decoder_output_9_accuracy: 0.1814 - val_loss: 8.4515 - val_decoder_output_loss: 0.0796 - val_decoder_output_1_loss: 0.0824 - val_decoder_output_2_loss: 0.9846 - val_decoder_output_3_loss: 2.1022 - val_decoder_output_4_loss: 0.0091 - val_decoder_output_5_loss: 0.1728 - val_decoder_output_6_loss: 1.8024 - val_decoder_output_7_loss: 0.0074 - val_decoder_output_8_loss: 1.0500 - val_decoder_output_9_loss: 2.1611 - val_decoder_output_accuracy: 0.9670 - val_decoder_output_1_accuracy: 0.9670 - val_decoder_output_2_accuracy: 0.5540 - val_decoder_output_3_accuracy: 0.2260 - val_decoder_output_4_accuracy: 1.0000 - val_decoder_output_5_accuracy: 0.9400 - val_decoder_output_6_accuracy: 0.3220 - val_decoder_output_7_accuracy: 1.0000 - val_decoder_output_8_accuracy: 0.5510 - val_decoder_output_9_accuracy: 0.2010\n",
      "Epoch 19/120\n",
      "5/5 [==============================] - 1s 187ms/step - loss: 8.3360 - decoder_output_loss: 0.0718 - decoder_output_1_loss: 0.0743 - decoder_output_2_loss: 0.9940 - decoder_output_3_loss: 2.0742 - decoder_output_4_loss: 0.0093 - decoder_output_5_loss: 0.1536 - decoder_output_6_loss: 1.7606 - decoder_output_7_loss: 0.0072 - decoder_output_8_loss: 1.0223 - decoder_output_9_loss: 2.1688 - decoder_output_accuracy: 0.9721 - decoder_output_1_accuracy: 0.9721 - decoder_output_2_accuracy: 0.5508 - decoder_output_3_accuracy: 0.2406 - decoder_output_4_accuracy: 1.0000 - decoder_output_5_accuracy: 0.9480 - decoder_output_6_accuracy: 0.3630 - decoder_output_7_accuracy: 1.0000 - decoder_output_8_accuracy: 0.5537 - decoder_output_9_accuracy: 0.1987 - val_loss: 8.3208 - val_decoder_output_loss: 0.0741 - val_decoder_output_1_loss: 0.0762 - val_decoder_output_2_loss: 0.9623 - val_decoder_output_3_loss: 2.0886 - val_decoder_output_4_loss: 0.0071 - val_decoder_output_5_loss: 0.1555 - val_decoder_output_6_loss: 1.7407 - val_decoder_output_7_loss: 0.0059 - val_decoder_output_8_loss: 1.0678 - val_decoder_output_9_loss: 2.1424 - val_decoder_output_accuracy: 0.9660 - val_decoder_output_1_accuracy: 0.9670 - val_decoder_output_2_accuracy: 0.5690 - val_decoder_output_3_accuracy: 0.2270 - val_decoder_output_4_accuracy: 1.0000 - val_decoder_output_5_accuracy: 0.9460 - val_decoder_output_6_accuracy: 0.3450 - val_decoder_output_7_accuracy: 1.0000 - val_decoder_output_8_accuracy: 0.5420 - val_decoder_output_9_accuracy: 0.2140\n",
      "Epoch 20/120\n",
      "5/5 [==============================] - 1s 242ms/step - loss: 8.2401 - decoder_output_loss: 0.0702 - decoder_output_1_loss: 0.0729 - decoder_output_2_loss: 0.9757 - decoder_output_3_loss: 2.0491 - decoder_output_4_loss: 0.0077 - decoder_output_5_loss: 0.1407 - decoder_output_6_loss: 1.7032 - decoder_output_7_loss: 0.0065 - decoder_output_8_loss: 1.0521 - decoder_output_9_loss: 2.1620 - decoder_output_accuracy: 0.9721 - decoder_output_1_accuracy: 0.9723 - decoder_output_2_accuracy: 0.5699 - decoder_output_3_accuracy: 0.2573 - decoder_output_4_accuracy: 1.0000 - decoder_output_5_accuracy: 0.9574 - decoder_output_6_accuracy: 0.3759 - decoder_output_7_accuracy: 1.0000 - decoder_output_8_accuracy: 0.5364 - decoder_output_9_accuracy: 0.2013 - val_loss: 8.1802 - val_decoder_output_loss: 0.0720 - val_decoder_output_1_loss: 0.0747 - val_decoder_output_2_loss: 0.9462 - val_decoder_output_3_loss: 2.0679 - val_decoder_output_4_loss: 0.0074 - val_decoder_output_5_loss: 0.1561 - val_decoder_output_6_loss: 1.6831 - val_decoder_output_7_loss: 0.0053 - val_decoder_output_8_loss: 1.0372 - val_decoder_output_9_loss: 2.1303 - val_decoder_output_accuracy: 0.9660 - val_decoder_output_1_accuracy: 0.9680 - val_decoder_output_2_accuracy: 0.5740 - val_decoder_output_3_accuracy: 0.2510 - val_decoder_output_4_accuracy: 1.0000 - val_decoder_output_5_accuracy: 0.9480 - val_decoder_output_6_accuracy: 0.4080 - val_decoder_output_7_accuracy: 1.0000 - val_decoder_output_8_accuracy: 0.5580 - val_decoder_output_9_accuracy: 0.2210\n",
      "Epoch 21/120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 1s 166ms/step - loss: 8.0810 - decoder_output_loss: 0.0686 - decoder_output_1_loss: 0.0724 - decoder_output_2_loss: 0.9577 - decoder_output_3_loss: 2.0284 - decoder_output_4_loss: 0.0085 - decoder_output_5_loss: 0.1398 - decoder_output_6_loss: 1.6450 - decoder_output_7_loss: 0.0065 - decoder_output_8_loss: 1.0139 - decoder_output_9_loss: 2.1403 - decoder_output_accuracy: 0.9726 - decoder_output_1_accuracy: 0.9719 - decoder_output_2_accuracy: 0.5810 - decoder_output_3_accuracy: 0.2643 - decoder_output_4_accuracy: 1.0000 - decoder_output_5_accuracy: 0.9543 - decoder_output_6_accuracy: 0.4212 - decoder_output_7_accuracy: 0.9999 - decoder_output_8_accuracy: 0.5618 - decoder_output_9_accuracy: 0.2089 - val_loss: 8.0297 - val_decoder_output_loss: 0.0786 - val_decoder_output_1_loss: 0.0784 - val_decoder_output_2_loss: 0.9380 - val_decoder_output_3_loss: 2.0405 - val_decoder_output_4_loss: 0.0060 - val_decoder_output_5_loss: 0.1374 - val_decoder_output_6_loss: 1.6141 - val_decoder_output_7_loss: 0.0044 - val_decoder_output_8_loss: 1.0180 - val_decoder_output_9_loss: 2.1145 - val_decoder_output_accuracy: 0.9640 - val_decoder_output_1_accuracy: 0.9630 - val_decoder_output_2_accuracy: 0.6050 - val_decoder_output_3_accuracy: 0.2620 - val_decoder_output_4_accuracy: 1.0000 - val_decoder_output_5_accuracy: 0.9570 - val_decoder_output_6_accuracy: 0.4390 - val_decoder_output_7_accuracy: 1.0000 - val_decoder_output_8_accuracy: 0.5590 - val_decoder_output_9_accuracy: 0.2300\n",
      "Epoch 22/120\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 7.9333 - decoder_output_loss: 0.0685 - decoder_output_1_loss: 0.0715 - decoder_output_2_loss: 0.9506 - decoder_output_3_loss: 2.0036 - decoder_output_4_loss: 0.0080 - decoder_output_5_loss: 0.1277 - decoder_output_6_loss: 1.5813 - decoder_output_7_loss: 0.0062 - decoder_output_8_loss: 0.9938 - decoder_output_9_loss: 2.1221 - decoder_output_accuracy: 0.9720 - decoder_output_1_accuracy: 0.9720 - decoder_output_2_accuracy: 0.5804 - decoder_output_3_accuracy: 0.2697 - decoder_output_4_accuracy: 1.0000 - decoder_output_5_accuracy: 0.9592 - decoder_output_6_accuracy: 0.4596 - decoder_output_7_accuracy: 1.0000 - decoder_output_8_accuracy: 0.5691 - decoder_output_9_accuracy: 0.2212 - val_loss: 7.8769 - val_decoder_output_loss: 0.0713 - val_decoder_output_1_loss: 0.0733 - val_decoder_output_2_loss: 0.9194 - val_decoder_output_3_loss: 2.0046 - val_decoder_output_4_loss: 0.0085 - val_decoder_output_5_loss: 0.1428 - val_decoder_output_6_loss: 1.5532 - val_decoder_output_7_loss: 0.0073 - val_decoder_output_8_loss: 1.0035 - val_decoder_output_9_loss: 2.0930 - val_decoder_output_accuracy: 0.9680 - val_decoder_output_1_accuracy: 0.9680 - val_decoder_output_2_accuracy: 0.5930 - val_decoder_output_3_accuracy: 0.2770 - val_decoder_output_4_accuracy: 1.0000 - val_decoder_output_5_accuracy: 0.9450 - val_decoder_output_6_accuracy: 0.4720 - val_decoder_output_7_accuracy: 1.0000 - val_decoder_output_8_accuracy: 0.5720 - val_decoder_output_9_accuracy: 0.2270\n",
      "Epoch 23/120\n",
      "5/5 [==============================] - 1s 173ms/step - loss: 7.7494 - decoder_output_loss: 0.0660 - decoder_output_1_loss: 0.0693 - decoder_output_2_loss: 0.9248 - decoder_output_3_loss: 1.9718 - decoder_output_4_loss: 0.0077 - decoder_output_5_loss: 0.1195 - decoder_output_6_loss: 1.5003 - decoder_output_7_loss: 0.0068 - decoder_output_8_loss: 0.9836 - decoder_output_9_loss: 2.0996 - decoder_output_accuracy: 0.9736 - decoder_output_1_accuracy: 0.9730 - decoder_output_2_accuracy: 0.5981 - decoder_output_3_accuracy: 0.2766 - decoder_output_4_accuracy: 1.0000 - decoder_output_5_accuracy: 0.9636 - decoder_output_6_accuracy: 0.4982 - decoder_output_7_accuracy: 1.0000 - decoder_output_8_accuracy: 0.5689 - decoder_output_9_accuracy: 0.2206 - val_loss: 7.6891 - val_decoder_output_loss: 0.0685 - val_decoder_output_1_loss: 0.0722 - val_decoder_output_2_loss: 0.8984 - val_decoder_output_3_loss: 1.9596 - val_decoder_output_4_loss: 0.0092 - val_decoder_output_5_loss: 0.1329 - val_decoder_output_6_loss: 1.4756 - val_decoder_output_7_loss: 0.0056 - val_decoder_output_8_loss: 0.9937 - val_decoder_output_9_loss: 2.0734 - val_decoder_output_accuracy: 0.9680 - val_decoder_output_1_accuracy: 0.9690 - val_decoder_output_2_accuracy: 0.6240 - val_decoder_output_3_accuracy: 0.2930 - val_decoder_output_4_accuracy: 1.0000 - val_decoder_output_5_accuracy: 0.9600 - val_decoder_output_6_accuracy: 0.5060 - val_decoder_output_7_accuracy: 1.0000 - val_decoder_output_8_accuracy: 0.5530 - val_decoder_output_9_accuracy: 0.2270\n",
      "Epoch 24/120\n",
      "5/5 [==============================] - 1s 186ms/step - loss: 7.5910 - decoder_output_loss: 0.0643 - decoder_output_1_loss: 0.0678 - decoder_output_2_loss: 0.9086 - decoder_output_3_loss: 1.9365 - decoder_output_4_loss: 0.0084 - decoder_output_5_loss: 0.1152 - decoder_output_6_loss: 1.4234 - decoder_output_7_loss: 0.0061 - decoder_output_8_loss: 0.9741 - decoder_output_9_loss: 2.0865 - decoder_output_accuracy: 0.9741 - decoder_output_1_accuracy: 0.9742 - decoder_output_2_accuracy: 0.6087 - decoder_output_3_accuracy: 0.2878 - decoder_output_4_accuracy: 1.0000 - decoder_output_5_accuracy: 0.9663 - decoder_output_6_accuracy: 0.5223 - decoder_output_7_accuracy: 1.0000 - decoder_output_8_accuracy: 0.5749 - decoder_output_9_accuracy: 0.2214 - val_loss: 7.5108 - val_decoder_output_loss: 0.0659 - val_decoder_output_1_loss: 0.0696 - val_decoder_output_2_loss: 0.8783 - val_decoder_output_3_loss: 1.9378 - val_decoder_output_4_loss: 0.0067 - val_decoder_output_5_loss: 0.1317 - val_decoder_output_6_loss: 1.4057 - val_decoder_output_7_loss: 0.0041 - val_decoder_output_8_loss: 0.9548 - val_decoder_output_9_loss: 2.0561 - val_decoder_output_accuracy: 0.9700 - val_decoder_output_1_accuracy: 0.9710 - val_decoder_output_2_accuracy: 0.6320 - val_decoder_output_3_accuracy: 0.3140 - val_decoder_output_4_accuracy: 1.0000 - val_decoder_output_5_accuracy: 0.9630 - val_decoder_output_6_accuracy: 0.5170 - val_decoder_output_7_accuracy: 1.0000 - val_decoder_output_8_accuracy: 0.5910 - val_decoder_output_9_accuracy: 0.2430\n",
      "Epoch 25/120\n",
      "5/5 [==============================] - 1s 175ms/step - loss: 7.4278 - decoder_output_loss: 0.0626 - decoder_output_1_loss: 0.0670 - decoder_output_2_loss: 0.8838 - decoder_output_3_loss: 1.9074 - decoder_output_4_loss: 0.0073 - decoder_output_5_loss: 0.1110 - decoder_output_6_loss: 1.3603 - decoder_output_7_loss: 0.0054 - decoder_output_8_loss: 0.9527 - decoder_output_9_loss: 2.0702 - decoder_output_accuracy: 0.9749 - decoder_output_1_accuracy: 0.9738 - decoder_output_2_accuracy: 0.6248 - decoder_output_3_accuracy: 0.3051 - decoder_output_4_accuracy: 1.0000 - decoder_output_5_accuracy: 0.9682 - decoder_output_6_accuracy: 0.5396 - decoder_output_7_accuracy: 1.0000 - decoder_output_8_accuracy: 0.5866 - decoder_output_9_accuracy: 0.2326 - val_loss: 7.4439 - val_decoder_output_loss: 0.0694 - val_decoder_output_1_loss: 0.0751 - val_decoder_output_2_loss: 0.8745 - val_decoder_output_3_loss: 1.9137 - val_decoder_output_4_loss: 0.0056 - val_decoder_output_5_loss: 0.1244 - val_decoder_output_6_loss: 1.3521 - val_decoder_output_7_loss: 0.0050 - val_decoder_output_8_loss: 0.9714 - val_decoder_output_9_loss: 2.0529 - val_decoder_output_accuracy: 0.9690 - val_decoder_output_1_accuracy: 0.9670 - val_decoder_output_2_accuracy: 0.6090 - val_decoder_output_3_accuracy: 0.2990 - val_decoder_output_4_accuracy: 1.0000 - val_decoder_output_5_accuracy: 0.9620 - val_decoder_output_6_accuracy: 0.5470 - val_decoder_output_7_accuracy: 1.0000 - val_decoder_output_8_accuracy: 0.5830 - val_decoder_output_9_accuracy: 0.2390\n",
      "Epoch 26/120\n",
      "5/5 [==============================] - 1s 163ms/step - loss: 7.2836 - decoder_output_loss: 0.0614 - decoder_output_1_loss: 0.0664 - decoder_output_2_loss: 0.8720 - decoder_output_3_loss: 1.8850 - decoder_output_4_loss: 0.0068 - decoder_output_5_loss: 0.1040 - decoder_output_6_loss: 1.2918 - decoder_output_7_loss: 0.0053 - decoder_output_8_loss: 0.9400 - decoder_output_9_loss: 2.0510 - decoder_output_accuracy: 0.9740 - decoder_output_1_accuracy: 0.9739 - decoder_output_2_accuracy: 0.6251 - decoder_output_3_accuracy: 0.3054 - decoder_output_4_accuracy: 1.0000 - decoder_output_5_accuracy: 0.9736 - decoder_output_6_accuracy: 0.5684 - decoder_output_7_accuracy: 1.0000 - decoder_output_8_accuracy: 0.5972 - decoder_output_9_accuracy: 0.2406 - val_loss: 7.1800 - val_decoder_output_loss: 0.0594 - val_decoder_output_1_loss: 0.0664 - val_decoder_output_2_loss: 0.8422 - val_decoder_output_3_loss: 1.8568 - val_decoder_output_4_loss: 0.0063 - val_decoder_output_5_loss: 0.1190 - val_decoder_output_6_loss: 1.2535 - val_decoder_output_7_loss: 0.0051 - val_decoder_output_8_loss: 0.9503 - val_decoder_output_9_loss: 2.0213 - val_decoder_output_accuracy: 0.9750 - val_decoder_output_1_accuracy: 0.9740 - val_decoder_output_2_accuracy: 0.6540 - val_decoder_output_3_accuracy: 0.3290 - val_decoder_output_4_accuracy: 1.0000 - val_decoder_output_5_accuracy: 0.9640 - val_decoder_output_6_accuracy: 0.5860 - val_decoder_output_7_accuracy: 1.0000 - val_decoder_output_8_accuracy: 0.5870 - val_decoder_output_9_accuracy: 0.2630\n",
      "Epoch 27/120\n",
      "5/5 [==============================] - 1s 176ms/step - loss: 7.0856 - decoder_output_loss: 0.0606 - decoder_output_1_loss: 0.0663 - decoder_output_2_loss: 0.8518 - decoder_output_3_loss: 1.8402 - decoder_output_4_loss: 0.0067 - decoder_output_5_loss: 0.1037 - decoder_output_6_loss: 1.2164 - decoder_output_7_loss: 0.0050 - decoder_output_8_loss: 0.9179 - decoder_output_9_loss: 2.0171 - decoder_output_accuracy: 0.9756 - decoder_output_1_accuracy: 0.9748 - decoder_output_2_accuracy: 0.6441 - decoder_output_3_accuracy: 0.3284 - decoder_output_4_accuracy: 1.0000 - decoder_output_5_accuracy: 0.9732 - decoder_output_6_accuracy: 0.5977 - decoder_output_7_accuracy: 1.0000 - decoder_output_8_accuracy: 0.6072 - decoder_output_9_accuracy: 0.2557 - val_loss: 7.0058 - val_decoder_output_loss: 0.0572 - val_decoder_output_1_loss: 0.0644 - val_decoder_output_2_loss: 0.8193 - val_decoder_output_3_loss: 1.8441 - val_decoder_output_4_loss: 0.0064 - val_decoder_output_5_loss: 0.1240 - val_decoder_output_6_loss: 1.2157 - val_decoder_output_7_loss: 0.0041 - val_decoder_output_8_loss: 0.8966 - val_decoder_output_9_loss: 1.9741 - val_decoder_output_accuracy: 0.9720 - val_decoder_output_1_accuracy: 0.9700 - val_decoder_output_2_accuracy: 0.6680 - val_decoder_output_3_accuracy: 0.3440 - val_decoder_output_4_accuracy: 1.0000 - val_decoder_output_5_accuracy: 0.9640 - val_decoder_output_6_accuracy: 0.5850 - val_decoder_output_7_accuracy: 1.0000 - val_decoder_output_8_accuracy: 0.6190 - val_decoder_output_9_accuracy: 0.2860\n",
      "Epoch 28/120\n",
      "5/5 [==============================] - 1s 171ms/step - loss: 6.9077 - decoder_output_loss: 0.0600 - decoder_output_1_loss: 0.0655 - decoder_output_2_loss: 0.8308 - decoder_output_3_loss: 1.8053 - decoder_output_4_loss: 0.0068 - decoder_output_5_loss: 0.1047 - decoder_output_6_loss: 1.1681 - decoder_output_7_loss: 0.0052 - decoder_output_8_loss: 0.8870 - decoder_output_9_loss: 1.9744 - decoder_output_accuracy: 0.9744 - decoder_output_1_accuracy: 0.9736 - decoder_output_2_accuracy: 0.6490 - decoder_output_3_accuracy: 0.3410 - decoder_output_4_accuracy: 1.0000 - decoder_output_5_accuracy: 0.9717 - decoder_output_6_accuracy: 0.6166 - decoder_output_7_accuracy: 1.0000 - decoder_output_8_accuracy: 0.6262 - decoder_output_9_accuracy: 0.2781 - val_loss: 6.8155 - val_decoder_output_loss: 0.0577 - val_decoder_output_1_loss: 0.0627 - val_decoder_output_2_loss: 0.7995 - val_decoder_output_3_loss: 1.8121 - val_decoder_output_4_loss: 0.0086 - val_decoder_output_5_loss: 0.1161 - val_decoder_output_6_loss: 1.1296 - val_decoder_output_7_loss: 0.0073 - val_decoder_output_8_loss: 0.8739 - val_decoder_output_9_loss: 1.9479 - val_decoder_output_accuracy: 0.9710 - val_decoder_output_1_accuracy: 0.9700 - val_decoder_output_2_accuracy: 0.6650 - val_decoder_output_3_accuracy: 0.3410 - val_decoder_output_4_accuracy: 1.0000 - val_decoder_output_5_accuracy: 0.9550 - val_decoder_output_6_accuracy: 0.6280 - val_decoder_output_7_accuracy: 0.9990 - val_decoder_output_8_accuracy: 0.6190 - val_decoder_output_9_accuracy: 0.2930\n",
      "Epoch 29/120\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 6.7016 - decoder_output_loss: 0.0571 - decoder_output_1_loss: 0.0625 - decoder_output_2_loss: 0.8104 - decoder_output_3_loss: 1.7769 - decoder_output_4_loss: 0.0067 - decoder_output_5_loss: 0.0980 - decoder_output_6_loss: 1.0916 - decoder_output_7_loss: 0.0056 - decoder_output_8_loss: 0.8558 - decoder_output_9_loss: 1.9370 - decoder_output_accuracy: 0.9774 - decoder_output_1_accuracy: 0.9759 - decoder_output_2_accuracy: 0.6529 - decoder_output_3_accuracy: 0.3517 - decoder_output_4_accuracy: 1.0000 - decoder_output_5_accuracy: 0.9738 - decoder_output_6_accuracy: 0.6416 - decoder_output_7_accuracy: 1.0000 - decoder_output_8_accuracy: 0.6396 - decoder_output_9_accuracy: 0.2910 - val_loss: 6.6049 - val_decoder_output_loss: 0.0589 - val_decoder_output_1_loss: 0.0656 - val_decoder_output_2_loss: 0.7694 - val_decoder_output_3_loss: 1.7741 - val_decoder_output_4_loss: 0.0071 - val_decoder_output_5_loss: 0.1132 - val_decoder_output_6_loss: 1.0526 - val_decoder_output_7_loss: 0.0048 - val_decoder_output_8_loss: 0.8596 - val_decoder_output_9_loss: 1.8996 - val_decoder_output_accuracy: 0.9700 - val_decoder_output_1_accuracy: 0.9720 - val_decoder_output_2_accuracy: 0.6910 - val_decoder_output_3_accuracy: 0.3540 - val_decoder_output_4_accuracy: 1.0000 - val_decoder_output_5_accuracy: 0.9680 - val_decoder_output_6_accuracy: 0.6540 - val_decoder_output_7_accuracy: 1.0000 - val_decoder_output_8_accuracy: 0.6290 - val_decoder_output_9_accuracy: 0.3160\n",
      "Epoch 30/120\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 6.5169 - decoder_output_loss: 0.0567 - decoder_output_1_loss: 0.0616 - decoder_output_2_loss: 0.7886 - decoder_output_3_loss: 1.7342 - decoder_output_4_loss: 0.0074 - decoder_output_5_loss: 0.0946 - decoder_output_6_loss: 1.0226 - decoder_output_7_loss: 0.0053 - decoder_output_8_loss: 0.8479 - decoder_output_9_loss: 1.8981 - decoder_output_accuracy: 0.9762 - decoder_output_1_accuracy: 0.9756 - decoder_output_2_accuracy: 0.6658 - decoder_output_3_accuracy: 0.3632 - decoder_output_4_accuracy: 1.0000 - decoder_output_5_accuracy: 0.9749 - decoder_output_6_accuracy: 0.6651 - decoder_output_7_accuracy: 1.0000 - decoder_output_8_accuracy: 0.6413 - decoder_output_9_accuracy: 0.3103 - val_loss: 6.4234 - val_decoder_output_loss: 0.0545 - val_decoder_output_1_loss: 0.0610 - val_decoder_output_2_loss: 0.7505 - val_decoder_output_3_loss: 1.7330 - val_decoder_output_4_loss: 0.0069 - val_decoder_output_5_loss: 0.1002 - val_decoder_output_6_loss: 0.9859 - val_decoder_output_7_loss: 0.0058 - val_decoder_output_8_loss: 0.8581 - val_decoder_output_9_loss: 1.8676 - val_decoder_output_accuracy: 0.9780 - val_decoder_output_1_accuracy: 0.9800 - val_decoder_output_2_accuracy: 0.6910 - val_decoder_output_3_accuracy: 0.3660 - val_decoder_output_4_accuracy: 1.0000 - val_decoder_output_5_accuracy: 0.9690 - val_decoder_output_6_accuracy: 0.6880 - val_decoder_output_7_accuracy: 1.0000 - val_decoder_output_8_accuracy: 0.6330 - val_decoder_output_9_accuracy: 0.3270\n",
      "Epoch 31/120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 1s 162ms/step - loss: 6.3219 - decoder_output_loss: 0.0565 - decoder_output_1_loss: 0.0618 - decoder_output_2_loss: 0.7563 - decoder_output_3_loss: 1.6787 - decoder_output_4_loss: 0.0065 - decoder_output_5_loss: 0.0930 - decoder_output_6_loss: 0.9731 - decoder_output_7_loss: 0.0059 - decoder_output_8_loss: 0.8352 - decoder_output_9_loss: 1.8551 - decoder_output_accuracy: 0.9761 - decoder_output_1_accuracy: 0.9759 - decoder_output_2_accuracy: 0.6928 - decoder_output_3_accuracy: 0.3877 - decoder_output_4_accuracy: 1.0000 - decoder_output_5_accuracy: 0.9762 - decoder_output_6_accuracy: 0.6814 - decoder_output_7_accuracy: 1.0000 - decoder_output_8_accuracy: 0.6489 - decoder_output_9_accuracy: 0.3306 - val_loss: 6.2764 - val_decoder_output_loss: 0.0547 - val_decoder_output_1_loss: 0.0633 - val_decoder_output_2_loss: 0.7203 - val_decoder_output_3_loss: 1.6580 - val_decoder_output_4_loss: 0.0079 - val_decoder_output_5_loss: 0.1002 - val_decoder_output_6_loss: 0.9644 - val_decoder_output_7_loss: 0.0073 - val_decoder_output_8_loss: 0.8646 - val_decoder_output_9_loss: 1.8357 - val_decoder_output_accuracy: 0.9720 - val_decoder_output_1_accuracy: 0.9740 - val_decoder_output_2_accuracy: 0.6970 - val_decoder_output_3_accuracy: 0.4090 - val_decoder_output_4_accuracy: 1.0000 - val_decoder_output_5_accuracy: 0.9690 - val_decoder_output_6_accuracy: 0.6980 - val_decoder_output_7_accuracy: 1.0000 - val_decoder_output_8_accuracy: 0.6290 - val_decoder_output_9_accuracy: 0.3410\n",
      "Epoch 32/120\n",
      "5/5 [==============================] - 1s 172ms/step - loss: 6.1449 - decoder_output_loss: 0.0558 - decoder_output_1_loss: 0.0615 - decoder_output_2_loss: 0.7416 - decoder_output_3_loss: 1.6395 - decoder_output_4_loss: 0.0069 - decoder_output_5_loss: 0.0902 - decoder_output_6_loss: 0.9213 - decoder_output_7_loss: 0.0058 - decoder_output_8_loss: 0.8145 - decoder_output_9_loss: 1.8080 - decoder_output_accuracy: 0.9757 - decoder_output_1_accuracy: 0.9751 - decoder_output_2_accuracy: 0.6962 - decoder_output_3_accuracy: 0.3987 - decoder_output_4_accuracy: 1.0000 - decoder_output_5_accuracy: 0.9761 - decoder_output_6_accuracy: 0.7083 - decoder_output_7_accuracy: 1.0000 - decoder_output_8_accuracy: 0.6611 - decoder_output_9_accuracy: 0.3389 - val_loss: 5.9946 - val_decoder_output_loss: 0.0532 - val_decoder_output_1_loss: 0.0601 - val_decoder_output_2_loss: 0.7018 - val_decoder_output_3_loss: 1.6273 - val_decoder_output_4_loss: 0.0071 - val_decoder_output_5_loss: 0.0951 - val_decoder_output_6_loss: 0.8961 - val_decoder_output_7_loss: 0.0054 - val_decoder_output_8_loss: 0.7999 - val_decoder_output_9_loss: 1.7487 - val_decoder_output_accuracy: 0.9760 - val_decoder_output_1_accuracy: 0.9730 - val_decoder_output_2_accuracy: 0.7190 - val_decoder_output_3_accuracy: 0.4110 - val_decoder_output_4_accuracy: 1.0000 - val_decoder_output_5_accuracy: 0.9730 - val_decoder_output_6_accuracy: 0.7270 - val_decoder_output_7_accuracy: 1.0000 - val_decoder_output_8_accuracy: 0.6470 - val_decoder_output_9_accuracy: 0.3700\n",
      "Epoch 33/120\n",
      "5/5 [==============================] - 1s 178ms/step - loss: 5.9310 - decoder_output_loss: 0.0561 - decoder_output_1_loss: 0.0617 - decoder_output_2_loss: 0.7135 - decoder_output_3_loss: 1.5736 - decoder_output_4_loss: 0.0071 - decoder_output_5_loss: 0.0878 - decoder_output_6_loss: 0.8800 - decoder_output_7_loss: 0.0057 - decoder_output_8_loss: 0.7901 - decoder_output_9_loss: 1.7554 - decoder_output_accuracy: 0.9757 - decoder_output_1_accuracy: 0.9756 - decoder_output_2_accuracy: 0.7123 - decoder_output_3_accuracy: 0.4230 - decoder_output_4_accuracy: 1.0000 - decoder_output_5_accuracy: 0.9782 - decoder_output_6_accuracy: 0.7163 - decoder_output_7_accuracy: 1.0000 - decoder_output_8_accuracy: 0.6716 - decoder_output_9_accuracy: 0.3534 - val_loss: 5.8175 - val_decoder_output_loss: 0.0542 - val_decoder_output_1_loss: 0.0616 - val_decoder_output_2_loss: 0.6892 - val_decoder_output_3_loss: 1.5830 - val_decoder_output_4_loss: 0.0060 - val_decoder_output_5_loss: 0.0920 - val_decoder_output_6_loss: 0.8474 - val_decoder_output_7_loss: 0.0042 - val_decoder_output_8_loss: 0.7927 - val_decoder_output_9_loss: 1.6872 - val_decoder_output_accuracy: 0.9730 - val_decoder_output_1_accuracy: 0.9720 - val_decoder_output_2_accuracy: 0.7260 - val_decoder_output_3_accuracy: 0.4180 - val_decoder_output_4_accuracy: 1.0000 - val_decoder_output_5_accuracy: 0.9720 - val_decoder_output_6_accuracy: 0.7410 - val_decoder_output_7_accuracy: 1.0000 - val_decoder_output_8_accuracy: 0.6570 - val_decoder_output_9_accuracy: 0.3750\n",
      "Epoch 34/120\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 5.7744 - decoder_output_loss: 0.0554 - decoder_output_1_loss: 0.0598 - decoder_output_2_loss: 0.6864 - decoder_output_3_loss: 1.5425 - decoder_output_4_loss: 0.0068 - decoder_output_5_loss: 0.0853 - decoder_output_6_loss: 0.8448 - decoder_output_7_loss: 0.0052 - decoder_output_8_loss: 0.7863 - decoder_output_9_loss: 1.7018 - decoder_output_accuracy: 0.9762 - decoder_output_1_accuracy: 0.9756 - decoder_output_2_accuracy: 0.7268 - decoder_output_3_accuracy: 0.4287 - decoder_output_4_accuracy: 1.0000 - decoder_output_5_accuracy: 0.9781 - decoder_output_6_accuracy: 0.7313 - decoder_output_7_accuracy: 1.0000 - decoder_output_8_accuracy: 0.6704 - decoder_output_9_accuracy: 0.3706 - val_loss: 5.6254 - val_decoder_output_loss: 0.0531 - val_decoder_output_1_loss: 0.0598 - val_decoder_output_2_loss: 0.6473 - val_decoder_output_3_loss: 1.5239 - val_decoder_output_4_loss: 0.0054 - val_decoder_output_5_loss: 0.0980 - val_decoder_output_6_loss: 0.8257 - val_decoder_output_7_loss: 0.0061 - val_decoder_output_8_loss: 0.7620 - val_decoder_output_9_loss: 1.6440 - val_decoder_output_accuracy: 0.9750 - val_decoder_output_1_accuracy: 0.9730 - val_decoder_output_2_accuracy: 0.7480 - val_decoder_output_3_accuracy: 0.4250 - val_decoder_output_4_accuracy: 1.0000 - val_decoder_output_5_accuracy: 0.9720 - val_decoder_output_6_accuracy: 0.7470 - val_decoder_output_7_accuracy: 1.0000 - val_decoder_output_8_accuracy: 0.6880 - val_decoder_output_9_accuracy: 0.4150\n",
      "Epoch 35/120\n",
      "5/5 [==============================] - 1s 166ms/step - loss: 5.5491 - decoder_output_loss: 0.0559 - decoder_output_1_loss: 0.0605 - decoder_output_2_loss: 0.6511 - decoder_output_3_loss: 1.4826 - decoder_output_4_loss: 0.0062 - decoder_output_5_loss: 0.0862 - decoder_output_6_loss: 0.8055 - decoder_output_7_loss: 0.0055 - decoder_output_8_loss: 0.7532 - decoder_output_9_loss: 1.6423 - decoder_output_accuracy: 0.9758 - decoder_output_1_accuracy: 0.9750 - decoder_output_2_accuracy: 0.7469 - decoder_output_3_accuracy: 0.4464 - decoder_output_4_accuracy: 1.0000 - decoder_output_5_accuracy: 0.9771 - decoder_output_6_accuracy: 0.7472 - decoder_output_7_accuracy: 1.0000 - decoder_output_8_accuracy: 0.6926 - decoder_output_9_accuracy: 0.3919 - val_loss: 5.3768 - val_decoder_output_loss: 0.0531 - val_decoder_output_1_loss: 0.0594 - val_decoder_output_2_loss: 0.6185 - val_decoder_output_3_loss: 1.4660 - val_decoder_output_4_loss: 0.0061 - val_decoder_output_5_loss: 0.0862 - val_decoder_output_6_loss: 0.7738 - val_decoder_output_7_loss: 0.0052 - val_decoder_output_8_loss: 0.7428 - val_decoder_output_9_loss: 1.5658 - val_decoder_output_accuracy: 0.9750 - val_decoder_output_1_accuracy: 0.9720 - val_decoder_output_2_accuracy: 0.7660 - val_decoder_output_3_accuracy: 0.4530 - val_decoder_output_4_accuracy: 1.0000 - val_decoder_output_5_accuracy: 0.9730 - val_decoder_output_6_accuracy: 0.7720 - val_decoder_output_7_accuracy: 1.0000 - val_decoder_output_8_accuracy: 0.6990 - val_decoder_output_9_accuracy: 0.4310\n",
      "Epoch 36/120\n",
      "5/5 [==============================] - 1s 173ms/step - loss: 5.3733 - decoder_output_loss: 0.0554 - decoder_output_1_loss: 0.0588 - decoder_output_2_loss: 0.6218 - decoder_output_3_loss: 1.4382 - decoder_output_4_loss: 0.0064 - decoder_output_5_loss: 0.0797 - decoder_output_6_loss: 0.7715 - decoder_output_7_loss: 0.0059 - decoder_output_8_loss: 0.7506 - decoder_output_9_loss: 1.5850 - decoder_output_accuracy: 0.9757 - decoder_output_1_accuracy: 0.9758 - decoder_output_2_accuracy: 0.7691 - decoder_output_3_accuracy: 0.4642 - decoder_output_4_accuracy: 1.0000 - decoder_output_5_accuracy: 0.9804 - decoder_output_6_accuracy: 0.7572 - decoder_output_7_accuracy: 1.0000 - decoder_output_8_accuracy: 0.6904 - decoder_output_9_accuracy: 0.4190 - val_loss: 5.4197 - val_decoder_output_loss: 0.0519 - val_decoder_output_1_loss: 0.0570 - val_decoder_output_2_loss: 0.5977 - val_decoder_output_3_loss: 1.4479 - val_decoder_output_4_loss: 0.0075 - val_decoder_output_5_loss: 0.0918 - val_decoder_output_6_loss: 0.8064 - val_decoder_output_7_loss: 0.0042 - val_decoder_output_8_loss: 0.7682 - val_decoder_output_9_loss: 1.5871 - val_decoder_output_accuracy: 0.9740 - val_decoder_output_1_accuracy: 0.9740 - val_decoder_output_2_accuracy: 0.7710 - val_decoder_output_3_accuracy: 0.4320 - val_decoder_output_4_accuracy: 1.0000 - val_decoder_output_5_accuracy: 0.9750 - val_decoder_output_6_accuracy: 0.7300 - val_decoder_output_7_accuracy: 1.0000 - val_decoder_output_8_accuracy: 0.6840 - val_decoder_output_9_accuracy: 0.4060\n",
      "Epoch 37/120\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 5.2795 - decoder_output_loss: 0.0564 - decoder_output_1_loss: 0.0600 - decoder_output_2_loss: 0.6004 - decoder_output_3_loss: 1.4025 - decoder_output_4_loss: 0.0059 - decoder_output_5_loss: 0.0831 - decoder_output_6_loss: 0.7551 - decoder_output_7_loss: 0.0049 - decoder_output_8_loss: 0.7479 - decoder_output_9_loss: 1.5633 - decoder_output_accuracy: 0.9749 - decoder_output_1_accuracy: 0.9748 - decoder_output_2_accuracy: 0.7738 - decoder_output_3_accuracy: 0.4787 - decoder_output_4_accuracy: 1.0000 - decoder_output_5_accuracy: 0.9774 - decoder_output_6_accuracy: 0.7684 - decoder_output_7_accuracy: 1.0000 - decoder_output_8_accuracy: 0.6891 - decoder_output_9_accuracy: 0.4261 - val_loss: 5.1289 - val_decoder_output_loss: 0.0523 - val_decoder_output_1_loss: 0.0574 - val_decoder_output_2_loss: 0.5776 - val_decoder_output_3_loss: 1.3696 - val_decoder_output_4_loss: 0.0072 - val_decoder_output_5_loss: 0.0855 - val_decoder_output_6_loss: 0.7142 - val_decoder_output_7_loss: 0.0097 - val_decoder_output_8_loss: 0.7436 - val_decoder_output_9_loss: 1.5119 - val_decoder_output_accuracy: 0.9770 - val_decoder_output_1_accuracy: 0.9740 - val_decoder_output_2_accuracy: 0.7970 - val_decoder_output_3_accuracy: 0.4910 - val_decoder_output_4_accuracy: 1.0000 - val_decoder_output_5_accuracy: 0.9760 - val_decoder_output_6_accuracy: 0.8060 - val_decoder_output_7_accuracy: 1.0000 - val_decoder_output_8_accuracy: 0.6720 - val_decoder_output_9_accuracy: 0.4430\n",
      "Epoch 38/120\n",
      "5/5 [==============================] - 1s 186ms/step - loss: 5.0105 - decoder_output_loss: 0.0546 - decoder_output_1_loss: 0.0574 - decoder_output_2_loss: 0.5589 - decoder_output_3_loss: 1.3424 - decoder_output_4_loss: 0.0059 - decoder_output_5_loss: 0.0804 - decoder_output_6_loss: 0.7094 - decoder_output_7_loss: 0.0063 - decoder_output_8_loss: 0.7265 - decoder_output_9_loss: 1.4687 - decoder_output_accuracy: 0.9762 - decoder_output_1_accuracy: 0.9752 - decoder_output_2_accuracy: 0.7936 - decoder_output_3_accuracy: 0.5016 - decoder_output_4_accuracy: 1.0000 - decoder_output_5_accuracy: 0.9801 - decoder_output_6_accuracy: 0.7848 - decoder_output_7_accuracy: 1.0000 - decoder_output_8_accuracy: 0.6974 - decoder_output_9_accuracy: 0.4606 - val_loss: 4.8606 - val_decoder_output_loss: 0.0528 - val_decoder_output_1_loss: 0.0567 - val_decoder_output_2_loss: 0.5404 - val_decoder_output_3_loss: 1.3402 - val_decoder_output_4_loss: 0.0058 - val_decoder_output_5_loss: 0.0832 - val_decoder_output_6_loss: 0.6730 - val_decoder_output_7_loss: 0.0043 - val_decoder_output_8_loss: 0.7110 - val_decoder_output_9_loss: 1.3933 - val_decoder_output_accuracy: 0.9750 - val_decoder_output_1_accuracy: 0.9740 - val_decoder_output_2_accuracy: 0.7990 - val_decoder_output_3_accuracy: 0.4990 - val_decoder_output_4_accuracy: 1.0000 - val_decoder_output_5_accuracy: 0.9750 - val_decoder_output_6_accuracy: 0.8080 - val_decoder_output_7_accuracy: 1.0000 - val_decoder_output_8_accuracy: 0.6860 - val_decoder_output_9_accuracy: 0.4890\n",
      "Epoch 39/120\n",
      "5/5 [==============================] - 1s 163ms/step - loss: 4.8243 - decoder_output_loss: 0.0548 - decoder_output_1_loss: 0.0568 - decoder_output_2_loss: 0.5270 - decoder_output_3_loss: 1.2996 - decoder_output_4_loss: 0.0058 - decoder_output_5_loss: 0.0783 - decoder_output_6_loss: 0.6831 - decoder_output_7_loss: 0.0055 - decoder_output_8_loss: 0.7086 - decoder_output_9_loss: 1.4048 - decoder_output_accuracy: 0.9760 - decoder_output_1_accuracy: 0.9759 - decoder_output_2_accuracy: 0.8087 - decoder_output_3_accuracy: 0.5158 - decoder_output_4_accuracy: 1.0000 - decoder_output_5_accuracy: 0.9794 - decoder_output_6_accuracy: 0.7883 - decoder_output_7_accuracy: 1.0000 - decoder_output_8_accuracy: 0.7061 - decoder_output_9_accuracy: 0.4879 - val_loss: 4.7111 - val_decoder_output_loss: 0.0518 - val_decoder_output_1_loss: 0.0565 - val_decoder_output_2_loss: 0.5192 - val_decoder_output_3_loss: 1.2796 - val_decoder_output_4_loss: 0.0058 - val_decoder_output_5_loss: 0.0925 - val_decoder_output_6_loss: 0.6506 - val_decoder_output_7_loss: 0.0058 - val_decoder_output_8_loss: 0.7169 - val_decoder_output_9_loss: 1.3324 - val_decoder_output_accuracy: 0.9770 - val_decoder_output_1_accuracy: 0.9750 - val_decoder_output_2_accuracy: 0.7990 - val_decoder_output_3_accuracy: 0.5340 - val_decoder_output_4_accuracy: 1.0000 - val_decoder_output_5_accuracy: 0.9740 - val_decoder_output_6_accuracy: 0.8070 - val_decoder_output_7_accuracy: 1.0000 - val_decoder_output_8_accuracy: 0.7010 - val_decoder_output_9_accuracy: 0.5170\n",
      "Epoch 40/120\n",
      "5/5 [==============================] - 1s 163ms/step - loss: 4.7316 - decoder_output_loss: 0.0545 - decoder_output_1_loss: 0.0570 - decoder_output_2_loss: 0.5071 - decoder_output_3_loss: 1.2607 - decoder_output_4_loss: 0.0061 - decoder_output_5_loss: 0.0832 - decoder_output_6_loss: 0.6675 - decoder_output_7_loss: 0.0061 - decoder_output_8_loss: 0.7301 - decoder_output_9_loss: 1.3592 - decoder_output_accuracy: 0.9758 - decoder_output_1_accuracy: 0.9760 - decoder_output_2_accuracy: 0.8114 - decoder_output_3_accuracy: 0.5306 - decoder_output_4_accuracy: 1.0000 - decoder_output_5_accuracy: 0.9771 - decoder_output_6_accuracy: 0.7912 - decoder_output_7_accuracy: 1.0000 - decoder_output_8_accuracy: 0.6887 - decoder_output_9_accuracy: 0.5030 - val_loss: 4.8505 - val_decoder_output_loss: 0.0516 - val_decoder_output_1_loss: 0.0574 - val_decoder_output_2_loss: 0.4969 - val_decoder_output_3_loss: 1.2334 - val_decoder_output_4_loss: 0.0081 - val_decoder_output_5_loss: 0.0846 - val_decoder_output_6_loss: 0.6425 - val_decoder_output_7_loss: 0.0070 - val_decoder_output_8_loss: 0.8755 - val_decoder_output_9_loss: 1.3936 - val_decoder_output_accuracy: 0.9740 - val_decoder_output_1_accuracy: 0.9750 - val_decoder_output_2_accuracy: 0.8130 - val_decoder_output_3_accuracy: 0.5370 - val_decoder_output_4_accuracy: 1.0000 - val_decoder_output_5_accuracy: 0.9740 - val_decoder_output_6_accuracy: 0.8060 - val_decoder_output_7_accuracy: 1.0000 - val_decoder_output_8_accuracy: 0.6140 - val_decoder_output_9_accuracy: 0.4950\n",
      "Epoch 41/120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 1s 164ms/step - loss: 4.6631 - decoder_output_loss: 0.0562 - decoder_output_1_loss: 0.0579 - decoder_output_2_loss: 0.4951 - decoder_output_3_loss: 1.2543 - decoder_output_4_loss: 0.0066 - decoder_output_5_loss: 0.0784 - decoder_output_6_loss: 0.6361 - decoder_output_7_loss: 0.0063 - decoder_output_8_loss: 0.7391 - decoder_output_9_loss: 1.3331 - decoder_output_accuracy: 0.9762 - decoder_output_1_accuracy: 0.9759 - decoder_output_2_accuracy: 0.8139 - decoder_output_3_accuracy: 0.5296 - decoder_output_4_accuracy: 1.0000 - decoder_output_5_accuracy: 0.9788 - decoder_output_6_accuracy: 0.8064 - decoder_output_7_accuracy: 1.0000 - decoder_output_8_accuracy: 0.6892 - decoder_output_9_accuracy: 0.5109 - val_loss: 4.4588 - val_decoder_output_loss: 0.0544 - val_decoder_output_1_loss: 0.0584 - val_decoder_output_2_loss: 0.4924 - val_decoder_output_3_loss: 1.2314 - val_decoder_output_4_loss: 0.0061 - val_decoder_output_5_loss: 0.0845 - val_decoder_output_6_loss: 0.5961 - val_decoder_output_7_loss: 0.0057 - val_decoder_output_8_loss: 0.6715 - val_decoder_output_9_loss: 1.2583 - val_decoder_output_accuracy: 0.9810 - val_decoder_output_1_accuracy: 0.9780 - val_decoder_output_2_accuracy: 0.8150 - val_decoder_output_3_accuracy: 0.5510 - val_decoder_output_4_accuracy: 1.0000 - val_decoder_output_5_accuracy: 0.9750 - val_decoder_output_6_accuracy: 0.8320 - val_decoder_output_7_accuracy: 1.0000 - val_decoder_output_8_accuracy: 0.7340 - val_decoder_output_9_accuracy: 0.5320\n",
      "Epoch 42/120\n",
      "5/5 [==============================] - 1s 164ms/step - loss: 4.4168 - decoder_output_loss: 0.0554 - decoder_output_1_loss: 0.0574 - decoder_output_2_loss: 0.4699 - decoder_output_3_loss: 1.2120 - decoder_output_4_loss: 0.0063 - decoder_output_5_loss: 0.0760 - decoder_output_6_loss: 0.6097 - decoder_output_7_loss: 0.0057 - decoder_output_8_loss: 0.6793 - decoder_output_9_loss: 1.2451 - decoder_output_accuracy: 0.9764 - decoder_output_1_accuracy: 0.9762 - decoder_output_2_accuracy: 0.8232 - decoder_output_3_accuracy: 0.5513 - decoder_output_4_accuracy: 1.0000 - decoder_output_5_accuracy: 0.9800 - decoder_output_6_accuracy: 0.8149 - decoder_output_7_accuracy: 1.0000 - decoder_output_8_accuracy: 0.7216 - decoder_output_9_accuracy: 0.5496 - val_loss: 4.3327 - val_decoder_output_loss: 0.0500 - val_decoder_output_1_loss: 0.0552 - val_decoder_output_2_loss: 0.4644 - val_decoder_output_3_loss: 1.1813 - val_decoder_output_4_loss: 0.0056 - val_decoder_output_5_loss: 0.0906 - val_decoder_output_6_loss: 0.5936 - val_decoder_output_7_loss: 0.0052 - val_decoder_output_8_loss: 0.6971 - val_decoder_output_9_loss: 1.1897 - val_decoder_output_accuracy: 0.9750 - val_decoder_output_1_accuracy: 0.9760 - val_decoder_output_2_accuracy: 0.8150 - val_decoder_output_3_accuracy: 0.5430 - val_decoder_output_4_accuracy: 1.0000 - val_decoder_output_5_accuracy: 0.9740 - val_decoder_output_6_accuracy: 0.8130 - val_decoder_output_7_accuracy: 1.0000 - val_decoder_output_8_accuracy: 0.7170 - val_decoder_output_9_accuracy: 0.5840\n",
      "Epoch 43/120\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 4.2387 - decoder_output_loss: 0.0518 - decoder_output_1_loss: 0.0535 - decoder_output_2_loss: 0.4455 - decoder_output_3_loss: 1.1556 - decoder_output_4_loss: 0.0063 - decoder_output_5_loss: 0.0781 - decoder_output_6_loss: 0.5918 - decoder_output_7_loss: 0.0061 - decoder_output_8_loss: 0.6722 - decoder_output_9_loss: 1.1776 - decoder_output_accuracy: 0.9773 - decoder_output_1_accuracy: 0.9760 - decoder_output_2_accuracy: 0.8297 - decoder_output_3_accuracy: 0.5648 - decoder_output_4_accuracy: 1.0000 - decoder_output_5_accuracy: 0.9791 - decoder_output_6_accuracy: 0.8199 - decoder_output_7_accuracy: 1.0000 - decoder_output_8_accuracy: 0.7213 - decoder_output_9_accuracy: 0.5742 - val_loss: 4.1175 - val_decoder_output_loss: 0.0506 - val_decoder_output_1_loss: 0.0537 - val_decoder_output_2_loss: 0.4531 - val_decoder_output_3_loss: 1.1574 - val_decoder_output_4_loss: 0.0055 - val_decoder_output_5_loss: 0.0853 - val_decoder_output_6_loss: 0.5666 - val_decoder_output_7_loss: 0.0049 - val_decoder_output_8_loss: 0.6387 - val_decoder_output_9_loss: 1.1019 - val_decoder_output_accuracy: 0.9830 - val_decoder_output_1_accuracy: 0.9780 - val_decoder_output_2_accuracy: 0.8310 - val_decoder_output_3_accuracy: 0.5620 - val_decoder_output_4_accuracy: 1.0000 - val_decoder_output_5_accuracy: 0.9730 - val_decoder_output_6_accuracy: 0.8330 - val_decoder_output_7_accuracy: 1.0000 - val_decoder_output_8_accuracy: 0.7480 - val_decoder_output_9_accuracy: 0.6000\n",
      "Epoch 44/120\n",
      "5/5 [==============================] - 1s 199ms/step - loss: 4.0576 - decoder_output_loss: 0.0526 - decoder_output_1_loss: 0.0543 - decoder_output_2_loss: 0.4341 - decoder_output_3_loss: 1.1369 - decoder_output_4_loss: 0.0056 - decoder_output_5_loss: 0.0751 - decoder_output_6_loss: 0.5743 - decoder_output_7_loss: 0.0055 - decoder_output_8_loss: 0.6357 - decoder_output_9_loss: 1.0835 - decoder_output_accuracy: 0.9767 - decoder_output_1_accuracy: 0.9770 - decoder_output_2_accuracy: 0.8350 - decoder_output_3_accuracy: 0.5747 - decoder_output_4_accuracy: 1.0000 - decoder_output_5_accuracy: 0.9804 - decoder_output_6_accuracy: 0.8229 - decoder_output_7_accuracy: 1.0000 - decoder_output_8_accuracy: 0.7432 - decoder_output_9_accuracy: 0.6108 - val_loss: 3.9617 - val_decoder_output_loss: 0.0501 - val_decoder_output_1_loss: 0.0561 - val_decoder_output_2_loss: 0.4397 - val_decoder_output_3_loss: 1.1097 - val_decoder_output_4_loss: 0.0065 - val_decoder_output_5_loss: 0.0859 - val_decoder_output_6_loss: 0.5521 - val_decoder_output_7_loss: 0.0067 - val_decoder_output_8_loss: 0.6197 - val_decoder_output_9_loss: 1.0352 - val_decoder_output_accuracy: 0.9770 - val_decoder_output_1_accuracy: 0.9740 - val_decoder_output_2_accuracy: 0.8350 - val_decoder_output_3_accuracy: 0.5780 - val_decoder_output_4_accuracy: 1.0000 - val_decoder_output_5_accuracy: 0.9760 - val_decoder_output_6_accuracy: 0.8320 - val_decoder_output_7_accuracy: 1.0000 - val_decoder_output_8_accuracy: 0.7490 - val_decoder_output_9_accuracy: 0.6240\n",
      "Epoch 45/120\n",
      "5/5 [==============================] - 1s 163ms/step - loss: 3.9373 - decoder_output_loss: 0.0516 - decoder_output_1_loss: 0.0529 - decoder_output_2_loss: 0.4203 - decoder_output_3_loss: 1.0844 - decoder_output_4_loss: 0.0061 - decoder_output_5_loss: 0.0736 - decoder_output_6_loss: 0.5631 - decoder_output_7_loss: 0.0064 - decoder_output_8_loss: 0.6299 - decoder_output_9_loss: 1.0491 - decoder_output_accuracy: 0.9767 - decoder_output_1_accuracy: 0.9761 - decoder_output_2_accuracy: 0.8338 - decoder_output_3_accuracy: 0.6006 - decoder_output_4_accuracy: 1.0000 - decoder_output_5_accuracy: 0.9799 - decoder_output_6_accuracy: 0.8241 - decoder_output_7_accuracy: 1.0000 - decoder_output_8_accuracy: 0.7448 - decoder_output_9_accuracy: 0.6224 - val_loss: 3.8390 - val_decoder_output_loss: 0.0482 - val_decoder_output_1_loss: 0.0532 - val_decoder_output_2_loss: 0.4275 - val_decoder_output_3_loss: 1.0624 - val_decoder_output_4_loss: 0.0052 - val_decoder_output_5_loss: 0.0872 - val_decoder_output_6_loss: 0.5359 - val_decoder_output_7_loss: 0.0083 - val_decoder_output_8_loss: 0.6169 - val_decoder_output_9_loss: 0.9942 - val_decoder_output_accuracy: 0.9760 - val_decoder_output_1_accuracy: 0.9770 - val_decoder_output_2_accuracy: 0.8330 - val_decoder_output_3_accuracy: 0.6040 - val_decoder_output_4_accuracy: 1.0000 - val_decoder_output_5_accuracy: 0.9750 - val_decoder_output_6_accuracy: 0.8380 - val_decoder_output_7_accuracy: 1.0000 - val_decoder_output_8_accuracy: 0.7450 - val_decoder_output_9_accuracy: 0.6450\n",
      "Epoch 46/120\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 3.8263 - decoder_output_loss: 0.0510 - decoder_output_1_loss: 0.0521 - decoder_output_2_loss: 0.4081 - decoder_output_3_loss: 1.0502 - decoder_output_4_loss: 0.0052 - decoder_output_5_loss: 0.0738 - decoder_output_6_loss: 0.5520 - decoder_output_7_loss: 0.0067 - decoder_output_8_loss: 0.6097 - decoder_output_9_loss: 1.0175 - decoder_output_accuracy: 0.9761 - decoder_output_1_accuracy: 0.9767 - decoder_output_2_accuracy: 0.8397 - decoder_output_3_accuracy: 0.6184 - decoder_output_4_accuracy: 1.0000 - decoder_output_5_accuracy: 0.9804 - decoder_output_6_accuracy: 0.8290 - decoder_output_7_accuracy: 1.0000 - decoder_output_8_accuracy: 0.7582 - decoder_output_9_accuracy: 0.6280 - val_loss: 3.7741 - val_decoder_output_loss: 0.0478 - val_decoder_output_1_loss: 0.0526 - val_decoder_output_2_loss: 0.4189 - val_decoder_output_3_loss: 1.0347 - val_decoder_output_4_loss: 0.0066 - val_decoder_output_5_loss: 0.0842 - val_decoder_output_6_loss: 0.5374 - val_decoder_output_7_loss: 0.0058 - val_decoder_output_8_loss: 0.6070 - val_decoder_output_9_loss: 0.9791 - val_decoder_output_accuracy: 0.9770 - val_decoder_output_1_accuracy: 0.9790 - val_decoder_output_2_accuracy: 0.8350 - val_decoder_output_3_accuracy: 0.6080 - val_decoder_output_4_accuracy: 1.0000 - val_decoder_output_5_accuracy: 0.9770 - val_decoder_output_6_accuracy: 0.8440 - val_decoder_output_7_accuracy: 1.0000 - val_decoder_output_8_accuracy: 0.7320 - val_decoder_output_9_accuracy: 0.6340\n",
      "Epoch 47/120\n",
      "5/5 [==============================] - 1s 183ms/step - loss: 3.6859 - decoder_output_loss: 0.0501 - decoder_output_1_loss: 0.0511 - decoder_output_2_loss: 0.3984 - decoder_output_3_loss: 1.0222 - decoder_output_4_loss: 0.0055 - decoder_output_5_loss: 0.0751 - decoder_output_6_loss: 0.5412 - decoder_output_7_loss: 0.0058 - decoder_output_8_loss: 0.6019 - decoder_output_9_loss: 0.9346 - decoder_output_accuracy: 0.9771 - decoder_output_1_accuracy: 0.9767 - decoder_output_2_accuracy: 0.8417 - decoder_output_3_accuracy: 0.6231 - decoder_output_4_accuracy: 1.0000 - decoder_output_5_accuracy: 0.9810 - decoder_output_6_accuracy: 0.8340 - decoder_output_7_accuracy: 1.0000 - decoder_output_8_accuracy: 0.7562 - decoder_output_9_accuracy: 0.6647 - val_loss: 3.6177 - val_decoder_output_loss: 0.0476 - val_decoder_output_1_loss: 0.0522 - val_decoder_output_2_loss: 0.4180 - val_decoder_output_3_loss: 1.0137 - val_decoder_output_4_loss: 0.0043 - val_decoder_output_5_loss: 0.0845 - val_decoder_output_6_loss: 0.4920 - val_decoder_output_7_loss: 0.0048 - val_decoder_output_8_loss: 0.5737 - val_decoder_output_9_loss: 0.9269 - val_decoder_output_accuracy: 0.9810 - val_decoder_output_1_accuracy: 0.9760 - val_decoder_output_2_accuracy: 0.8330 - val_decoder_output_3_accuracy: 0.6250 - val_decoder_output_4_accuracy: 1.0000 - val_decoder_output_5_accuracy: 0.9770 - val_decoder_output_6_accuracy: 0.8530 - val_decoder_output_7_accuracy: 1.0000 - val_decoder_output_8_accuracy: 0.7640 - val_decoder_output_9_accuracy: 0.6690\n",
      "Epoch 48/120\n",
      "5/5 [==============================] - 1s 189ms/step - loss: 3.5976 - decoder_output_loss: 0.0512 - decoder_output_1_loss: 0.0525 - decoder_output_2_loss: 0.3923 - decoder_output_3_loss: 0.9890 - decoder_output_4_loss: 0.0056 - decoder_output_5_loss: 0.0723 - decoder_output_6_loss: 0.5195 - decoder_output_7_loss: 0.0059 - decoder_output_8_loss: 0.6232 - decoder_output_9_loss: 0.8861 - decoder_output_accuracy: 0.9762 - decoder_output_1_accuracy: 0.9769 - decoder_output_2_accuracy: 0.8422 - decoder_output_3_accuracy: 0.6422 - decoder_output_4_accuracy: 1.0000 - decoder_output_5_accuracy: 0.9811 - decoder_output_6_accuracy: 0.8427 - decoder_output_7_accuracy: 1.0000 - decoder_output_8_accuracy: 0.7392 - decoder_output_9_accuracy: 0.6851 - val_loss: 3.5606 - val_decoder_output_loss: 0.0487 - val_decoder_output_1_loss: 0.0535 - val_decoder_output_2_loss: 0.4033 - val_decoder_output_3_loss: 0.9699 - val_decoder_output_4_loss: 0.0047 - val_decoder_output_5_loss: 0.0893 - val_decoder_output_6_loss: 0.5077 - val_decoder_output_7_loss: 0.0052 - val_decoder_output_8_loss: 0.6198 - val_decoder_output_9_loss: 0.8586 - val_decoder_output_accuracy: 0.9770 - val_decoder_output_1_accuracy: 0.9800 - val_decoder_output_2_accuracy: 0.8230 - val_decoder_output_3_accuracy: 0.6370 - val_decoder_output_4_accuracy: 1.0000 - val_decoder_output_5_accuracy: 0.9730 - val_decoder_output_6_accuracy: 0.8400 - val_decoder_output_7_accuracy: 1.0000 - val_decoder_output_8_accuracy: 0.7500 - val_decoder_output_9_accuracy: 0.6810\n",
      "Epoch 49/120\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 3.4477 - decoder_output_loss: 0.0503 - decoder_output_1_loss: 0.0509 - decoder_output_2_loss: 0.3798 - decoder_output_3_loss: 0.9524 - decoder_output_4_loss: 0.0054 - decoder_output_5_loss: 0.0763 - decoder_output_6_loss: 0.5070 - decoder_output_7_loss: 0.0059 - decoder_output_8_loss: 0.5884 - decoder_output_9_loss: 0.8313 - decoder_output_accuracy: 0.9761 - decoder_output_1_accuracy: 0.9767 - decoder_output_2_accuracy: 0.8417 - decoder_output_3_accuracy: 0.6586 - decoder_output_4_accuracy: 1.0000 - decoder_output_5_accuracy: 0.9784 - decoder_output_6_accuracy: 0.8462 - decoder_output_7_accuracy: 1.0000 - decoder_output_8_accuracy: 0.7666 - decoder_output_9_accuracy: 0.7104 - val_loss: 3.3762 - val_decoder_output_loss: 0.0460 - val_decoder_output_1_loss: 0.0521 - val_decoder_output_2_loss: 0.3848 - val_decoder_output_3_loss: 0.9317 - val_decoder_output_4_loss: 0.0044 - val_decoder_output_5_loss: 0.1010 - val_decoder_output_6_loss: 0.4995 - val_decoder_output_7_loss: 0.0048 - val_decoder_output_8_loss: 0.5568 - val_decoder_output_9_loss: 0.7950 - val_decoder_output_accuracy: 0.9770 - val_decoder_output_1_accuracy: 0.9790 - val_decoder_output_2_accuracy: 0.8390 - val_decoder_output_3_accuracy: 0.6500 - val_decoder_output_4_accuracy: 1.0000 - val_decoder_output_5_accuracy: 0.9670 - val_decoder_output_6_accuracy: 0.8470 - val_decoder_output_7_accuracy: 1.0000 - val_decoder_output_8_accuracy: 0.7650 - val_decoder_output_9_accuracy: 0.7140\n",
      "Epoch 50/120\n",
      "5/5 [==============================] - 1s 181ms/step - loss: 3.3173 - decoder_output_loss: 0.0492 - decoder_output_1_loss: 0.0505 - decoder_output_2_loss: 0.3673 - decoder_output_3_loss: 0.9168 - decoder_output_4_loss: 0.0053 - decoder_output_5_loss: 0.0741 - decoder_output_6_loss: 0.4935 - decoder_output_7_loss: 0.0055 - decoder_output_8_loss: 0.5611 - decoder_output_9_loss: 0.7941 - decoder_output_accuracy: 0.9770 - decoder_output_1_accuracy: 0.9768 - decoder_output_2_accuracy: 0.8457 - decoder_output_3_accuracy: 0.6712 - decoder_output_4_accuracy: 1.0000 - decoder_output_5_accuracy: 0.9796 - decoder_output_6_accuracy: 0.8463 - decoder_output_7_accuracy: 1.0000 - decoder_output_8_accuracy: 0.7858 - decoder_output_9_accuracy: 0.7226 - val_loss: 3.2145 - val_decoder_output_loss: 0.0461 - val_decoder_output_1_loss: 0.0519 - val_decoder_output_2_loss: 0.3806 - val_decoder_output_3_loss: 0.8926 - val_decoder_output_4_loss: 0.0045 - val_decoder_output_5_loss: 0.0817 - val_decoder_output_6_loss: 0.4618 - val_decoder_output_7_loss: 0.0055 - val_decoder_output_8_loss: 0.5386 - val_decoder_output_9_loss: 0.7513 - val_decoder_output_accuracy: 0.9770 - val_decoder_output_1_accuracy: 0.9790 - val_decoder_output_2_accuracy: 0.8350 - val_decoder_output_3_accuracy: 0.6760 - val_decoder_output_4_accuracy: 1.0000 - val_decoder_output_5_accuracy: 0.9780 - val_decoder_output_6_accuracy: 0.8560 - val_decoder_output_7_accuracy: 1.0000 - val_decoder_output_8_accuracy: 0.7890 - val_decoder_output_9_accuracy: 0.7220\n",
      "Epoch 51/120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 1s 161ms/step - loss: 3.1913 - decoder_output_loss: 0.0489 - decoder_output_1_loss: 0.0502 - decoder_output_2_loss: 0.3624 - decoder_output_3_loss: 0.8835 - decoder_output_4_loss: 0.0052 - decoder_output_5_loss: 0.0665 - decoder_output_6_loss: 0.4722 - decoder_output_7_loss: 0.0052 - decoder_output_8_loss: 0.5425 - decoder_output_9_loss: 0.7547 - decoder_output_accuracy: 0.9768 - decoder_output_1_accuracy: 0.9773 - decoder_output_2_accuracy: 0.8454 - decoder_output_3_accuracy: 0.6840 - decoder_output_4_accuracy: 1.0000 - decoder_output_5_accuracy: 0.9820 - decoder_output_6_accuracy: 0.8588 - decoder_output_7_accuracy: 1.0000 - decoder_output_8_accuracy: 0.7940 - decoder_output_9_accuracy: 0.7298 - val_loss: 3.1167 - val_decoder_output_loss: 0.0453 - val_decoder_output_1_loss: 0.0513 - val_decoder_output_2_loss: 0.3720 - val_decoder_output_3_loss: 0.8623 - val_decoder_output_4_loss: 0.0048 - val_decoder_output_5_loss: 0.0785 - val_decoder_output_6_loss: 0.4363 - val_decoder_output_7_loss: 0.0053 - val_decoder_output_8_loss: 0.5186 - val_decoder_output_9_loss: 0.7423 - val_decoder_output_accuracy: 0.9780 - val_decoder_output_1_accuracy: 0.9790 - val_decoder_output_2_accuracy: 0.8370 - val_decoder_output_3_accuracy: 0.6860 - val_decoder_output_4_accuracy: 1.0000 - val_decoder_output_5_accuracy: 0.9780 - val_decoder_output_6_accuracy: 0.8690 - val_decoder_output_7_accuracy: 1.0000 - val_decoder_output_8_accuracy: 0.8100 - val_decoder_output_9_accuracy: 0.7460\n",
      "Epoch 52/120\n",
      "5/5 [==============================] - 1s 168ms/step - loss: 3.0784 - decoder_output_loss: 0.0487 - decoder_output_1_loss: 0.0501 - decoder_output_2_loss: 0.3573 - decoder_output_3_loss: 0.8553 - decoder_output_4_loss: 0.0050 - decoder_output_5_loss: 0.0660 - decoder_output_6_loss: 0.4499 - decoder_output_7_loss: 0.0051 - decoder_output_8_loss: 0.5243 - decoder_output_9_loss: 0.7167 - decoder_output_accuracy: 0.9771 - decoder_output_1_accuracy: 0.9776 - decoder_output_2_accuracy: 0.8480 - decoder_output_3_accuracy: 0.7019 - decoder_output_4_accuracy: 1.0000 - decoder_output_5_accuracy: 0.9811 - decoder_output_6_accuracy: 0.8671 - decoder_output_7_accuracy: 1.0000 - decoder_output_8_accuracy: 0.8039 - decoder_output_9_accuracy: 0.7477 - val_loss: 3.0073 - val_decoder_output_loss: 0.0469 - val_decoder_output_1_loss: 0.0525 - val_decoder_output_2_loss: 0.3682 - val_decoder_output_3_loss: 0.8285 - val_decoder_output_4_loss: 0.0044 - val_decoder_output_5_loss: 0.0820 - val_decoder_output_6_loss: 0.4292 - val_decoder_output_7_loss: 0.0058 - val_decoder_output_8_loss: 0.5189 - val_decoder_output_9_loss: 0.6709 - val_decoder_output_accuracy: 0.9770 - val_decoder_output_1_accuracy: 0.9790 - val_decoder_output_2_accuracy: 0.8390 - val_decoder_output_3_accuracy: 0.7180 - val_decoder_output_4_accuracy: 1.0000 - val_decoder_output_5_accuracy: 0.9780 - val_decoder_output_6_accuracy: 0.8750 - val_decoder_output_7_accuracy: 1.0000 - val_decoder_output_8_accuracy: 0.8020 - val_decoder_output_9_accuracy: 0.7630\n",
      "Epoch 53/120\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 2.9523 - decoder_output_loss: 0.0485 - decoder_output_1_loss: 0.0497 - decoder_output_2_loss: 0.3497 - decoder_output_3_loss: 0.8219 - decoder_output_4_loss: 0.0052 - decoder_output_5_loss: 0.0654 - decoder_output_6_loss: 0.4359 - decoder_output_7_loss: 0.0053 - decoder_output_8_loss: 0.5101 - decoder_output_9_loss: 0.6605 - decoder_output_accuracy: 0.9777 - decoder_output_1_accuracy: 0.9773 - decoder_output_2_accuracy: 0.8459 - decoder_output_3_accuracy: 0.7140 - decoder_output_4_accuracy: 1.0000 - decoder_output_5_accuracy: 0.9821 - decoder_output_6_accuracy: 0.8714 - decoder_output_7_accuracy: 1.0000 - decoder_output_8_accuracy: 0.8097 - decoder_output_9_accuracy: 0.7671 - val_loss: 2.9251 - val_decoder_output_loss: 0.0461 - val_decoder_output_1_loss: 0.0518 - val_decoder_output_2_loss: 0.3594 - val_decoder_output_3_loss: 0.8075 - val_decoder_output_4_loss: 0.0044 - val_decoder_output_5_loss: 0.0788 - val_decoder_output_6_loss: 0.4212 - val_decoder_output_7_loss: 0.0057 - val_decoder_output_8_loss: 0.4992 - val_decoder_output_9_loss: 0.6510 - val_decoder_output_accuracy: 0.9770 - val_decoder_output_1_accuracy: 0.9770 - val_decoder_output_2_accuracy: 0.8380 - val_decoder_output_3_accuracy: 0.7360 - val_decoder_output_4_accuracy: 1.0000 - val_decoder_output_5_accuracy: 0.9780 - val_decoder_output_6_accuracy: 0.8740 - val_decoder_output_7_accuracy: 1.0000 - val_decoder_output_8_accuracy: 0.8080 - val_decoder_output_9_accuracy: 0.7710\n",
      "Epoch 54/120\n",
      "5/5 [==============================] - 1s 174ms/step - loss: 2.8489 - decoder_output_loss: 0.0474 - decoder_output_1_loss: 0.0491 - decoder_output_2_loss: 0.3421 - decoder_output_3_loss: 0.7957 - decoder_output_4_loss: 0.0051 - decoder_output_5_loss: 0.0631 - decoder_output_6_loss: 0.4249 - decoder_output_7_loss: 0.0048 - decoder_output_8_loss: 0.4958 - decoder_output_9_loss: 0.6209 - decoder_output_accuracy: 0.9772 - decoder_output_1_accuracy: 0.9767 - decoder_output_2_accuracy: 0.8466 - decoder_output_3_accuracy: 0.7299 - decoder_output_4_accuracy: 1.0000 - decoder_output_5_accuracy: 0.9828 - decoder_output_6_accuracy: 0.8736 - decoder_output_7_accuracy: 1.0000 - decoder_output_8_accuracy: 0.8184 - decoder_output_9_accuracy: 0.7857 - val_loss: 2.8023 - val_decoder_output_loss: 0.0454 - val_decoder_output_1_loss: 0.0517 - val_decoder_output_2_loss: 0.3557 - val_decoder_output_3_loss: 0.7709 - val_decoder_output_4_loss: 0.0044 - val_decoder_output_5_loss: 0.0774 - val_decoder_output_6_loss: 0.4026 - val_decoder_output_7_loss: 0.0047 - val_decoder_output_8_loss: 0.4849 - val_decoder_output_9_loss: 0.6046 - val_decoder_output_accuracy: 0.9770 - val_decoder_output_1_accuracy: 0.9770 - val_decoder_output_2_accuracy: 0.8380 - val_decoder_output_3_accuracy: 0.7470 - val_decoder_output_4_accuracy: 1.0000 - val_decoder_output_5_accuracy: 0.9770 - val_decoder_output_6_accuracy: 0.8860 - val_decoder_output_7_accuracy: 1.0000 - val_decoder_output_8_accuracy: 0.8090 - val_decoder_output_9_accuracy: 0.7930\n",
      "Epoch 55/120\n",
      "5/5 [==============================] - 1s 170ms/step - loss: 2.7539 - decoder_output_loss: 0.0475 - decoder_output_1_loss: 0.0490 - decoder_output_2_loss: 0.3404 - decoder_output_3_loss: 0.7658 - decoder_output_4_loss: 0.0050 - decoder_output_5_loss: 0.0620 - decoder_output_6_loss: 0.4127 - decoder_output_7_loss: 0.0050 - decoder_output_8_loss: 0.4813 - decoder_output_9_loss: 0.5851 - decoder_output_accuracy: 0.9764 - decoder_output_1_accuracy: 0.9761 - decoder_output_2_accuracy: 0.8437 - decoder_output_3_accuracy: 0.7422 - decoder_output_4_accuracy: 1.0000 - decoder_output_5_accuracy: 0.9827 - decoder_output_6_accuracy: 0.8804 - decoder_output_7_accuracy: 1.0000 - decoder_output_8_accuracy: 0.8239 - decoder_output_9_accuracy: 0.7943 - val_loss: 2.7317 - val_decoder_output_loss: 0.0442 - val_decoder_output_1_loss: 0.0502 - val_decoder_output_2_loss: 0.3589 - val_decoder_output_3_loss: 0.7474 - val_decoder_output_4_loss: 0.0044 - val_decoder_output_5_loss: 0.0747 - val_decoder_output_6_loss: 0.3980 - val_decoder_output_7_loss: 0.0042 - val_decoder_output_8_loss: 0.4745 - val_decoder_output_9_loss: 0.5752 - val_decoder_output_accuracy: 0.9800 - val_decoder_output_1_accuracy: 0.9780 - val_decoder_output_2_accuracy: 0.8340 - val_decoder_output_3_accuracy: 0.7590 - val_decoder_output_4_accuracy: 1.0000 - val_decoder_output_5_accuracy: 0.9760 - val_decoder_output_6_accuracy: 0.8830 - val_decoder_output_7_accuracy: 1.0000 - val_decoder_output_8_accuracy: 0.8160 - val_decoder_output_9_accuracy: 0.8010\n",
      "Epoch 56/120\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 2.6832 - decoder_output_loss: 0.0477 - decoder_output_1_loss: 0.0489 - decoder_output_2_loss: 0.3381 - decoder_output_3_loss: 0.7414 - decoder_output_4_loss: 0.0049 - decoder_output_5_loss: 0.0601 - decoder_output_6_loss: 0.4064 - decoder_output_7_loss: 0.0045 - decoder_output_8_loss: 0.4687 - decoder_output_9_loss: 0.5625 - decoder_output_accuracy: 0.9772 - decoder_output_1_accuracy: 0.9777 - decoder_output_2_accuracy: 0.8432 - decoder_output_3_accuracy: 0.7506 - decoder_output_4_accuracy: 1.0000 - decoder_output_5_accuracy: 0.9839 - decoder_output_6_accuracy: 0.8811 - decoder_output_7_accuracy: 1.0000 - decoder_output_8_accuracy: 0.8302 - decoder_output_9_accuracy: 0.8067 - val_loss: 2.7127 - val_decoder_output_loss: 0.0439 - val_decoder_output_1_loss: 0.0502 - val_decoder_output_2_loss: 0.3547 - val_decoder_output_3_loss: 0.7069 - val_decoder_output_4_loss: 0.0048 - val_decoder_output_5_loss: 0.0737 - val_decoder_output_6_loss: 0.3951 - val_decoder_output_7_loss: 0.0043 - val_decoder_output_8_loss: 0.4931 - val_decoder_output_9_loss: 0.5861 - val_decoder_output_accuracy: 0.9780 - val_decoder_output_1_accuracy: 0.9790 - val_decoder_output_2_accuracy: 0.8390 - val_decoder_output_3_accuracy: 0.7740 - val_decoder_output_4_accuracy: 1.0000 - val_decoder_output_5_accuracy: 0.9750 - val_decoder_output_6_accuracy: 0.8760 - val_decoder_output_7_accuracy: 1.0000 - val_decoder_output_8_accuracy: 0.7960 - val_decoder_output_9_accuracy: 0.7810\n",
      "Epoch 57/120\n",
      "5/5 [==============================] - 1s 163ms/step - loss: 2.5987 - decoder_output_loss: 0.0472 - decoder_output_1_loss: 0.0482 - decoder_output_2_loss: 0.3321 - decoder_output_3_loss: 0.7072 - decoder_output_4_loss: 0.0047 - decoder_output_5_loss: 0.0598 - decoder_output_6_loss: 0.3929 - decoder_output_7_loss: 0.0044 - decoder_output_8_loss: 0.4600 - decoder_output_9_loss: 0.5423 - decoder_output_accuracy: 0.9764 - decoder_output_1_accuracy: 0.9769 - decoder_output_2_accuracy: 0.8464 - decoder_output_3_accuracy: 0.7674 - decoder_output_4_accuracy: 1.0000 - decoder_output_5_accuracy: 0.9826 - decoder_output_6_accuracy: 0.8839 - decoder_output_7_accuracy: 1.0000 - decoder_output_8_accuracy: 0.8293 - decoder_output_9_accuracy: 0.8126 - val_loss: 2.5900 - val_decoder_output_loss: 0.0440 - val_decoder_output_1_loss: 0.0510 - val_decoder_output_2_loss: 0.3526 - val_decoder_output_3_loss: 0.6916 - val_decoder_output_4_loss: 0.0040 - val_decoder_output_5_loss: 0.0724 - val_decoder_output_6_loss: 0.3792 - val_decoder_output_7_loss: 0.0047 - val_decoder_output_8_loss: 0.4475 - val_decoder_output_9_loss: 0.5429 - val_decoder_output_accuracy: 0.9770 - val_decoder_output_1_accuracy: 0.9790 - val_decoder_output_2_accuracy: 0.8370 - val_decoder_output_3_accuracy: 0.7780 - val_decoder_output_4_accuracy: 1.0000 - val_decoder_output_5_accuracy: 0.9760 - val_decoder_output_6_accuracy: 0.8890 - val_decoder_output_7_accuracy: 1.0000 - val_decoder_output_8_accuracy: 0.8250 - val_decoder_output_9_accuracy: 0.8080\n",
      "Epoch 58/120\n",
      "5/5 [==============================] - 1s 165ms/step - loss: 2.5217 - decoder_output_loss: 0.0466 - decoder_output_1_loss: 0.0479 - decoder_output_2_loss: 0.3296 - decoder_output_3_loss: 0.6803 - decoder_output_4_loss: 0.0043 - decoder_output_5_loss: 0.0604 - decoder_output_6_loss: 0.3842 - decoder_output_7_loss: 0.0044 - decoder_output_8_loss: 0.4479 - decoder_output_9_loss: 0.5163 - decoder_output_accuracy: 0.9769 - decoder_output_1_accuracy: 0.9777 - decoder_output_2_accuracy: 0.8474 - decoder_output_3_accuracy: 0.7826 - decoder_output_4_accuracy: 1.0000 - decoder_output_5_accuracy: 0.9827 - decoder_output_6_accuracy: 0.8893 - decoder_output_7_accuracy: 1.0000 - decoder_output_8_accuracy: 0.8346 - decoder_output_9_accuracy: 0.8199 - val_loss: 2.5157 - val_decoder_output_loss: 0.0434 - val_decoder_output_1_loss: 0.0495 - val_decoder_output_2_loss: 0.3432 - val_decoder_output_3_loss: 0.6537 - val_decoder_output_4_loss: 0.0045 - val_decoder_output_5_loss: 0.0717 - val_decoder_output_6_loss: 0.3732 - val_decoder_output_7_loss: 0.0045 - val_decoder_output_8_loss: 0.4501 - val_decoder_output_9_loss: 0.5219 - val_decoder_output_accuracy: 0.9790 - val_decoder_output_1_accuracy: 0.9780 - val_decoder_output_2_accuracy: 0.8360 - val_decoder_output_3_accuracy: 0.7870 - val_decoder_output_4_accuracy: 1.0000 - val_decoder_output_5_accuracy: 0.9750 - val_decoder_output_6_accuracy: 0.8910 - val_decoder_output_7_accuracy: 1.0000 - val_decoder_output_8_accuracy: 0.8230 - val_decoder_output_9_accuracy: 0.8080\n",
      "Epoch 59/120\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 2.4546 - decoder_output_loss: 0.0464 - decoder_output_1_loss: 0.0474 - decoder_output_2_loss: 0.3275 - decoder_output_3_loss: 0.6546 - decoder_output_4_loss: 0.0047 - decoder_output_5_loss: 0.0578 - decoder_output_6_loss: 0.3735 - decoder_output_7_loss: 0.0045 - decoder_output_8_loss: 0.4381 - decoder_output_9_loss: 0.5001 - decoder_output_accuracy: 0.9773 - decoder_output_1_accuracy: 0.9773 - decoder_output_2_accuracy: 0.8439 - decoder_output_3_accuracy: 0.7919 - decoder_output_4_accuracy: 1.0000 - decoder_output_5_accuracy: 0.9840 - decoder_output_6_accuracy: 0.8906 - decoder_output_7_accuracy: 1.0000 - decoder_output_8_accuracy: 0.8409 - decoder_output_9_accuracy: 0.8251 - val_loss: 2.4462 - val_decoder_output_loss: 0.0438 - val_decoder_output_1_loss: 0.0497 - val_decoder_output_2_loss: 0.3442 - val_decoder_output_3_loss: 0.6370 - val_decoder_output_4_loss: 0.0047 - val_decoder_output_5_loss: 0.0699 - val_decoder_output_6_loss: 0.3627 - val_decoder_output_7_loss: 0.0044 - val_decoder_output_8_loss: 0.4301 - val_decoder_output_9_loss: 0.4996 - val_decoder_output_accuracy: 0.9770 - val_decoder_output_1_accuracy: 0.9800 - val_decoder_output_2_accuracy: 0.8310 - val_decoder_output_3_accuracy: 0.7890 - val_decoder_output_4_accuracy: 1.0000 - val_decoder_output_5_accuracy: 0.9760 - val_decoder_output_6_accuracy: 0.8960 - val_decoder_output_7_accuracy: 1.0000 - val_decoder_output_8_accuracy: 0.8270 - val_decoder_output_9_accuracy: 0.8190\n",
      "Epoch 60/120\n",
      "5/5 [==============================] - 1s 178ms/step - loss: 2.4087 - decoder_output_loss: 0.0467 - decoder_output_1_loss: 0.0477 - decoder_output_2_loss: 0.3256 - decoder_output_3_loss: 0.6383 - decoder_output_4_loss: 0.0046 - decoder_output_5_loss: 0.0578 - decoder_output_6_loss: 0.3629 - decoder_output_7_loss: 0.0044 - decoder_output_8_loss: 0.4329 - decoder_output_9_loss: 0.4878 - decoder_output_accuracy: 0.9772 - decoder_output_1_accuracy: 0.9768 - decoder_output_2_accuracy: 0.8409 - decoder_output_3_accuracy: 0.7936 - decoder_output_4_accuracy: 1.0000 - decoder_output_5_accuracy: 0.9830 - decoder_output_6_accuracy: 0.8934 - decoder_output_7_accuracy: 1.0000 - decoder_output_8_accuracy: 0.8391 - decoder_output_9_accuracy: 0.8307 - val_loss: 2.3834 - val_decoder_output_loss: 0.0430 - val_decoder_output_1_loss: 0.0484 - val_decoder_output_2_loss: 0.3384 - val_decoder_output_3_loss: 0.6180 - val_decoder_output_4_loss: 0.0035 - val_decoder_output_5_loss: 0.0750 - val_decoder_output_6_loss: 0.3545 - val_decoder_output_7_loss: 0.0039 - val_decoder_output_8_loss: 0.4249 - val_decoder_output_9_loss: 0.4739 - val_decoder_output_accuracy: 0.9790 - val_decoder_output_1_accuracy: 0.9790 - val_decoder_output_2_accuracy: 0.8350 - val_decoder_output_3_accuracy: 0.7990 - val_decoder_output_4_accuracy: 1.0000 - val_decoder_output_5_accuracy: 0.9770 - val_decoder_output_6_accuracy: 0.9000 - val_decoder_output_7_accuracy: 1.0000 - val_decoder_output_8_accuracy: 0.8370 - val_decoder_output_9_accuracy: 0.8360\n",
      "Epoch 61/120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 1s 167ms/step - loss: 2.3367 - decoder_output_loss: 0.0462 - decoder_output_1_loss: 0.0471 - decoder_output_2_loss: 0.3226 - decoder_output_3_loss: 0.6123 - decoder_output_4_loss: 0.0045 - decoder_output_5_loss: 0.0551 - decoder_output_6_loss: 0.3549 - decoder_output_7_loss: 0.0043 - decoder_output_8_loss: 0.4177 - decoder_output_9_loss: 0.4719 - decoder_output_accuracy: 0.9768 - decoder_output_1_accuracy: 0.9778 - decoder_output_2_accuracy: 0.8438 - decoder_output_3_accuracy: 0.8047 - decoder_output_4_accuracy: 1.0000 - decoder_output_5_accuracy: 0.9849 - decoder_output_6_accuracy: 0.8990 - decoder_output_7_accuracy: 1.0000 - decoder_output_8_accuracy: 0.8446 - decoder_output_9_accuracy: 0.8378 - val_loss: 2.3985 - val_decoder_output_loss: 0.0434 - val_decoder_output_1_loss: 0.0496 - val_decoder_output_2_loss: 0.3374 - val_decoder_output_3_loss: 0.5973 - val_decoder_output_4_loss: 0.0038 - val_decoder_output_5_loss: 0.0755 - val_decoder_output_6_loss: 0.3549 - val_decoder_output_7_loss: 0.0033 - val_decoder_output_8_loss: 0.4284 - val_decoder_output_9_loss: 0.5049 - val_decoder_output_accuracy: 0.9800 - val_decoder_output_1_accuracy: 0.9780 - val_decoder_output_2_accuracy: 0.8250 - val_decoder_output_3_accuracy: 0.8090 - val_decoder_output_4_accuracy: 1.0000 - val_decoder_output_5_accuracy: 0.9760 - val_decoder_output_6_accuracy: 0.8930 - val_decoder_output_7_accuracy: 1.0000 - val_decoder_output_8_accuracy: 0.8270 - val_decoder_output_9_accuracy: 0.8260\n",
      "Epoch 62/120\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 2.2783 - decoder_output_loss: 0.0457 - decoder_output_1_loss: 0.0467 - decoder_output_2_loss: 0.3199 - decoder_output_3_loss: 0.5881 - decoder_output_4_loss: 0.0044 - decoder_output_5_loss: 0.0577 - decoder_output_6_loss: 0.3478 - decoder_output_7_loss: 0.0042 - decoder_output_8_loss: 0.4088 - decoder_output_9_loss: 0.4550 - decoder_output_accuracy: 0.9767 - decoder_output_1_accuracy: 0.9774 - decoder_output_2_accuracy: 0.8404 - decoder_output_3_accuracy: 0.8172 - decoder_output_4_accuracy: 1.0000 - decoder_output_5_accuracy: 0.9836 - decoder_output_6_accuracy: 0.9004 - decoder_output_7_accuracy: 1.0000 - decoder_output_8_accuracy: 0.8491 - decoder_output_9_accuracy: 0.8443 - val_loss: 2.2717 - val_decoder_output_loss: 0.0427 - val_decoder_output_1_loss: 0.0481 - val_decoder_output_2_loss: 0.3336 - val_decoder_output_3_loss: 0.5691 - val_decoder_output_4_loss: 0.0038 - val_decoder_output_5_loss: 0.0691 - val_decoder_output_6_loss: 0.3417 - val_decoder_output_7_loss: 0.0041 - val_decoder_output_8_loss: 0.4106 - val_decoder_output_9_loss: 0.4490 - val_decoder_output_accuracy: 0.9800 - val_decoder_output_1_accuracy: 0.9790 - val_decoder_output_2_accuracy: 0.8300 - val_decoder_output_3_accuracy: 0.8090 - val_decoder_output_4_accuracy: 1.0000 - val_decoder_output_5_accuracy: 0.9760 - val_decoder_output_6_accuracy: 0.9030 - val_decoder_output_7_accuracy: 1.0000 - val_decoder_output_8_accuracy: 0.8460 - val_decoder_output_9_accuracy: 0.8450\n",
      "Epoch 63/120\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 2.1942 - decoder_output_loss: 0.0452 - decoder_output_1_loss: 0.0462 - decoder_output_2_loss: 0.3160 - decoder_output_3_loss: 0.5643 - decoder_output_4_loss: 0.0042 - decoder_output_5_loss: 0.0526 - decoder_output_6_loss: 0.3376 - decoder_output_7_loss: 0.0044 - decoder_output_8_loss: 0.3958 - decoder_output_9_loss: 0.4279 - decoder_output_accuracy: 0.9770 - decoder_output_1_accuracy: 0.9773 - decoder_output_2_accuracy: 0.8426 - decoder_output_3_accuracy: 0.8232 - decoder_output_4_accuracy: 1.0000 - decoder_output_5_accuracy: 0.9847 - decoder_output_6_accuracy: 0.9049 - decoder_output_7_accuracy: 1.0000 - decoder_output_8_accuracy: 0.8522 - decoder_output_9_accuracy: 0.8548 - val_loss: 2.2243 - val_decoder_output_loss: 0.0428 - val_decoder_output_1_loss: 0.0483 - val_decoder_output_2_loss: 0.3354 - val_decoder_output_3_loss: 0.5427 - val_decoder_output_4_loss: 0.0038 - val_decoder_output_5_loss: 0.0696 - val_decoder_output_6_loss: 0.3363 - val_decoder_output_7_loss: 0.0041 - val_decoder_output_8_loss: 0.4073 - val_decoder_output_9_loss: 0.4340 - val_decoder_output_accuracy: 0.9790 - val_decoder_output_1_accuracy: 0.9790 - val_decoder_output_2_accuracy: 0.8290 - val_decoder_output_3_accuracy: 0.8220 - val_decoder_output_4_accuracy: 1.0000 - val_decoder_output_5_accuracy: 0.9730 - val_decoder_output_6_accuracy: 0.9080 - val_decoder_output_7_accuracy: 1.0000 - val_decoder_output_8_accuracy: 0.8480 - val_decoder_output_9_accuracy: 0.8500\n",
      "Epoch 64/120\n",
      "5/5 [==============================] - 1s 170ms/step - loss: 2.1514 - decoder_output_loss: 0.0447 - decoder_output_1_loss: 0.0456 - decoder_output_2_loss: 0.3167 - decoder_output_3_loss: 0.5501 - decoder_output_4_loss: 0.0042 - decoder_output_5_loss: 0.0534 - decoder_output_6_loss: 0.3311 - decoder_output_7_loss: 0.0042 - decoder_output_8_loss: 0.3902 - decoder_output_9_loss: 0.4112 - decoder_output_accuracy: 0.9772 - decoder_output_1_accuracy: 0.9789 - decoder_output_2_accuracy: 0.8423 - decoder_output_3_accuracy: 0.8281 - decoder_output_4_accuracy: 1.0000 - decoder_output_5_accuracy: 0.9839 - decoder_output_6_accuracy: 0.9077 - decoder_output_7_accuracy: 1.0000 - decoder_output_8_accuracy: 0.8567 - decoder_output_9_accuracy: 0.8600 - val_loss: 2.2041 - val_decoder_output_loss: 0.0427 - val_decoder_output_1_loss: 0.0483 - val_decoder_output_2_loss: 0.3328 - val_decoder_output_3_loss: 0.5348 - val_decoder_output_4_loss: 0.0041 - val_decoder_output_5_loss: 0.0654 - val_decoder_output_6_loss: 0.3293 - val_decoder_output_7_loss: 0.0045 - val_decoder_output_8_loss: 0.4120 - val_decoder_output_9_loss: 0.4302 - val_decoder_output_accuracy: 0.9790 - val_decoder_output_1_accuracy: 0.9790 - val_decoder_output_2_accuracy: 0.8300 - val_decoder_output_3_accuracy: 0.8300 - val_decoder_output_4_accuracy: 1.0000 - val_decoder_output_5_accuracy: 0.9740 - val_decoder_output_6_accuracy: 0.9100 - val_decoder_output_7_accuracy: 1.0000 - val_decoder_output_8_accuracy: 0.8360 - val_decoder_output_9_accuracy: 0.8470\n",
      "Epoch 65/120\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 2.1010 - decoder_output_loss: 0.0446 - decoder_output_1_loss: 0.0457 - decoder_output_2_loss: 0.3139 - decoder_output_3_loss: 0.5267 - decoder_output_4_loss: 0.0040 - decoder_output_5_loss: 0.0520 - decoder_output_6_loss: 0.3234 - decoder_output_7_loss: 0.0040 - decoder_output_8_loss: 0.3853 - decoder_output_9_loss: 0.4012 - decoder_output_accuracy: 0.9770 - decoder_output_1_accuracy: 0.9779 - decoder_output_2_accuracy: 0.8410 - decoder_output_3_accuracy: 0.8367 - decoder_output_4_accuracy: 1.0000 - decoder_output_5_accuracy: 0.9844 - decoder_output_6_accuracy: 0.9074 - decoder_output_7_accuracy: 1.0000 - decoder_output_8_accuracy: 0.8570 - decoder_output_9_accuracy: 0.8628 - val_loss: 2.1381 - val_decoder_output_loss: 0.0427 - val_decoder_output_1_loss: 0.0482 - val_decoder_output_2_loss: 0.3322 - val_decoder_output_3_loss: 0.5150 - val_decoder_output_4_loss: 0.0032 - val_decoder_output_5_loss: 0.0710 - val_decoder_output_6_loss: 0.3205 - val_decoder_output_7_loss: 0.0041 - val_decoder_output_8_loss: 0.3838 - val_decoder_output_9_loss: 0.4173 - val_decoder_output_accuracy: 0.9770 - val_decoder_output_1_accuracy: 0.9790 - val_decoder_output_2_accuracy: 0.8310 - val_decoder_output_3_accuracy: 0.8440 - val_decoder_output_4_accuracy: 1.0000 - val_decoder_output_5_accuracy: 0.9760 - val_decoder_output_6_accuracy: 0.9020 - val_decoder_output_7_accuracy: 1.0000 - val_decoder_output_8_accuracy: 0.8460 - val_decoder_output_9_accuracy: 0.8470\n",
      "Epoch 66/120\n",
      "5/5 [==============================] - 1s 163ms/step - loss: 2.0458 - decoder_output_loss: 0.0446 - decoder_output_1_loss: 0.0458 - decoder_output_2_loss: 0.3114 - decoder_output_3_loss: 0.5076 - decoder_output_4_loss: 0.0041 - decoder_output_5_loss: 0.0501 - decoder_output_6_loss: 0.3160 - decoder_output_7_loss: 0.0040 - decoder_output_8_loss: 0.3719 - decoder_output_9_loss: 0.3902 - decoder_output_accuracy: 0.9778 - decoder_output_1_accuracy: 0.9780 - decoder_output_2_accuracy: 0.8433 - decoder_output_3_accuracy: 0.8458 - decoder_output_4_accuracy: 1.0000 - decoder_output_5_accuracy: 0.9847 - decoder_output_6_accuracy: 0.9122 - decoder_output_7_accuracy: 1.0000 - decoder_output_8_accuracy: 0.8602 - decoder_output_9_accuracy: 0.8664 - val_loss: 2.1121 - val_decoder_output_loss: 0.0424 - val_decoder_output_1_loss: 0.0468 - val_decoder_output_2_loss: 0.3264 - val_decoder_output_3_loss: 0.4896 - val_decoder_output_4_loss: 0.0038 - val_decoder_output_5_loss: 0.0634 - val_decoder_output_6_loss: 0.3145 - val_decoder_output_7_loss: 0.0039 - val_decoder_output_8_loss: 0.3946 - val_decoder_output_9_loss: 0.4267 - val_decoder_output_accuracy: 0.9770 - val_decoder_output_1_accuracy: 0.9800 - val_decoder_output_2_accuracy: 0.8280 - val_decoder_output_3_accuracy: 0.8410 - val_decoder_output_4_accuracy: 1.0000 - val_decoder_output_5_accuracy: 0.9750 - val_decoder_output_6_accuracy: 0.9090 - val_decoder_output_7_accuracy: 1.0000 - val_decoder_output_8_accuracy: 0.8410 - val_decoder_output_9_accuracy: 0.8490\n",
      "Epoch 67/120\n",
      "5/5 [==============================] - 1s 177ms/step - loss: 2.0088 - decoder_output_loss: 0.0440 - decoder_output_1_loss: 0.0446 - decoder_output_2_loss: 0.3085 - decoder_output_3_loss: 0.4895 - decoder_output_4_loss: 0.0039 - decoder_output_5_loss: 0.0493 - decoder_output_6_loss: 0.3102 - decoder_output_7_loss: 0.0037 - decoder_output_8_loss: 0.3726 - decoder_output_9_loss: 0.3825 - decoder_output_accuracy: 0.9777 - decoder_output_1_accuracy: 0.9780 - decoder_output_2_accuracy: 0.8390 - decoder_output_3_accuracy: 0.8531 - decoder_output_4_accuracy: 1.0000 - decoder_output_5_accuracy: 0.9859 - decoder_output_6_accuracy: 0.9147 - decoder_output_7_accuracy: 1.0000 - decoder_output_8_accuracy: 0.8580 - decoder_output_9_accuracy: 0.8678 - val_loss: 2.0901 - val_decoder_output_loss: 0.0418 - val_decoder_output_1_loss: 0.0461 - val_decoder_output_2_loss: 0.3245 - val_decoder_output_3_loss: 0.4793 - val_decoder_output_4_loss: 0.0045 - val_decoder_output_5_loss: 0.0647 - val_decoder_output_6_loss: 0.3203 - val_decoder_output_7_loss: 0.0053 - val_decoder_output_8_loss: 0.3834 - val_decoder_output_9_loss: 0.4203 - val_decoder_output_accuracy: 0.9770 - val_decoder_output_1_accuracy: 0.9800 - val_decoder_output_2_accuracy: 0.8290 - val_decoder_output_3_accuracy: 0.8370 - val_decoder_output_4_accuracy: 1.0000 - val_decoder_output_5_accuracy: 0.9740 - val_decoder_output_6_accuracy: 0.9100 - val_decoder_output_7_accuracy: 1.0000 - val_decoder_output_8_accuracy: 0.8400 - val_decoder_output_9_accuracy: 0.8520\n",
      "Epoch 68/120\n",
      "5/5 [==============================] - 1s 171ms/step - loss: 1.9738 - decoder_output_loss: 0.0442 - decoder_output_1_loss: 0.0447 - decoder_output_2_loss: 0.3072 - decoder_output_3_loss: 0.4794 - decoder_output_4_loss: 0.0041 - decoder_output_5_loss: 0.0523 - decoder_output_6_loss: 0.3022 - decoder_output_7_loss: 0.0042 - decoder_output_8_loss: 0.3649 - decoder_output_9_loss: 0.3706 - decoder_output_accuracy: 0.9776 - decoder_output_1_accuracy: 0.9784 - decoder_output_2_accuracy: 0.8384 - decoder_output_3_accuracy: 0.8518 - decoder_output_4_accuracy: 1.0000 - decoder_output_5_accuracy: 0.9840 - decoder_output_6_accuracy: 0.9146 - decoder_output_7_accuracy: 1.0000 - decoder_output_8_accuracy: 0.8612 - decoder_output_9_accuracy: 0.8713 - val_loss: 2.0374 - val_decoder_output_loss: 0.0411 - val_decoder_output_1_loss: 0.0457 - val_decoder_output_2_loss: 0.3262 - val_decoder_output_3_loss: 0.4884 - val_decoder_output_4_loss: 0.0034 - val_decoder_output_5_loss: 0.0723 - val_decoder_output_6_loss: 0.3061 - val_decoder_output_7_loss: 0.0033 - val_decoder_output_8_loss: 0.3717 - val_decoder_output_9_loss: 0.3794 - val_decoder_output_accuracy: 0.9820 - val_decoder_output_1_accuracy: 0.9790 - val_decoder_output_2_accuracy: 0.8230 - val_decoder_output_3_accuracy: 0.8430 - val_decoder_output_4_accuracy: 1.0000 - val_decoder_output_5_accuracy: 0.9760 - val_decoder_output_6_accuracy: 0.9080 - val_decoder_output_7_accuracy: 1.0000 - val_decoder_output_8_accuracy: 0.8540 - val_decoder_output_9_accuracy: 0.8670\n",
      "Epoch 69/120\n",
      "5/5 [==============================] - 1s 173ms/step - loss: 1.9272 - decoder_output_loss: 0.0437 - decoder_output_1_loss: 0.0441 - decoder_output_2_loss: 0.3064 - decoder_output_3_loss: 0.4704 - decoder_output_4_loss: 0.0036 - decoder_output_5_loss: 0.0489 - decoder_output_6_loss: 0.2947 - decoder_output_7_loss: 0.0034 - decoder_output_8_loss: 0.3563 - decoder_output_9_loss: 0.3555 - decoder_output_accuracy: 0.9780 - decoder_output_1_accuracy: 0.9784 - decoder_output_2_accuracy: 0.8407 - decoder_output_3_accuracy: 0.8563 - decoder_output_4_accuracy: 1.0000 - decoder_output_5_accuracy: 0.9847 - decoder_output_6_accuracy: 0.9162 - decoder_output_7_accuracy: 1.0000 - decoder_output_8_accuracy: 0.8618 - decoder_output_9_accuracy: 0.8778 - val_loss: 1.9466 - val_decoder_output_loss: 0.0413 - val_decoder_output_1_loss: 0.0455 - val_decoder_output_2_loss: 0.3227 - val_decoder_output_3_loss: 0.4426 - val_decoder_output_4_loss: 0.0034 - val_decoder_output_5_loss: 0.0616 - val_decoder_output_6_loss: 0.2913 - val_decoder_output_7_loss: 0.0036 - val_decoder_output_8_loss: 0.3608 - val_decoder_output_9_loss: 0.3738 - val_decoder_output_accuracy: 0.9770 - val_decoder_output_1_accuracy: 0.9800 - val_decoder_output_2_accuracy: 0.8270 - val_decoder_output_3_accuracy: 0.8590 - val_decoder_output_4_accuracy: 1.0000 - val_decoder_output_5_accuracy: 0.9770 - val_decoder_output_6_accuracy: 0.9210 - val_decoder_output_7_accuracy: 1.0000 - val_decoder_output_8_accuracy: 0.8570 - val_decoder_output_9_accuracy: 0.8650\n",
      "Epoch 70/120\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 1.8753 - decoder_output_loss: 0.0435 - decoder_output_1_loss: 0.0440 - decoder_output_2_loss: 0.3046 - decoder_output_3_loss: 0.4452 - decoder_output_4_loss: 0.0037 - decoder_output_5_loss: 0.0464 - decoder_output_6_loss: 0.2856 - decoder_output_7_loss: 0.0036 - decoder_output_8_loss: 0.3499 - decoder_output_9_loss: 0.3487 - decoder_output_accuracy: 0.9778 - decoder_output_1_accuracy: 0.9789 - decoder_output_2_accuracy: 0.8374 - decoder_output_3_accuracy: 0.8672 - decoder_output_4_accuracy: 1.0000 - decoder_output_5_accuracy: 0.9862 - decoder_output_6_accuracy: 0.9203 - decoder_output_7_accuracy: 1.0000 - decoder_output_8_accuracy: 0.8607 - decoder_output_9_accuracy: 0.8777 - val_loss: 1.9301 - val_decoder_output_loss: 0.0403 - val_decoder_output_1_loss: 0.0441 - val_decoder_output_2_loss: 0.3185 - val_decoder_output_3_loss: 0.4323 - val_decoder_output_4_loss: 0.0035 - val_decoder_output_5_loss: 0.0621 - val_decoder_output_6_loss: 0.2905 - val_decoder_output_7_loss: 0.0034 - val_decoder_output_8_loss: 0.3592 - val_decoder_output_9_loss: 0.3761 - val_decoder_output_accuracy: 0.9800 - val_decoder_output_1_accuracy: 0.9790 - val_decoder_output_2_accuracy: 0.8250 - val_decoder_output_3_accuracy: 0.8650 - val_decoder_output_4_accuracy: 1.0000 - val_decoder_output_5_accuracy: 0.9770 - val_decoder_output_6_accuracy: 0.9190 - val_decoder_output_7_accuracy: 1.0000 - val_decoder_output_8_accuracy: 0.8580 - val_decoder_output_9_accuracy: 0.8660\n",
      "Epoch 71/120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 1s 214ms/step - loss: 1.8273 - decoder_output_loss: 0.0429 - decoder_output_1_loss: 0.0433 - decoder_output_2_loss: 0.2992 - decoder_output_3_loss: 0.4309 - decoder_output_4_loss: 0.0037 - decoder_output_5_loss: 0.0456 - decoder_output_6_loss: 0.2780 - decoder_output_7_loss: 0.0039 - decoder_output_8_loss: 0.3413 - decoder_output_9_loss: 0.3385 - decoder_output_accuracy: 0.9769 - decoder_output_1_accuracy: 0.9790 - decoder_output_2_accuracy: 0.8412 - decoder_output_3_accuracy: 0.8769 - decoder_output_4_accuracy: 1.0000 - decoder_output_5_accuracy: 0.9861 - decoder_output_6_accuracy: 0.9221 - decoder_output_7_accuracy: 1.0000 - decoder_output_8_accuracy: 0.8660 - decoder_output_9_accuracy: 0.8838 - val_loss: 1.8800 - val_decoder_output_loss: 0.0398 - val_decoder_output_1_loss: 0.0436 - val_decoder_output_2_loss: 0.3166 - val_decoder_output_3_loss: 0.4219 - val_decoder_output_4_loss: 0.0032 - val_decoder_output_5_loss: 0.0607 - val_decoder_output_6_loss: 0.2801 - val_decoder_output_7_loss: 0.0036 - val_decoder_output_8_loss: 0.3476 - val_decoder_output_9_loss: 0.3628 - val_decoder_output_accuracy: 0.9830 - val_decoder_output_1_accuracy: 0.9810 - val_decoder_output_2_accuracy: 0.8230 - val_decoder_output_3_accuracy: 0.8610 - val_decoder_output_4_accuracy: 1.0000 - val_decoder_output_5_accuracy: 0.9770 - val_decoder_output_6_accuracy: 0.9210 - val_decoder_output_7_accuracy: 1.0000 - val_decoder_output_8_accuracy: 0.8550 - val_decoder_output_9_accuracy: 0.8750\n",
      "Epoch 72/120\n",
      "5/5 [==============================] - 1s 193ms/step - loss: 1.7775 - decoder_output_loss: 0.0430 - decoder_output_1_loss: 0.0432 - decoder_output_2_loss: 0.2978 - decoder_output_3_loss: 0.4170 - decoder_output_4_loss: 0.0034 - decoder_output_5_loss: 0.0446 - decoder_output_6_loss: 0.2714 - decoder_output_7_loss: 0.0035 - decoder_output_8_loss: 0.3289 - decoder_output_9_loss: 0.3246 - decoder_output_accuracy: 0.9779 - decoder_output_1_accuracy: 0.9790 - decoder_output_2_accuracy: 0.8382 - decoder_output_3_accuracy: 0.8794 - decoder_output_4_accuracy: 1.0000 - decoder_output_5_accuracy: 0.9867 - decoder_output_6_accuracy: 0.9247 - decoder_output_7_accuracy: 1.0000 - decoder_output_8_accuracy: 0.8731 - decoder_output_9_accuracy: 0.8889 - val_loss: 1.8432 - val_decoder_output_loss: 0.0404 - val_decoder_output_1_loss: 0.0441 - val_decoder_output_2_loss: 0.3134 - val_decoder_output_3_loss: 0.4001 - val_decoder_output_4_loss: 0.0030 - val_decoder_output_5_loss: 0.0611 - val_decoder_output_6_loss: 0.2799 - val_decoder_output_7_loss: 0.0033 - val_decoder_output_8_loss: 0.3363 - val_decoder_output_9_loss: 0.3616 - val_decoder_output_accuracy: 0.9770 - val_decoder_output_1_accuracy: 0.9800 - val_decoder_output_2_accuracy: 0.8260 - val_decoder_output_3_accuracy: 0.8730 - val_decoder_output_4_accuracy: 1.0000 - val_decoder_output_5_accuracy: 0.9770 - val_decoder_output_6_accuracy: 0.9260 - val_decoder_output_7_accuracy: 1.0000 - val_decoder_output_8_accuracy: 0.8580 - val_decoder_output_9_accuracy: 0.8750\n",
      "Epoch 73/120\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 1.7442 - decoder_output_loss: 0.0422 - decoder_output_1_loss: 0.0428 - decoder_output_2_loss: 0.2973 - decoder_output_3_loss: 0.4005 - decoder_output_4_loss: 0.0034 - decoder_output_5_loss: 0.0445 - decoder_output_6_loss: 0.2650 - decoder_output_7_loss: 0.0034 - decoder_output_8_loss: 0.3243 - decoder_output_9_loss: 0.3208 - decoder_output_accuracy: 0.9777 - decoder_output_1_accuracy: 0.9793 - decoder_output_2_accuracy: 0.8383 - decoder_output_3_accuracy: 0.8882 - decoder_output_4_accuracy: 1.0000 - decoder_output_5_accuracy: 0.9867 - decoder_output_6_accuracy: 0.9252 - decoder_output_7_accuracy: 1.0000 - decoder_output_8_accuracy: 0.8704 - decoder_output_9_accuracy: 0.8896 - val_loss: 1.8095 - val_decoder_output_loss: 0.0411 - val_decoder_output_1_loss: 0.0443 - val_decoder_output_2_loss: 0.3169 - val_decoder_output_3_loss: 0.3913 - val_decoder_output_4_loss: 0.0032 - val_decoder_output_5_loss: 0.0599 - val_decoder_output_6_loss: 0.2710 - val_decoder_output_7_loss: 0.0032 - val_decoder_output_8_loss: 0.3330 - val_decoder_output_9_loss: 0.3455 - val_decoder_output_accuracy: 0.9790 - val_decoder_output_1_accuracy: 0.9800 - val_decoder_output_2_accuracy: 0.8260 - val_decoder_output_3_accuracy: 0.8850 - val_decoder_output_4_accuracy: 1.0000 - val_decoder_output_5_accuracy: 0.9760 - val_decoder_output_6_accuracy: 0.9290 - val_decoder_output_7_accuracy: 1.0000 - val_decoder_output_8_accuracy: 0.8580 - val_decoder_output_9_accuracy: 0.8790\n",
      "Epoch 74/120\n",
      "5/5 [==============================] - 1s 175ms/step - loss: 1.7039 - decoder_output_loss: 0.0420 - decoder_output_1_loss: 0.0424 - decoder_output_2_loss: 0.2953 - decoder_output_3_loss: 0.3894 - decoder_output_4_loss: 0.0034 - decoder_output_5_loss: 0.0429 - decoder_output_6_loss: 0.2596 - decoder_output_7_loss: 0.0033 - decoder_output_8_loss: 0.3157 - decoder_output_9_loss: 0.3099 - decoder_output_accuracy: 0.9773 - decoder_output_1_accuracy: 0.9793 - decoder_output_2_accuracy: 0.8376 - decoder_output_3_accuracy: 0.8896 - decoder_output_4_accuracy: 1.0000 - decoder_output_5_accuracy: 0.9866 - decoder_output_6_accuracy: 0.9277 - decoder_output_7_accuracy: 1.0000 - decoder_output_8_accuracy: 0.8759 - decoder_output_9_accuracy: 0.8942 - val_loss: 1.7839 - val_decoder_output_loss: 0.0401 - val_decoder_output_1_loss: 0.0435 - val_decoder_output_2_loss: 0.3130 - val_decoder_output_3_loss: 0.3767 - val_decoder_output_4_loss: 0.0030 - val_decoder_output_5_loss: 0.0590 - val_decoder_output_6_loss: 0.2687 - val_decoder_output_7_loss: 0.0032 - val_decoder_output_8_loss: 0.3278 - val_decoder_output_9_loss: 0.3488 - val_decoder_output_accuracy: 0.9770 - val_decoder_output_1_accuracy: 0.9820 - val_decoder_output_2_accuracy: 0.8250 - val_decoder_output_3_accuracy: 0.8810 - val_decoder_output_4_accuracy: 1.0000 - val_decoder_output_5_accuracy: 0.9770 - val_decoder_output_6_accuracy: 0.9270 - val_decoder_output_7_accuracy: 1.0000 - val_decoder_output_8_accuracy: 0.8620 - val_decoder_output_9_accuracy: 0.8720\n",
      "Epoch 75/120\n",
      "5/5 [==============================] - 1s 164ms/step - loss: 1.6700 - decoder_output_loss: 0.0417 - decoder_output_1_loss: 0.0421 - decoder_output_2_loss: 0.2927 - decoder_output_3_loss: 0.3780 - decoder_output_4_loss: 0.0032 - decoder_output_5_loss: 0.0423 - decoder_output_6_loss: 0.2523 - decoder_output_7_loss: 0.0034 - decoder_output_8_loss: 0.3086 - decoder_output_9_loss: 0.3058 - decoder_output_accuracy: 0.9782 - decoder_output_1_accuracy: 0.9798 - decoder_output_2_accuracy: 0.8368 - decoder_output_3_accuracy: 0.8967 - decoder_output_4_accuracy: 1.0000 - decoder_output_5_accuracy: 0.9870 - decoder_output_6_accuracy: 0.9317 - decoder_output_7_accuracy: 1.0000 - decoder_output_8_accuracy: 0.8764 - decoder_output_9_accuracy: 0.8944 - val_loss: 1.7568 - val_decoder_output_loss: 0.0404 - val_decoder_output_1_loss: 0.0435 - val_decoder_output_2_loss: 0.3123 - val_decoder_output_3_loss: 0.3653 - val_decoder_output_4_loss: 0.0029 - val_decoder_output_5_loss: 0.0607 - val_decoder_output_6_loss: 0.2697 - val_decoder_output_7_loss: 0.0030 - val_decoder_output_8_loss: 0.3179 - val_decoder_output_9_loss: 0.3409 - val_decoder_output_accuracy: 0.9780 - val_decoder_output_1_accuracy: 0.9800 - val_decoder_output_2_accuracy: 0.8260 - val_decoder_output_3_accuracy: 0.8920 - val_decoder_output_4_accuracy: 1.0000 - val_decoder_output_5_accuracy: 0.9780 - val_decoder_output_6_accuracy: 0.9250 - val_decoder_output_7_accuracy: 1.0000 - val_decoder_output_8_accuracy: 0.8610 - val_decoder_output_9_accuracy: 0.8770\n",
      "Epoch 76/120\n",
      "5/5 [==============================] - 1s 192ms/step - loss: 1.6428 - decoder_output_loss: 0.0414 - decoder_output_1_loss: 0.0420 - decoder_output_2_loss: 0.2904 - decoder_output_3_loss: 0.3659 - decoder_output_4_loss: 0.0033 - decoder_output_5_loss: 0.0423 - decoder_output_6_loss: 0.2523 - decoder_output_7_loss: 0.0033 - decoder_output_8_loss: 0.3036 - decoder_output_9_loss: 0.2983 - decoder_output_accuracy: 0.9781 - decoder_output_1_accuracy: 0.9801 - decoder_output_2_accuracy: 0.8367 - decoder_output_3_accuracy: 0.8996 - decoder_output_4_accuracy: 1.0000 - decoder_output_5_accuracy: 0.9869 - decoder_output_6_accuracy: 0.9299 - decoder_output_7_accuracy: 1.0000 - decoder_output_8_accuracy: 0.8784 - decoder_output_9_accuracy: 0.8969 - val_loss: 1.7255 - val_decoder_output_loss: 0.0398 - val_decoder_output_1_loss: 0.0430 - val_decoder_output_2_loss: 0.3099 - val_decoder_output_3_loss: 0.3557 - val_decoder_output_4_loss: 0.0033 - val_decoder_output_5_loss: 0.0564 - val_decoder_output_6_loss: 0.2597 - val_decoder_output_7_loss: 0.0028 - val_decoder_output_8_loss: 0.3229 - val_decoder_output_9_loss: 0.3320 - val_decoder_output_accuracy: 0.9780 - val_decoder_output_1_accuracy: 0.9800 - val_decoder_output_2_accuracy: 0.8260 - val_decoder_output_3_accuracy: 0.8950 - val_decoder_output_4_accuracy: 1.0000 - val_decoder_output_5_accuracy: 0.9780 - val_decoder_output_6_accuracy: 0.9310 - val_decoder_output_7_accuracy: 1.0000 - val_decoder_output_8_accuracy: 0.8580 - val_decoder_output_9_accuracy: 0.8700\n",
      "Epoch 77/120\n",
      "5/5 [==============================] - 1s 163ms/step - loss: 1.6114 - decoder_output_loss: 0.0409 - decoder_output_1_loss: 0.0414 - decoder_output_2_loss: 0.2895 - decoder_output_3_loss: 0.3558 - decoder_output_4_loss: 0.0033 - decoder_output_5_loss: 0.0408 - decoder_output_6_loss: 0.2451 - decoder_output_7_loss: 0.0032 - decoder_output_8_loss: 0.2988 - decoder_output_9_loss: 0.2926 - decoder_output_accuracy: 0.9784 - decoder_output_1_accuracy: 0.9799 - decoder_output_2_accuracy: 0.8376 - decoder_output_3_accuracy: 0.9046 - decoder_output_4_accuracy: 1.0000 - decoder_output_5_accuracy: 0.9877 - decoder_output_6_accuracy: 0.9331 - decoder_output_7_accuracy: 1.0000 - decoder_output_8_accuracy: 0.8796 - decoder_output_9_accuracy: 0.8977 - val_loss: 1.6873 - val_decoder_output_loss: 0.0386 - val_decoder_output_1_loss: 0.0422 - val_decoder_output_2_loss: 0.3088 - val_decoder_output_3_loss: 0.3451 - val_decoder_output_4_loss: 0.0029 - val_decoder_output_5_loss: 0.0558 - val_decoder_output_6_loss: 0.2565 - val_decoder_output_7_loss: 0.0028 - val_decoder_output_8_loss: 0.3085 - val_decoder_output_9_loss: 0.3261 - val_decoder_output_accuracy: 0.9830 - val_decoder_output_1_accuracy: 0.9820 - val_decoder_output_2_accuracy: 0.8290 - val_decoder_output_3_accuracy: 0.8970 - val_decoder_output_4_accuracy: 1.0000 - val_decoder_output_5_accuracy: 0.9800 - val_decoder_output_6_accuracy: 0.9340 - val_decoder_output_7_accuracy: 1.0000 - val_decoder_output_8_accuracy: 0.8650 - val_decoder_output_9_accuracy: 0.8810\n",
      "Epoch 78/120\n",
      "5/5 [==============================] - 1s 164ms/step - loss: 1.5741 - decoder_output_loss: 0.0408 - decoder_output_1_loss: 0.0412 - decoder_output_2_loss: 0.2860 - decoder_output_3_loss: 0.3450 - decoder_output_4_loss: 0.0031 - decoder_output_5_loss: 0.0400 - decoder_output_6_loss: 0.2398 - decoder_output_7_loss: 0.0031 - decoder_output_8_loss: 0.2897 - decoder_output_9_loss: 0.2854 - decoder_output_accuracy: 0.9779 - decoder_output_1_accuracy: 0.9804 - decoder_output_2_accuracy: 0.8374 - decoder_output_3_accuracy: 0.9041 - decoder_output_4_accuracy: 1.0000 - decoder_output_5_accuracy: 0.9866 - decoder_output_6_accuracy: 0.9333 - decoder_output_7_accuracy: 1.0000 - decoder_output_8_accuracy: 0.8849 - decoder_output_9_accuracy: 0.9012 - val_loss: 1.6599 - val_decoder_output_loss: 0.0383 - val_decoder_output_1_loss: 0.0417 - val_decoder_output_2_loss: 0.3054 - val_decoder_output_3_loss: 0.3328 - val_decoder_output_4_loss: 0.0030 - val_decoder_output_5_loss: 0.0555 - val_decoder_output_6_loss: 0.2471 - val_decoder_output_7_loss: 0.0033 - val_decoder_output_8_loss: 0.3026 - val_decoder_output_9_loss: 0.3300 - val_decoder_output_accuracy: 0.9820 - val_decoder_output_1_accuracy: 0.9820 - val_decoder_output_2_accuracy: 0.8250 - val_decoder_output_3_accuracy: 0.8980 - val_decoder_output_4_accuracy: 1.0000 - val_decoder_output_5_accuracy: 0.9790 - val_decoder_output_6_accuracy: 0.9300 - val_decoder_output_7_accuracy: 1.0000 - val_decoder_output_8_accuracy: 0.8630 - val_decoder_output_9_accuracy: 0.8760\n",
      "Epoch 79/120\n",
      "5/5 [==============================] - 1s 181ms/step - loss: 1.5471 - decoder_output_loss: 0.0411 - decoder_output_1_loss: 0.0412 - decoder_output_2_loss: 0.2867 - decoder_output_3_loss: 0.3310 - decoder_output_4_loss: 0.0030 - decoder_output_5_loss: 0.0398 - decoder_output_6_loss: 0.2343 - decoder_output_7_loss: 0.0032 - decoder_output_8_loss: 0.2839 - decoder_output_9_loss: 0.2829 - decoder_output_accuracy: 0.9777 - decoder_output_1_accuracy: 0.9808 - decoder_output_2_accuracy: 0.8371 - decoder_output_3_accuracy: 0.9133 - decoder_output_4_accuracy: 1.0000 - decoder_output_5_accuracy: 0.9871 - decoder_output_6_accuracy: 0.9339 - decoder_output_7_accuracy: 1.0000 - decoder_output_8_accuracy: 0.8878 - decoder_output_9_accuracy: 0.9020 - val_loss: 1.6250 - val_decoder_output_loss: 0.0381 - val_decoder_output_1_loss: 0.0414 - val_decoder_output_2_loss: 0.3027 - val_decoder_output_3_loss: 0.3289 - val_decoder_output_4_loss: 0.0028 - val_decoder_output_5_loss: 0.0552 - val_decoder_output_6_loss: 0.2459 - val_decoder_output_7_loss: 0.0031 - val_decoder_output_8_loss: 0.2950 - val_decoder_output_9_loss: 0.3119 - val_decoder_output_accuracy: 0.9810 - val_decoder_output_1_accuracy: 0.9810 - val_decoder_output_2_accuracy: 0.8230 - val_decoder_output_3_accuracy: 0.9010 - val_decoder_output_4_accuracy: 1.0000 - val_decoder_output_5_accuracy: 0.9810 - val_decoder_output_6_accuracy: 0.9330 - val_decoder_output_7_accuracy: 1.0000 - val_decoder_output_8_accuracy: 0.8740 - val_decoder_output_9_accuracy: 0.8870\n",
      "Epoch 80/120\n",
      "5/5 [==============================] - 1s 212ms/step - loss: 1.5143 - decoder_output_loss: 0.0400 - decoder_output_1_loss: 0.0405 - decoder_output_2_loss: 0.2835 - decoder_output_3_loss: 0.3247 - decoder_output_4_loss: 0.0029 - decoder_output_5_loss: 0.0395 - decoder_output_6_loss: 0.2296 - decoder_output_7_loss: 0.0032 - decoder_output_8_loss: 0.2752 - decoder_output_9_loss: 0.2752 - decoder_output_accuracy: 0.9786 - decoder_output_1_accuracy: 0.9807 - decoder_output_2_accuracy: 0.8353 - decoder_output_3_accuracy: 0.9140 - decoder_output_4_accuracy: 1.0000 - decoder_output_5_accuracy: 0.9871 - decoder_output_6_accuracy: 0.9348 - decoder_output_7_accuracy: 1.0000 - decoder_output_8_accuracy: 0.8899 - decoder_output_9_accuracy: 0.9051 - val_loss: 1.6020 - val_decoder_output_loss: 0.0377 - val_decoder_output_1_loss: 0.0412 - val_decoder_output_2_loss: 0.3031 - val_decoder_output_3_loss: 0.3135 - val_decoder_output_4_loss: 0.0029 - val_decoder_output_5_loss: 0.0559 - val_decoder_output_6_loss: 0.2418 - val_decoder_output_7_loss: 0.0026 - val_decoder_output_8_loss: 0.2894 - val_decoder_output_9_loss: 0.3140 - val_decoder_output_accuracy: 0.9810 - val_decoder_output_1_accuracy: 0.9810 - val_decoder_output_2_accuracy: 0.8250 - val_decoder_output_3_accuracy: 0.9060 - val_decoder_output_4_accuracy: 1.0000 - val_decoder_output_5_accuracy: 0.9810 - val_decoder_output_6_accuracy: 0.9340 - val_decoder_output_7_accuracy: 1.0000 - val_decoder_output_8_accuracy: 0.8780 - val_decoder_output_9_accuracy: 0.8820\n",
      "Epoch 81/120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 1s 177ms/step - loss: 1.4878 - decoder_output_loss: 0.0398 - decoder_output_1_loss: 0.0403 - decoder_output_2_loss: 0.2825 - decoder_output_3_loss: 0.3143 - decoder_output_4_loss: 0.0029 - decoder_output_5_loss: 0.0397 - decoder_output_6_loss: 0.2264 - decoder_output_7_loss: 0.0032 - decoder_output_8_loss: 0.2702 - decoder_output_9_loss: 0.2684 - decoder_output_accuracy: 0.9790 - decoder_output_1_accuracy: 0.9804 - decoder_output_2_accuracy: 0.8342 - decoder_output_3_accuracy: 0.9162 - decoder_output_4_accuracy: 1.0000 - decoder_output_5_accuracy: 0.9869 - decoder_output_6_accuracy: 0.9362 - decoder_output_7_accuracy: 1.0000 - decoder_output_8_accuracy: 0.8933 - decoder_output_9_accuracy: 0.9061 - val_loss: 1.6057 - val_decoder_output_loss: 0.0383 - val_decoder_output_1_loss: 0.0415 - val_decoder_output_2_loss: 0.3028 - val_decoder_output_3_loss: 0.3079 - val_decoder_output_4_loss: 0.0027 - val_decoder_output_5_loss: 0.0570 - val_decoder_output_6_loss: 0.2422 - val_decoder_output_7_loss: 0.0025 - val_decoder_output_8_loss: 0.2952 - val_decoder_output_9_loss: 0.3157 - val_decoder_output_accuracy: 0.9780 - val_decoder_output_1_accuracy: 0.9810 - val_decoder_output_2_accuracy: 0.8200 - val_decoder_output_3_accuracy: 0.9150 - val_decoder_output_4_accuracy: 1.0000 - val_decoder_output_5_accuracy: 0.9780 - val_decoder_output_6_accuracy: 0.9350 - val_decoder_output_7_accuracy: 1.0000 - val_decoder_output_8_accuracy: 0.8710 - val_decoder_output_9_accuracy: 0.8800\n",
      "Epoch 82/120\n",
      "5/5 [==============================] - 1s 163ms/step - loss: 1.4702 - decoder_output_loss: 0.0398 - decoder_output_1_loss: 0.0402 - decoder_output_2_loss: 0.2823 - decoder_output_3_loss: 0.3043 - decoder_output_4_loss: 0.0028 - decoder_output_5_loss: 0.0390 - decoder_output_6_loss: 0.2226 - decoder_output_7_loss: 0.0030 - decoder_output_8_loss: 0.2686 - decoder_output_9_loss: 0.2677 - decoder_output_accuracy: 0.9789 - decoder_output_1_accuracy: 0.9806 - decoder_output_2_accuracy: 0.8337 - decoder_output_3_accuracy: 0.9197 - decoder_output_4_accuracy: 1.0000 - decoder_output_5_accuracy: 0.9874 - decoder_output_6_accuracy: 0.9368 - decoder_output_7_accuracy: 1.0000 - decoder_output_8_accuracy: 0.8912 - decoder_output_9_accuracy: 0.9059 - val_loss: 1.5717 - val_decoder_output_loss: 0.0377 - val_decoder_output_1_loss: 0.0406 - val_decoder_output_2_loss: 0.2971 - val_decoder_output_3_loss: 0.2990 - val_decoder_output_4_loss: 0.0027 - val_decoder_output_5_loss: 0.0590 - val_decoder_output_6_loss: 0.2453 - val_decoder_output_7_loss: 0.0027 - val_decoder_output_8_loss: 0.2783 - val_decoder_output_9_loss: 0.3095 - val_decoder_output_accuracy: 0.9790 - val_decoder_output_1_accuracy: 0.9810 - val_decoder_output_2_accuracy: 0.8210 - val_decoder_output_3_accuracy: 0.9140 - val_decoder_output_4_accuracy: 1.0000 - val_decoder_output_5_accuracy: 0.9750 - val_decoder_output_6_accuracy: 0.9290 - val_decoder_output_7_accuracy: 1.0000 - val_decoder_output_8_accuracy: 0.8810 - val_decoder_output_9_accuracy: 0.8810\n",
      "Epoch 83/120\n",
      "5/5 [==============================] - 1s 163ms/step - loss: 1.4391 - decoder_output_loss: 0.0392 - decoder_output_1_loss: 0.0396 - decoder_output_2_loss: 0.2798 - decoder_output_3_loss: 0.2987 - decoder_output_4_loss: 0.0027 - decoder_output_5_loss: 0.0379 - decoder_output_6_loss: 0.2174 - decoder_output_7_loss: 0.0029 - decoder_output_8_loss: 0.2606 - decoder_output_9_loss: 0.2603 - decoder_output_accuracy: 0.9793 - decoder_output_1_accuracy: 0.9809 - decoder_output_2_accuracy: 0.8347 - decoder_output_3_accuracy: 0.9224 - decoder_output_4_accuracy: 1.0000 - decoder_output_5_accuracy: 0.9876 - decoder_output_6_accuracy: 0.9374 - decoder_output_7_accuracy: 1.0000 - decoder_output_8_accuracy: 0.8964 - decoder_output_9_accuracy: 0.9078 - val_loss: 1.5464 - val_decoder_output_loss: 0.0383 - val_decoder_output_1_loss: 0.0407 - val_decoder_output_2_loss: 0.2970 - val_decoder_output_3_loss: 0.3034 - val_decoder_output_4_loss: 0.0027 - val_decoder_output_5_loss: 0.0545 - val_decoder_output_6_loss: 0.2351 - val_decoder_output_7_loss: 0.0029 - val_decoder_output_8_loss: 0.2734 - val_decoder_output_9_loss: 0.2983 - val_decoder_output_accuracy: 0.9810 - val_decoder_output_1_accuracy: 0.9810 - val_decoder_output_2_accuracy: 0.8220 - val_decoder_output_3_accuracy: 0.9130 - val_decoder_output_4_accuracy: 1.0000 - val_decoder_output_5_accuracy: 0.9780 - val_decoder_output_6_accuracy: 0.9350 - val_decoder_output_7_accuracy: 1.0000 - val_decoder_output_8_accuracy: 0.8880 - val_decoder_output_9_accuracy: 0.8940\n",
      "Epoch 84/120\n",
      "5/5 [==============================] - 1s 164ms/step - loss: 1.4145 - decoder_output_loss: 0.0390 - decoder_output_1_loss: 0.0394 - decoder_output_2_loss: 0.2787 - decoder_output_3_loss: 0.2910 - decoder_output_4_loss: 0.0028 - decoder_output_5_loss: 0.0381 - decoder_output_6_loss: 0.2143 - decoder_output_7_loss: 0.0028 - decoder_output_8_loss: 0.2516 - decoder_output_9_loss: 0.2568 - decoder_output_accuracy: 0.9800 - decoder_output_1_accuracy: 0.9812 - decoder_output_2_accuracy: 0.8358 - decoder_output_3_accuracy: 0.9234 - decoder_output_4_accuracy: 1.0000 - decoder_output_5_accuracy: 0.9874 - decoder_output_6_accuracy: 0.9402 - decoder_output_7_accuracy: 1.0000 - decoder_output_8_accuracy: 0.8990 - decoder_output_9_accuracy: 0.9097 - val_loss: 1.5100 - val_decoder_output_loss: 0.0367 - val_decoder_output_1_loss: 0.0395 - val_decoder_output_2_loss: 0.2967 - val_decoder_output_3_loss: 0.2869 - val_decoder_output_4_loss: 0.0026 - val_decoder_output_5_loss: 0.0538 - val_decoder_output_6_loss: 0.2297 - val_decoder_output_7_loss: 0.0030 - val_decoder_output_8_loss: 0.2651 - val_decoder_output_9_loss: 0.2962 - val_decoder_output_accuracy: 0.9820 - val_decoder_output_1_accuracy: 0.9820 - val_decoder_output_2_accuracy: 0.8230 - val_decoder_output_3_accuracy: 0.9190 - val_decoder_output_4_accuracy: 1.0000 - val_decoder_output_5_accuracy: 0.9790 - val_decoder_output_6_accuracy: 0.9390 - val_decoder_output_7_accuracy: 1.0000 - val_decoder_output_8_accuracy: 0.8920 - val_decoder_output_9_accuracy: 0.8930\n",
      "Epoch 85/120\n",
      "5/5 [==============================] - 1s 168ms/step - loss: 1.3840 - decoder_output_loss: 0.0385 - decoder_output_1_loss: 0.0388 - decoder_output_2_loss: 0.2770 - decoder_output_3_loss: 0.2824 - decoder_output_4_loss: 0.0027 - decoder_output_5_loss: 0.0371 - decoder_output_6_loss: 0.2097 - decoder_output_7_loss: 0.0028 - decoder_output_8_loss: 0.2467 - decoder_output_9_loss: 0.2483 - decoder_output_accuracy: 0.9792 - decoder_output_1_accuracy: 0.9810 - decoder_output_2_accuracy: 0.8356 - decoder_output_3_accuracy: 0.9272 - decoder_output_4_accuracy: 1.0000 - decoder_output_5_accuracy: 0.9876 - decoder_output_6_accuracy: 0.9410 - decoder_output_7_accuracy: 1.0000 - decoder_output_8_accuracy: 0.9026 - decoder_output_9_accuracy: 0.9136 - val_loss: 1.4885 - val_decoder_output_loss: 0.0362 - val_decoder_output_1_loss: 0.0392 - val_decoder_output_2_loss: 0.2948 - val_decoder_output_3_loss: 0.2751 - val_decoder_output_4_loss: 0.0026 - val_decoder_output_5_loss: 0.0548 - val_decoder_output_6_loss: 0.2274 - val_decoder_output_7_loss: 0.0028 - val_decoder_output_8_loss: 0.2693 - val_decoder_output_9_loss: 0.2863 - val_decoder_output_accuracy: 0.9830 - val_decoder_output_1_accuracy: 0.9830 - val_decoder_output_2_accuracy: 0.8220 - val_decoder_output_3_accuracy: 0.9290 - val_decoder_output_4_accuracy: 1.0000 - val_decoder_output_5_accuracy: 0.9780 - val_decoder_output_6_accuracy: 0.9350 - val_decoder_output_7_accuracy: 1.0000 - val_decoder_output_8_accuracy: 0.8840 - val_decoder_output_9_accuracy: 0.8930\n",
      "Epoch 86/120\n",
      "5/5 [==============================] - 1s 176ms/step - loss: 1.3818 - decoder_output_loss: 0.0382 - decoder_output_1_loss: 0.0386 - decoder_output_2_loss: 0.2758 - decoder_output_3_loss: 0.2758 - decoder_output_4_loss: 0.0028 - decoder_output_5_loss: 0.0375 - decoder_output_6_loss: 0.2090 - decoder_output_7_loss: 0.0029 - decoder_output_8_loss: 0.2523 - decoder_output_9_loss: 0.2489 - decoder_output_accuracy: 0.9787 - decoder_output_1_accuracy: 0.9816 - decoder_output_2_accuracy: 0.8357 - decoder_output_3_accuracy: 0.9303 - decoder_output_4_accuracy: 1.0000 - decoder_output_5_accuracy: 0.9877 - decoder_output_6_accuracy: 0.9390 - decoder_output_7_accuracy: 1.0000 - decoder_output_8_accuracy: 0.8994 - decoder_output_9_accuracy: 0.9078 - val_loss: 1.4834 - val_decoder_output_loss: 0.0368 - val_decoder_output_1_loss: 0.0396 - val_decoder_output_2_loss: 0.2953 - val_decoder_output_3_loss: 0.2788 - val_decoder_output_4_loss: 0.0027 - val_decoder_output_5_loss: 0.0536 - val_decoder_output_6_loss: 0.2312 - val_decoder_output_7_loss: 0.0028 - val_decoder_output_8_loss: 0.2636 - val_decoder_output_9_loss: 0.2790 - val_decoder_output_accuracy: 0.9820 - val_decoder_output_1_accuracy: 0.9810 - val_decoder_output_2_accuracy: 0.8250 - val_decoder_output_3_accuracy: 0.9200 - val_decoder_output_4_accuracy: 1.0000 - val_decoder_output_5_accuracy: 0.9820 - val_decoder_output_6_accuracy: 0.9360 - val_decoder_output_7_accuracy: 1.0000 - val_decoder_output_8_accuracy: 0.8900 - val_decoder_output_9_accuracy: 0.8980\n",
      "Epoch 87/120\n",
      "5/5 [==============================] - 1s 166ms/step - loss: 1.3699 - decoder_output_loss: 0.0377 - decoder_output_1_loss: 0.0379 - decoder_output_2_loss: 0.2735 - decoder_output_3_loss: 0.2750 - decoder_output_4_loss: 0.0027 - decoder_output_5_loss: 0.0379 - decoder_output_6_loss: 0.2078 - decoder_output_7_loss: 0.0028 - decoder_output_8_loss: 0.2504 - decoder_output_9_loss: 0.2442 - decoder_output_accuracy: 0.9798 - decoder_output_1_accuracy: 0.9817 - decoder_output_2_accuracy: 0.8351 - decoder_output_3_accuracy: 0.9260 - decoder_output_4_accuracy: 1.0000 - decoder_output_5_accuracy: 0.9876 - decoder_output_6_accuracy: 0.9411 - decoder_output_7_accuracy: 1.0000 - decoder_output_8_accuracy: 0.8998 - decoder_output_9_accuracy: 0.9140 - val_loss: 1.4572 - val_decoder_output_loss: 0.0361 - val_decoder_output_1_loss: 0.0387 - val_decoder_output_2_loss: 0.2925 - val_decoder_output_3_loss: 0.2664 - val_decoder_output_4_loss: 0.0023 - val_decoder_output_5_loss: 0.0543 - val_decoder_output_6_loss: 0.2214 - val_decoder_output_7_loss: 0.0023 - val_decoder_output_8_loss: 0.2594 - val_decoder_output_9_loss: 0.2838 - val_decoder_output_accuracy: 0.9830 - val_decoder_output_1_accuracy: 0.9820 - val_decoder_output_2_accuracy: 0.8270 - val_decoder_output_3_accuracy: 0.9230 - val_decoder_output_4_accuracy: 1.0000 - val_decoder_output_5_accuracy: 0.9790 - val_decoder_output_6_accuracy: 0.9410 - val_decoder_output_7_accuracy: 1.0000 - val_decoder_output_8_accuracy: 0.8870 - val_decoder_output_9_accuracy: 0.8960\n",
      "Epoch 88/120\n",
      "5/5 [==============================] - 1s 184ms/step - loss: 1.3303 - decoder_output_loss: 0.0376 - decoder_output_1_loss: 0.0377 - decoder_output_2_loss: 0.2708 - decoder_output_3_loss: 0.2634 - decoder_output_4_loss: 0.0025 - decoder_output_5_loss: 0.0355 - decoder_output_6_loss: 0.2007 - decoder_output_7_loss: 0.0028 - decoder_output_8_loss: 0.2402 - decoder_output_9_loss: 0.2392 - decoder_output_accuracy: 0.9799 - decoder_output_1_accuracy: 0.9814 - decoder_output_2_accuracy: 0.8379 - decoder_output_3_accuracy: 0.9327 - decoder_output_4_accuracy: 1.0000 - decoder_output_5_accuracy: 0.9879 - decoder_output_6_accuracy: 0.9438 - decoder_output_7_accuracy: 1.0000 - decoder_output_8_accuracy: 0.9053 - decoder_output_9_accuracy: 0.9143 - val_loss: 1.4389 - val_decoder_output_loss: 0.0354 - val_decoder_output_1_loss: 0.0381 - val_decoder_output_2_loss: 0.2893 - val_decoder_output_3_loss: 0.2630 - val_decoder_output_4_loss: 0.0024 - val_decoder_output_5_loss: 0.0498 - val_decoder_output_6_loss: 0.2224 - val_decoder_output_7_loss: 0.0022 - val_decoder_output_8_loss: 0.2570 - val_decoder_output_9_loss: 0.2794 - val_decoder_output_accuracy: 0.9840 - val_decoder_output_1_accuracy: 0.9820 - val_decoder_output_2_accuracy: 0.8290 - val_decoder_output_3_accuracy: 0.9260 - val_decoder_output_4_accuracy: 1.0000 - val_decoder_output_5_accuracy: 0.9770 - val_decoder_output_6_accuracy: 0.9420 - val_decoder_output_7_accuracy: 1.0000 - val_decoder_output_8_accuracy: 0.8900 - val_decoder_output_9_accuracy: 0.8980\n",
      "Epoch 89/120\n",
      "5/5 [==============================] - 1s 178ms/step - loss: 1.3149 - decoder_output_loss: 0.0370 - decoder_output_1_loss: 0.0371 - decoder_output_2_loss: 0.2685 - decoder_output_3_loss: 0.2590 - decoder_output_4_loss: 0.0026 - decoder_output_5_loss: 0.0349 - decoder_output_6_loss: 0.2013 - decoder_output_7_loss: 0.0025 - decoder_output_8_loss: 0.2342 - decoder_output_9_loss: 0.2379 - decoder_output_accuracy: 0.9800 - decoder_output_1_accuracy: 0.9819 - decoder_output_2_accuracy: 0.8401 - decoder_output_3_accuracy: 0.9346 - decoder_output_4_accuracy: 1.0000 - decoder_output_5_accuracy: 0.9874 - decoder_output_6_accuracy: 0.9420 - decoder_output_7_accuracy: 1.0000 - decoder_output_8_accuracy: 0.9107 - decoder_output_9_accuracy: 0.9142 - val_loss: 1.4334 - val_decoder_output_loss: 0.0351 - val_decoder_output_1_loss: 0.0376 - val_decoder_output_2_loss: 0.2870 - val_decoder_output_3_loss: 0.2555 - val_decoder_output_4_loss: 0.0024 - val_decoder_output_5_loss: 0.0537 - val_decoder_output_6_loss: 0.2141 - val_decoder_output_7_loss: 0.0029 - val_decoder_output_8_loss: 0.2548 - val_decoder_output_9_loss: 0.2903 - val_decoder_output_accuracy: 0.9820 - val_decoder_output_1_accuracy: 0.9830 - val_decoder_output_2_accuracy: 0.8250 - val_decoder_output_3_accuracy: 0.9270 - val_decoder_output_4_accuracy: 1.0000 - val_decoder_output_5_accuracy: 0.9790 - val_decoder_output_6_accuracy: 0.9410 - val_decoder_output_7_accuracy: 1.0000 - val_decoder_output_8_accuracy: 0.8870 - val_decoder_output_9_accuracy: 0.8890\n",
      "Epoch 90/120\n",
      "5/5 [==============================] - 1s 216ms/step - loss: 1.2900 - decoder_output_loss: 0.0368 - decoder_output_1_loss: 0.0369 - decoder_output_2_loss: 0.2683 - decoder_output_3_loss: 0.2526 - decoder_output_4_loss: 0.0026 - decoder_output_5_loss: 0.0347 - decoder_output_6_loss: 0.1929 - decoder_output_7_loss: 0.0026 - decoder_output_8_loss: 0.2296 - decoder_output_9_loss: 0.2328 - decoder_output_accuracy: 0.9802 - decoder_output_1_accuracy: 0.9818 - decoder_output_2_accuracy: 0.8420 - decoder_output_3_accuracy: 0.9393 - decoder_output_4_accuracy: 1.0000 - decoder_output_5_accuracy: 0.9883 - decoder_output_6_accuracy: 0.9443 - decoder_output_7_accuracy: 1.0000 - decoder_output_8_accuracy: 0.9083 - decoder_output_9_accuracy: 0.9179 - val_loss: 1.3629 - val_decoder_output_loss: 0.0349 - val_decoder_output_1_loss: 0.0376 - val_decoder_output_2_loss: 0.2867 - val_decoder_output_3_loss: 0.2504 - val_decoder_output_4_loss: 0.0022 - val_decoder_output_5_loss: 0.0491 - val_decoder_output_6_loss: 0.2077 - val_decoder_output_7_loss: 0.0024 - val_decoder_output_8_loss: 0.2349 - val_decoder_output_9_loss: 0.2571 - val_decoder_output_accuracy: 0.9830 - val_decoder_output_1_accuracy: 0.9830 - val_decoder_output_2_accuracy: 0.8240 - val_decoder_output_3_accuracy: 0.9260 - val_decoder_output_4_accuracy: 1.0000 - val_decoder_output_5_accuracy: 0.9790 - val_decoder_output_6_accuracy: 0.9440 - val_decoder_output_7_accuracy: 1.0000 - val_decoder_output_8_accuracy: 0.9040 - val_decoder_output_9_accuracy: 0.9080\n",
      "Epoch 91/120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 1s 162ms/step - loss: 1.2691 - decoder_output_loss: 0.0364 - decoder_output_1_loss: 0.0365 - decoder_output_2_loss: 0.2653 - decoder_output_3_loss: 0.2472 - decoder_output_4_loss: 0.0025 - decoder_output_5_loss: 0.0353 - decoder_output_6_loss: 0.1890 - decoder_output_7_loss: 0.0026 - decoder_output_8_loss: 0.2211 - decoder_output_9_loss: 0.2333 - decoder_output_accuracy: 0.9801 - decoder_output_1_accuracy: 0.9822 - decoder_output_2_accuracy: 0.8408 - decoder_output_3_accuracy: 0.9364 - decoder_output_4_accuracy: 1.0000 - decoder_output_5_accuracy: 0.9883 - decoder_output_6_accuracy: 0.9460 - decoder_output_7_accuracy: 1.0000 - decoder_output_8_accuracy: 0.9097 - decoder_output_9_accuracy: 0.9149 - val_loss: 1.3886 - val_decoder_output_loss: 0.0348 - val_decoder_output_1_loss: 0.0371 - val_decoder_output_2_loss: 0.2852 - val_decoder_output_3_loss: 0.2432 - val_decoder_output_4_loss: 0.0023 - val_decoder_output_5_loss: 0.0522 - val_decoder_output_6_loss: 0.2108 - val_decoder_output_7_loss: 0.0022 - val_decoder_output_8_loss: 0.2435 - val_decoder_output_9_loss: 0.2773 - val_decoder_output_accuracy: 0.9830 - val_decoder_output_1_accuracy: 0.9830 - val_decoder_output_2_accuracy: 0.8290 - val_decoder_output_3_accuracy: 0.9360 - val_decoder_output_4_accuracy: 1.0000 - val_decoder_output_5_accuracy: 0.9790 - val_decoder_output_6_accuracy: 0.9430 - val_decoder_output_7_accuracy: 1.0000 - val_decoder_output_8_accuracy: 0.8990 - val_decoder_output_9_accuracy: 0.8970\n",
      "Epoch 92/120\n",
      "5/5 [==============================] - 1s 200ms/step - loss: 1.2455 - decoder_output_loss: 0.0361 - decoder_output_1_loss: 0.0361 - decoder_output_2_loss: 0.2630 - decoder_output_3_loss: 0.2421 - decoder_output_4_loss: 0.0025 - decoder_output_5_loss: 0.0354 - decoder_output_6_loss: 0.1901 - decoder_output_7_loss: 0.0025 - decoder_output_8_loss: 0.2124 - decoder_output_9_loss: 0.2253 - decoder_output_accuracy: 0.9803 - decoder_output_1_accuracy: 0.9821 - decoder_output_2_accuracy: 0.8432 - decoder_output_3_accuracy: 0.9392 - decoder_output_4_accuracy: 1.0000 - decoder_output_5_accuracy: 0.9879 - decoder_output_6_accuracy: 0.9466 - decoder_output_7_accuracy: 1.0000 - decoder_output_8_accuracy: 0.9187 - decoder_output_9_accuracy: 0.9212 - val_loss: 1.3423 - val_decoder_output_loss: 0.0350 - val_decoder_output_1_loss: 0.0369 - val_decoder_output_2_loss: 0.2812 - val_decoder_output_3_loss: 0.2379 - val_decoder_output_4_loss: 0.0022 - val_decoder_output_5_loss: 0.0507 - val_decoder_output_6_loss: 0.2065 - val_decoder_output_7_loss: 0.0023 - val_decoder_output_8_loss: 0.2265 - val_decoder_output_9_loss: 0.2631 - val_decoder_output_accuracy: 0.9830 - val_decoder_output_1_accuracy: 0.9830 - val_decoder_output_2_accuracy: 0.8270 - val_decoder_output_3_accuracy: 0.9330 - val_decoder_output_4_accuracy: 1.0000 - val_decoder_output_5_accuracy: 0.9770 - val_decoder_output_6_accuracy: 0.9410 - val_decoder_output_7_accuracy: 1.0000 - val_decoder_output_8_accuracy: 0.8990 - val_decoder_output_9_accuracy: 0.9050\n",
      "Epoch 93/120\n",
      "5/5 [==============================] - 1s 165ms/step - loss: 1.2140 - decoder_output_loss: 0.0358 - decoder_output_1_loss: 0.0359 - decoder_output_2_loss: 0.2613 - decoder_output_3_loss: 0.2368 - decoder_output_4_loss: 0.0024 - decoder_output_5_loss: 0.0336 - decoder_output_6_loss: 0.1846 - decoder_output_7_loss: 0.0025 - decoder_output_8_loss: 0.2047 - decoder_output_9_loss: 0.2163 - decoder_output_accuracy: 0.9807 - decoder_output_1_accuracy: 0.9823 - decoder_output_2_accuracy: 0.8447 - decoder_output_3_accuracy: 0.9406 - decoder_output_4_accuracy: 1.0000 - decoder_output_5_accuracy: 0.9888 - decoder_output_6_accuracy: 0.9472 - decoder_output_7_accuracy: 1.0000 - decoder_output_8_accuracy: 0.9192 - decoder_output_9_accuracy: 0.9238 - val_loss: 1.3513 - val_decoder_output_loss: 0.0338 - val_decoder_output_1_loss: 0.0366 - val_decoder_output_2_loss: 0.2812 - val_decoder_output_3_loss: 0.2384 - val_decoder_output_4_loss: 0.0022 - val_decoder_output_5_loss: 0.0537 - val_decoder_output_6_loss: 0.2074 - val_decoder_output_7_loss: 0.0026 - val_decoder_output_8_loss: 0.2235 - val_decoder_output_9_loss: 0.2719 - val_decoder_output_accuracy: 0.9860 - val_decoder_output_1_accuracy: 0.9830 - val_decoder_output_2_accuracy: 0.8240 - val_decoder_output_3_accuracy: 0.9370 - val_decoder_output_4_accuracy: 1.0000 - val_decoder_output_5_accuracy: 0.9800 - val_decoder_output_6_accuracy: 0.9410 - val_decoder_output_7_accuracy: 1.0000 - val_decoder_output_8_accuracy: 0.9000 - val_decoder_output_9_accuracy: 0.9000\n",
      "Epoch 94/120\n",
      "5/5 [==============================] - 1s 194ms/step - loss: 1.1967 - decoder_output_loss: 0.0356 - decoder_output_1_loss: 0.0355 - decoder_output_2_loss: 0.2603 - decoder_output_3_loss: 0.2315 - decoder_output_4_loss: 0.0025 - decoder_output_5_loss: 0.0336 - decoder_output_6_loss: 0.1805 - decoder_output_7_loss: 0.0028 - decoder_output_8_loss: 0.2027 - decoder_output_9_loss: 0.2118 - decoder_output_accuracy: 0.9806 - decoder_output_1_accuracy: 0.9823 - decoder_output_2_accuracy: 0.8443 - decoder_output_3_accuracy: 0.9422 - decoder_output_4_accuracy: 1.0000 - decoder_output_5_accuracy: 0.9883 - decoder_output_6_accuracy: 0.9472 - decoder_output_7_accuracy: 1.0000 - decoder_output_8_accuracy: 0.9228 - decoder_output_9_accuracy: 0.9238 - val_loss: 1.3058 - val_decoder_output_loss: 0.0337 - val_decoder_output_1_loss: 0.0361 - val_decoder_output_2_loss: 0.2778 - val_decoder_output_3_loss: 0.2321 - val_decoder_output_4_loss: 0.0022 - val_decoder_output_5_loss: 0.0488 - val_decoder_output_6_loss: 0.2028 - val_decoder_output_7_loss: 0.0025 - val_decoder_output_8_loss: 0.2208 - val_decoder_output_9_loss: 0.2489 - val_decoder_output_accuracy: 0.9860 - val_decoder_output_1_accuracy: 0.9830 - val_decoder_output_2_accuracy: 0.8270 - val_decoder_output_3_accuracy: 0.9350 - val_decoder_output_4_accuracy: 1.0000 - val_decoder_output_5_accuracy: 0.9770 - val_decoder_output_6_accuracy: 0.9410 - val_decoder_output_7_accuracy: 1.0000 - val_decoder_output_8_accuracy: 0.9050 - val_decoder_output_9_accuracy: 0.9060\n",
      "Epoch 95/120\n",
      "5/5 [==============================] - 1s 190ms/step - loss: 1.1771 - decoder_output_loss: 0.0352 - decoder_output_1_loss: 0.0352 - decoder_output_2_loss: 0.2582 - decoder_output_3_loss: 0.2270 - decoder_output_4_loss: 0.0023 - decoder_output_5_loss: 0.0335 - decoder_output_6_loss: 0.1794 - decoder_output_7_loss: 0.0025 - decoder_output_8_loss: 0.1963 - decoder_output_9_loss: 0.2076 - decoder_output_accuracy: 0.9809 - decoder_output_1_accuracy: 0.9826 - decoder_output_2_accuracy: 0.8478 - decoder_output_3_accuracy: 0.9458 - decoder_output_4_accuracy: 1.0000 - decoder_output_5_accuracy: 0.9883 - decoder_output_6_accuracy: 0.9476 - decoder_output_7_accuracy: 1.0000 - decoder_output_8_accuracy: 0.9243 - decoder_output_9_accuracy: 0.9267 - val_loss: 1.3005 - val_decoder_output_loss: 0.0335 - val_decoder_output_1_loss: 0.0361 - val_decoder_output_2_loss: 0.2797 - val_decoder_output_3_loss: 0.2218 - val_decoder_output_4_loss: 0.0022 - val_decoder_output_5_loss: 0.0494 - val_decoder_output_6_loss: 0.1953 - val_decoder_output_7_loss: 0.0029 - val_decoder_output_8_loss: 0.2195 - val_decoder_output_9_loss: 0.2602 - val_decoder_output_accuracy: 0.9860 - val_decoder_output_1_accuracy: 0.9830 - val_decoder_output_2_accuracy: 0.8370 - val_decoder_output_3_accuracy: 0.9370 - val_decoder_output_4_accuracy: 1.0000 - val_decoder_output_5_accuracy: 0.9810 - val_decoder_output_6_accuracy: 0.9480 - val_decoder_output_7_accuracy: 1.0000 - val_decoder_output_8_accuracy: 0.9030 - val_decoder_output_9_accuracy: 0.9040\n",
      "Epoch 96/120\n",
      "5/5 [==============================] - 1s 172ms/step - loss: 1.1647 - decoder_output_loss: 0.0350 - decoder_output_1_loss: 0.0351 - decoder_output_2_loss: 0.2574 - decoder_output_3_loss: 0.2217 - decoder_output_4_loss: 0.0024 - decoder_output_5_loss: 0.0337 - decoder_output_6_loss: 0.1770 - decoder_output_7_loss: 0.0024 - decoder_output_8_loss: 0.1918 - decoder_output_9_loss: 0.2082 - decoder_output_accuracy: 0.9804 - decoder_output_1_accuracy: 0.9827 - decoder_output_2_accuracy: 0.8490 - decoder_output_3_accuracy: 0.9463 - decoder_output_4_accuracy: 1.0000 - decoder_output_5_accuracy: 0.9884 - decoder_output_6_accuracy: 0.9497 - decoder_output_7_accuracy: 1.0000 - decoder_output_8_accuracy: 0.9259 - decoder_output_9_accuracy: 0.9252 - val_loss: 1.2801 - val_decoder_output_loss: 0.0337 - val_decoder_output_1_loss: 0.0355 - val_decoder_output_2_loss: 0.2787 - val_decoder_output_3_loss: 0.2264 - val_decoder_output_4_loss: 0.0020 - val_decoder_output_5_loss: 0.0502 - val_decoder_output_6_loss: 0.1979 - val_decoder_output_7_loss: 0.0023 - val_decoder_output_8_loss: 0.2051 - val_decoder_output_9_loss: 0.2484 - val_decoder_output_accuracy: 0.9830 - val_decoder_output_1_accuracy: 0.9830 - val_decoder_output_2_accuracy: 0.8250 - val_decoder_output_3_accuracy: 0.9330 - val_decoder_output_4_accuracy: 1.0000 - val_decoder_output_5_accuracy: 0.9810 - val_decoder_output_6_accuracy: 0.9430 - val_decoder_output_7_accuracy: 1.0000 - val_decoder_output_8_accuracy: 0.9150 - val_decoder_output_9_accuracy: 0.9070\n",
      "Epoch 97/120\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 1.1469 - decoder_output_loss: 0.0345 - decoder_output_1_loss: 0.0346 - decoder_output_2_loss: 0.2569 - decoder_output_3_loss: 0.2196 - decoder_output_4_loss: 0.0023 - decoder_output_5_loss: 0.0330 - decoder_output_6_loss: 0.1746 - decoder_output_7_loss: 0.0024 - decoder_output_8_loss: 0.1869 - decoder_output_9_loss: 0.2019 - decoder_output_accuracy: 0.9809 - decoder_output_1_accuracy: 0.9830 - decoder_output_2_accuracy: 0.8464 - decoder_output_3_accuracy: 0.9473 - decoder_output_4_accuracy: 1.0000 - decoder_output_5_accuracy: 0.9877 - decoder_output_6_accuracy: 0.9489 - decoder_output_7_accuracy: 1.0000 - decoder_output_8_accuracy: 0.9289 - decoder_output_9_accuracy: 0.9298 - val_loss: 1.2638 - val_decoder_output_loss: 0.0334 - val_decoder_output_1_loss: 0.0353 - val_decoder_output_2_loss: 0.2736 - val_decoder_output_3_loss: 0.2166 - val_decoder_output_4_loss: 0.0021 - val_decoder_output_5_loss: 0.0483 - val_decoder_output_6_loss: 0.1956 - val_decoder_output_7_loss: 0.0021 - val_decoder_output_8_loss: 0.2070 - val_decoder_output_9_loss: 0.2498 - val_decoder_output_accuracy: 0.9840 - val_decoder_output_1_accuracy: 0.9830 - val_decoder_output_2_accuracy: 0.8270 - val_decoder_output_3_accuracy: 0.9430 - val_decoder_output_4_accuracy: 1.0000 - val_decoder_output_5_accuracy: 0.9790 - val_decoder_output_6_accuracy: 0.9430 - val_decoder_output_7_accuracy: 1.0000 - val_decoder_output_8_accuracy: 0.9130 - val_decoder_output_9_accuracy: 0.9070\n",
      "Epoch 98/120\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 1.1307 - decoder_output_loss: 0.0342 - decoder_output_1_loss: 0.0343 - decoder_output_2_loss: 0.2532 - decoder_output_3_loss: 0.2147 - decoder_output_4_loss: 0.0023 - decoder_output_5_loss: 0.0330 - decoder_output_6_loss: 0.1738 - decoder_output_7_loss: 0.0025 - decoder_output_8_loss: 0.1814 - decoder_output_9_loss: 0.2012 - decoder_output_accuracy: 0.9810 - decoder_output_1_accuracy: 0.9829 - decoder_output_2_accuracy: 0.8508 - decoder_output_3_accuracy: 0.9469 - decoder_output_4_accuracy: 1.0000 - decoder_output_5_accuracy: 0.9880 - decoder_output_6_accuracy: 0.9499 - decoder_output_7_accuracy: 1.0000 - decoder_output_8_accuracy: 0.9326 - decoder_output_9_accuracy: 0.9290 - val_loss: 1.2689 - val_decoder_output_loss: 0.0327 - val_decoder_output_1_loss: 0.0350 - val_decoder_output_2_loss: 0.2742 - val_decoder_output_3_loss: 0.2192 - val_decoder_output_4_loss: 0.0020 - val_decoder_output_5_loss: 0.0556 - val_decoder_output_6_loss: 0.2058 - val_decoder_output_7_loss: 0.0021 - val_decoder_output_8_loss: 0.1967 - val_decoder_output_9_loss: 0.2454 - val_decoder_output_accuracy: 0.9850 - val_decoder_output_1_accuracy: 0.9840 - val_decoder_output_2_accuracy: 0.8300 - val_decoder_output_3_accuracy: 0.9360 - val_decoder_output_4_accuracy: 1.0000 - val_decoder_output_5_accuracy: 0.9790 - val_decoder_output_6_accuracy: 0.9420 - val_decoder_output_7_accuracy: 1.0000 - val_decoder_output_8_accuracy: 0.9170 - val_decoder_output_9_accuracy: 0.9130\n",
      "Epoch 99/120\n",
      "5/5 [==============================] - 1s 165ms/step - loss: 1.1332 - decoder_output_loss: 0.0339 - decoder_output_1_loss: 0.0340 - decoder_output_2_loss: 0.2533 - decoder_output_3_loss: 0.2137 - decoder_output_4_loss: 0.0022 - decoder_output_5_loss: 0.0342 - decoder_output_6_loss: 0.1764 - decoder_output_7_loss: 0.0022 - decoder_output_8_loss: 0.1800 - decoder_output_9_loss: 0.2032 - decoder_output_accuracy: 0.9818 - decoder_output_1_accuracy: 0.9831 - decoder_output_2_accuracy: 0.8521 - decoder_output_3_accuracy: 0.9474 - decoder_output_4_accuracy: 1.0000 - decoder_output_5_accuracy: 0.9890 - decoder_output_6_accuracy: 0.9477 - decoder_output_7_accuracy: 1.0000 - decoder_output_8_accuracy: 0.9287 - decoder_output_9_accuracy: 0.9258 - val_loss: 1.2386 - val_decoder_output_loss: 0.0322 - val_decoder_output_1_loss: 0.0347 - val_decoder_output_2_loss: 0.2699 - val_decoder_output_3_loss: 0.2113 - val_decoder_output_4_loss: 0.0021 - val_decoder_output_5_loss: 0.0488 - val_decoder_output_6_loss: 0.1908 - val_decoder_output_7_loss: 0.0020 - val_decoder_output_8_loss: 0.1954 - val_decoder_output_9_loss: 0.2515 - val_decoder_output_accuracy: 0.9870 - val_decoder_output_1_accuracy: 0.9830 - val_decoder_output_2_accuracy: 0.8300 - val_decoder_output_3_accuracy: 0.9440 - val_decoder_output_4_accuracy: 1.0000 - val_decoder_output_5_accuracy: 0.9790 - val_decoder_output_6_accuracy: 0.9430 - val_decoder_output_7_accuracy: 1.0000 - val_decoder_output_8_accuracy: 0.9110 - val_decoder_output_9_accuracy: 0.9060\n",
      "Epoch 100/120\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 1.1025 - decoder_output_loss: 0.0339 - decoder_output_1_loss: 0.0339 - decoder_output_2_loss: 0.2500 - decoder_output_3_loss: 0.2080 - decoder_output_4_loss: 0.0023 - decoder_output_5_loss: 0.0320 - decoder_output_6_loss: 0.1687 - decoder_output_7_loss: 0.0023 - decoder_output_8_loss: 0.1778 - decoder_output_9_loss: 0.1938 - decoder_output_accuracy: 0.9816 - decoder_output_1_accuracy: 0.9828 - decoder_output_2_accuracy: 0.8551 - decoder_output_3_accuracy: 0.9477 - decoder_output_4_accuracy: 1.0000 - decoder_output_5_accuracy: 0.9889 - decoder_output_6_accuracy: 0.9504 - decoder_output_7_accuracy: 1.0000 - decoder_output_8_accuracy: 0.9360 - decoder_output_9_accuracy: 0.9289 - val_loss: 1.2187 - val_decoder_output_loss: 0.0317 - val_decoder_output_1_loss: 0.0342 - val_decoder_output_2_loss: 0.2669 - val_decoder_output_3_loss: 0.2086 - val_decoder_output_4_loss: 0.0020 - val_decoder_output_5_loss: 0.0462 - val_decoder_output_6_loss: 0.1887 - val_decoder_output_7_loss: 0.0022 - val_decoder_output_8_loss: 0.1960 - val_decoder_output_9_loss: 0.2422 - val_decoder_output_accuracy: 0.9870 - val_decoder_output_1_accuracy: 0.9840 - val_decoder_output_2_accuracy: 0.8310 - val_decoder_output_3_accuracy: 0.9420 - val_decoder_output_4_accuracy: 1.0000 - val_decoder_output_5_accuracy: 0.9780 - val_decoder_output_6_accuracy: 0.9430 - val_decoder_output_7_accuracy: 1.0000 - val_decoder_output_8_accuracy: 0.9120 - val_decoder_output_9_accuracy: 0.9090\n",
      "Epoch 101/120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 1s 175ms/step - loss: 1.0829 - decoder_output_loss: 0.0340 - decoder_output_1_loss: 0.0337 - decoder_output_2_loss: 0.2471 - decoder_output_3_loss: 0.2034 - decoder_output_4_loss: 0.0021 - decoder_output_5_loss: 0.0309 - decoder_output_6_loss: 0.1667 - decoder_output_7_loss: 0.0022 - decoder_output_8_loss: 0.1722 - decoder_output_9_loss: 0.1905 - decoder_output_accuracy: 0.9813 - decoder_output_1_accuracy: 0.9831 - decoder_output_2_accuracy: 0.8563 - decoder_output_3_accuracy: 0.9514 - decoder_output_4_accuracy: 1.0000 - decoder_output_5_accuracy: 0.9891 - decoder_output_6_accuracy: 0.9518 - decoder_output_7_accuracy: 1.0000 - decoder_output_8_accuracy: 0.9360 - decoder_output_9_accuracy: 0.9331 - val_loss: 1.2009 - val_decoder_output_loss: 0.0315 - val_decoder_output_1_loss: 0.0337 - val_decoder_output_2_loss: 0.2640 - val_decoder_output_3_loss: 0.1999 - val_decoder_output_4_loss: 0.0021 - val_decoder_output_5_loss: 0.0469 - val_decoder_output_6_loss: 0.1847 - val_decoder_output_7_loss: 0.0022 - val_decoder_output_8_loss: 0.1995 - val_decoder_output_9_loss: 0.2363 - val_decoder_output_accuracy: 0.9860 - val_decoder_output_1_accuracy: 0.9840 - val_decoder_output_2_accuracy: 0.8420 - val_decoder_output_3_accuracy: 0.9490 - val_decoder_output_4_accuracy: 1.0000 - val_decoder_output_5_accuracy: 0.9810 - val_decoder_output_6_accuracy: 0.9470 - val_decoder_output_7_accuracy: 1.0000 - val_decoder_output_8_accuracy: 0.9130 - val_decoder_output_9_accuracy: 0.9130\n",
      "Epoch 102/120\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 1.0712 - decoder_output_loss: 0.0332 - decoder_output_1_loss: 0.0334 - decoder_output_2_loss: 0.2446 - decoder_output_3_loss: 0.1963 - decoder_output_4_loss: 0.0022 - decoder_output_5_loss: 0.0309 - decoder_output_6_loss: 0.1627 - decoder_output_7_loss: 0.0024 - decoder_output_8_loss: 0.1749 - decoder_output_9_loss: 0.1904 - decoder_output_accuracy: 0.9817 - decoder_output_1_accuracy: 0.9837 - decoder_output_2_accuracy: 0.8583 - decoder_output_3_accuracy: 0.9562 - decoder_output_4_accuracy: 1.0000 - decoder_output_5_accuracy: 0.9888 - decoder_output_6_accuracy: 0.9520 - decoder_output_7_accuracy: 1.0000 - decoder_output_8_accuracy: 0.9317 - decoder_output_9_accuracy: 0.9309 - val_loss: 1.1760 - val_decoder_output_loss: 0.0318 - val_decoder_output_1_loss: 0.0331 - val_decoder_output_2_loss: 0.2652 - val_decoder_output_3_loss: 0.1994 - val_decoder_output_4_loss: 0.0021 - val_decoder_output_5_loss: 0.0451 - val_decoder_output_6_loss: 0.1844 - val_decoder_output_7_loss: 0.0022 - val_decoder_output_8_loss: 0.1861 - val_decoder_output_9_loss: 0.2267 - val_decoder_output_accuracy: 0.9860 - val_decoder_output_1_accuracy: 0.9840 - val_decoder_output_2_accuracy: 0.8300 - val_decoder_output_3_accuracy: 0.9440 - val_decoder_output_4_accuracy: 1.0000 - val_decoder_output_5_accuracy: 0.9800 - val_decoder_output_6_accuracy: 0.9470 - val_decoder_output_7_accuracy: 1.0000 - val_decoder_output_8_accuracy: 0.9260 - val_decoder_output_9_accuracy: 0.9160\n",
      "Epoch 103/120\n",
      "5/5 [==============================] - 1s 178ms/step - loss: 1.0460 - decoder_output_loss: 0.0329 - decoder_output_1_loss: 0.0328 - decoder_output_2_loss: 0.2431 - decoder_output_3_loss: 0.1943 - decoder_output_4_loss: 0.0022 - decoder_output_5_loss: 0.0298 - decoder_output_6_loss: 0.1594 - decoder_output_7_loss: 0.0023 - decoder_output_8_loss: 0.1653 - decoder_output_9_loss: 0.1841 - decoder_output_accuracy: 0.9817 - decoder_output_1_accuracy: 0.9837 - decoder_output_2_accuracy: 0.8579 - decoder_output_3_accuracy: 0.9552 - decoder_output_4_accuracy: 1.0000 - decoder_output_5_accuracy: 0.9893 - decoder_output_6_accuracy: 0.9537 - decoder_output_7_accuracy: 1.0000 - decoder_output_8_accuracy: 0.9392 - decoder_output_9_accuracy: 0.9336 - val_loss: 1.1688 - val_decoder_output_loss: 0.0316 - val_decoder_output_1_loss: 0.0327 - val_decoder_output_2_loss: 0.2613 - val_decoder_output_3_loss: 0.1951 - val_decoder_output_4_loss: 0.0020 - val_decoder_output_5_loss: 0.0499 - val_decoder_output_6_loss: 0.1830 - val_decoder_output_7_loss: 0.0021 - val_decoder_output_8_loss: 0.1772 - val_decoder_output_9_loss: 0.2338 - val_decoder_output_accuracy: 0.9850 - val_decoder_output_1_accuracy: 0.9840 - val_decoder_output_2_accuracy: 0.8360 - val_decoder_output_3_accuracy: 0.9540 - val_decoder_output_4_accuracy: 1.0000 - val_decoder_output_5_accuracy: 0.9820 - val_decoder_output_6_accuracy: 0.9420 - val_decoder_output_7_accuracy: 1.0000 - val_decoder_output_8_accuracy: 0.9280 - val_decoder_output_9_accuracy: 0.9150\n",
      "Epoch 104/120\n",
      "5/5 [==============================] - 1s 167ms/step - loss: 1.0380 - decoder_output_loss: 0.0328 - decoder_output_1_loss: 0.0324 - decoder_output_2_loss: 0.2426 - decoder_output_3_loss: 0.1922 - decoder_output_4_loss: 0.0022 - decoder_output_5_loss: 0.0302 - decoder_output_6_loss: 0.1580 - decoder_output_7_loss: 0.0023 - decoder_output_8_loss: 0.1638 - decoder_output_9_loss: 0.1817 - decoder_output_accuracy: 0.9828 - decoder_output_1_accuracy: 0.9834 - decoder_output_2_accuracy: 0.8624 - decoder_output_3_accuracy: 0.9547 - decoder_output_4_accuracy: 1.0000 - decoder_output_5_accuracy: 0.9897 - decoder_output_6_accuracy: 0.9537 - decoder_output_7_accuracy: 1.0000 - decoder_output_8_accuracy: 0.9392 - decoder_output_9_accuracy: 0.9340 - val_loss: 1.1588 - val_decoder_output_loss: 0.0314 - val_decoder_output_1_loss: 0.0330 - val_decoder_output_2_loss: 0.2580 - val_decoder_output_3_loss: 0.1957 - val_decoder_output_4_loss: 0.0021 - val_decoder_output_5_loss: 0.0455 - val_decoder_output_6_loss: 0.1817 - val_decoder_output_7_loss: 0.0022 - val_decoder_output_8_loss: 0.1781 - val_decoder_output_9_loss: 0.2310 - val_decoder_output_accuracy: 0.9860 - val_decoder_output_1_accuracy: 0.9830 - val_decoder_output_2_accuracy: 0.8390 - val_decoder_output_3_accuracy: 0.9490 - val_decoder_output_4_accuracy: 1.0000 - val_decoder_output_5_accuracy: 0.9810 - val_decoder_output_6_accuracy: 0.9450 - val_decoder_output_7_accuracy: 1.0000 - val_decoder_output_8_accuracy: 0.9290 - val_decoder_output_9_accuracy: 0.9150\n",
      "Epoch 105/120\n",
      "5/5 [==============================] - 1s 188ms/step - loss: 1.0190 - decoder_output_loss: 0.0322 - decoder_output_1_loss: 0.0321 - decoder_output_2_loss: 0.2396 - decoder_output_3_loss: 0.1873 - decoder_output_4_loss: 0.0022 - decoder_output_5_loss: 0.0301 - decoder_output_6_loss: 0.1558 - decoder_output_7_loss: 0.0022 - decoder_output_8_loss: 0.1579 - decoder_output_9_loss: 0.1795 - decoder_output_accuracy: 0.9820 - decoder_output_1_accuracy: 0.9839 - decoder_output_2_accuracy: 0.8611 - decoder_output_3_accuracy: 0.9533 - decoder_output_4_accuracy: 1.0000 - decoder_output_5_accuracy: 0.9894 - decoder_output_6_accuracy: 0.9560 - decoder_output_7_accuracy: 1.0000 - decoder_output_8_accuracy: 0.9433 - decoder_output_9_accuracy: 0.9362 - val_loss: 1.1272 - val_decoder_output_loss: 0.0311 - val_decoder_output_1_loss: 0.0321 - val_decoder_output_2_loss: 0.2584 - val_decoder_output_3_loss: 0.1875 - val_decoder_output_4_loss: 0.0019 - val_decoder_output_5_loss: 0.0480 - val_decoder_output_6_loss: 0.1802 - val_decoder_output_7_loss: 0.0020 - val_decoder_output_8_loss: 0.1680 - val_decoder_output_9_loss: 0.2179 - val_decoder_output_accuracy: 0.9860 - val_decoder_output_1_accuracy: 0.9840 - val_decoder_output_2_accuracy: 0.8430 - val_decoder_output_3_accuracy: 0.9530 - val_decoder_output_4_accuracy: 1.0000 - val_decoder_output_5_accuracy: 0.9810 - val_decoder_output_6_accuracy: 0.9480 - val_decoder_output_7_accuracy: 1.0000 - val_decoder_output_8_accuracy: 0.9360 - val_decoder_output_9_accuracy: 0.9250\n",
      "Epoch 106/120\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 1.0060 - decoder_output_loss: 0.0319 - decoder_output_1_loss: 0.0317 - decoder_output_2_loss: 0.2364 - decoder_output_3_loss: 0.1835 - decoder_output_4_loss: 0.0021 - decoder_output_5_loss: 0.0299 - decoder_output_6_loss: 0.1535 - decoder_output_7_loss: 0.0022 - decoder_output_8_loss: 0.1561 - decoder_output_9_loss: 0.1786 - decoder_output_accuracy: 0.9827 - decoder_output_1_accuracy: 0.9838 - decoder_output_2_accuracy: 0.8677 - decoder_output_3_accuracy: 0.9572 - decoder_output_4_accuracy: 1.0000 - decoder_output_5_accuracy: 0.9894 - decoder_output_6_accuracy: 0.9550 - decoder_output_7_accuracy: 1.0000 - decoder_output_8_accuracy: 0.9453 - decoder_output_9_accuracy: 0.9351 - val_loss: 1.1375 - val_decoder_output_loss: 0.0307 - val_decoder_output_1_loss: 0.0319 - val_decoder_output_2_loss: 0.2578 - val_decoder_output_3_loss: 0.1882 - val_decoder_output_4_loss: 0.0019 - val_decoder_output_5_loss: 0.0459 - val_decoder_output_6_loss: 0.1774 - val_decoder_output_7_loss: 0.0021 - val_decoder_output_8_loss: 0.1726 - val_decoder_output_9_loss: 0.2291 - val_decoder_output_accuracy: 0.9870 - val_decoder_output_1_accuracy: 0.9840 - val_decoder_output_2_accuracy: 0.8390 - val_decoder_output_3_accuracy: 0.9540 - val_decoder_output_4_accuracy: 1.0000 - val_decoder_output_5_accuracy: 0.9820 - val_decoder_output_6_accuracy: 0.9470 - val_decoder_output_7_accuracy: 1.0000 - val_decoder_output_8_accuracy: 0.9240 - val_decoder_output_9_accuracy: 0.9110\n",
      "Epoch 107/120\n",
      "5/5 [==============================] - 1s 164ms/step - loss: 0.9964 - decoder_output_loss: 0.0315 - decoder_output_1_loss: 0.0314 - decoder_output_2_loss: 0.2354 - decoder_output_3_loss: 0.1822 - decoder_output_4_loss: 0.0020 - decoder_output_5_loss: 0.0286 - decoder_output_6_loss: 0.1505 - decoder_output_7_loss: 0.0021 - decoder_output_8_loss: 0.1547 - decoder_output_9_loss: 0.1781 - decoder_output_accuracy: 0.9830 - decoder_output_1_accuracy: 0.9834 - decoder_output_2_accuracy: 0.8666 - decoder_output_3_accuracy: 0.9562 - decoder_output_4_accuracy: 1.0000 - decoder_output_5_accuracy: 0.9898 - decoder_output_6_accuracy: 0.9553 - decoder_output_7_accuracy: 1.0000 - decoder_output_8_accuracy: 0.9453 - decoder_output_9_accuracy: 0.9374 - val_loss: 1.1587 - val_decoder_output_loss: 0.0299 - val_decoder_output_1_loss: 0.0314 - val_decoder_output_2_loss: 0.2534 - val_decoder_output_3_loss: 0.1861 - val_decoder_output_4_loss: 0.0020 - val_decoder_output_5_loss: 0.0465 - val_decoder_output_6_loss: 0.1864 - val_decoder_output_7_loss: 0.0021 - val_decoder_output_8_loss: 0.1878 - val_decoder_output_9_loss: 0.2332 - val_decoder_output_accuracy: 0.9870 - val_decoder_output_1_accuracy: 0.9840 - val_decoder_output_2_accuracy: 0.8520 - val_decoder_output_3_accuracy: 0.9490 - val_decoder_output_4_accuracy: 1.0000 - val_decoder_output_5_accuracy: 0.9780 - val_decoder_output_6_accuracy: 0.9440 - val_decoder_output_7_accuracy: 1.0000 - val_decoder_output_8_accuracy: 0.9170 - val_decoder_output_9_accuracy: 0.9010\n",
      "Epoch 108/120\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 1.0008 - decoder_output_loss: 0.0312 - decoder_output_1_loss: 0.0311 - decoder_output_2_loss: 0.2317 - decoder_output_3_loss: 0.1800 - decoder_output_4_loss: 0.0021 - decoder_output_5_loss: 0.0318 - decoder_output_6_loss: 0.1576 - decoder_output_7_loss: 0.0022 - decoder_output_8_loss: 0.1554 - decoder_output_9_loss: 0.1777 - decoder_output_accuracy: 0.9827 - decoder_output_1_accuracy: 0.9844 - decoder_output_2_accuracy: 0.8702 - decoder_output_3_accuracy: 0.9567 - decoder_output_4_accuracy: 1.0000 - decoder_output_5_accuracy: 0.9884 - decoder_output_6_accuracy: 0.9528 - decoder_output_7_accuracy: 1.0000 - decoder_output_8_accuracy: 0.9458 - decoder_output_9_accuracy: 0.9357 - val_loss: 1.1176 - val_decoder_output_loss: 0.0301 - val_decoder_output_1_loss: 0.0313 - val_decoder_output_2_loss: 0.2580 - val_decoder_output_3_loss: 0.1837 - val_decoder_output_4_loss: 0.0017 - val_decoder_output_5_loss: 0.0483 - val_decoder_output_6_loss: 0.1766 - val_decoder_output_7_loss: 0.0021 - val_decoder_output_8_loss: 0.1662 - val_decoder_output_9_loss: 0.2197 - val_decoder_output_accuracy: 0.9870 - val_decoder_output_1_accuracy: 0.9850 - val_decoder_output_2_accuracy: 0.8440 - val_decoder_output_3_accuracy: 0.9530 - val_decoder_output_4_accuracy: 1.0000 - val_decoder_output_5_accuracy: 0.9820 - val_decoder_output_6_accuracy: 0.9510 - val_decoder_output_7_accuracy: 1.0000 - val_decoder_output_8_accuracy: 0.9260 - val_decoder_output_9_accuracy: 0.9190\n",
      "Epoch 109/120\n",
      "5/5 [==============================] - 1s 164ms/step - loss: 0.9795 - decoder_output_loss: 0.0310 - decoder_output_1_loss: 0.0307 - decoder_output_2_loss: 0.2317 - decoder_output_3_loss: 0.1788 - decoder_output_4_loss: 0.0019 - decoder_output_5_loss: 0.0287 - decoder_output_6_loss: 0.1519 - decoder_output_7_loss: 0.0021 - decoder_output_8_loss: 0.1489 - decoder_output_9_loss: 0.1738 - decoder_output_accuracy: 0.9831 - decoder_output_1_accuracy: 0.9847 - decoder_output_2_accuracy: 0.8712 - decoder_output_3_accuracy: 0.9566 - decoder_output_4_accuracy: 1.0000 - decoder_output_5_accuracy: 0.9891 - decoder_output_6_accuracy: 0.9543 - decoder_output_7_accuracy: 1.0000 - decoder_output_8_accuracy: 0.9454 - decoder_output_9_accuracy: 0.9383 - val_loss: 1.0883 - val_decoder_output_loss: 0.0296 - val_decoder_output_1_loss: 0.0307 - val_decoder_output_2_loss: 0.2486 - val_decoder_output_3_loss: 0.1764 - val_decoder_output_4_loss: 0.0018 - val_decoder_output_5_loss: 0.0481 - val_decoder_output_6_loss: 0.1738 - val_decoder_output_7_loss: 0.0019 - val_decoder_output_8_loss: 0.1619 - val_decoder_output_9_loss: 0.2154 - val_decoder_output_accuracy: 0.9880 - val_decoder_output_1_accuracy: 0.9840 - val_decoder_output_2_accuracy: 0.8530 - val_decoder_output_3_accuracy: 0.9580 - val_decoder_output_4_accuracy: 1.0000 - val_decoder_output_5_accuracy: 0.9810 - val_decoder_output_6_accuracy: 0.9490 - val_decoder_output_7_accuracy: 1.0000 - val_decoder_output_8_accuracy: 0.9320 - val_decoder_output_9_accuracy: 0.9190\n",
      "Epoch 110/120\n",
      "5/5 [==============================] - 1s 164ms/step - loss: 0.9614 - decoder_output_loss: 0.0308 - decoder_output_1_loss: 0.0306 - decoder_output_2_loss: 0.2275 - decoder_output_3_loss: 0.1755 - decoder_output_4_loss: 0.0020 - decoder_output_5_loss: 0.0294 - decoder_output_6_loss: 0.1491 - decoder_output_7_loss: 0.0021 - decoder_output_8_loss: 0.1444 - decoder_output_9_loss: 0.1700 - decoder_output_accuracy: 0.9841 - decoder_output_1_accuracy: 0.9843 - decoder_output_2_accuracy: 0.8740 - decoder_output_3_accuracy: 0.9592 - decoder_output_4_accuracy: 1.0000 - decoder_output_5_accuracy: 0.9892 - decoder_output_6_accuracy: 0.9571 - decoder_output_7_accuracy: 1.0000 - decoder_output_8_accuracy: 0.9478 - decoder_output_9_accuracy: 0.9386 - val_loss: 1.0912 - val_decoder_output_loss: 0.0296 - val_decoder_output_1_loss: 0.0310 - val_decoder_output_2_loss: 0.2475 - val_decoder_output_3_loss: 0.1784 - val_decoder_output_4_loss: 0.0020 - val_decoder_output_5_loss: 0.0451 - val_decoder_output_6_loss: 0.1740 - val_decoder_output_7_loss: 0.0020 - val_decoder_output_8_loss: 0.1657 - val_decoder_output_9_loss: 0.2159 - val_decoder_output_accuracy: 0.9870 - val_decoder_output_1_accuracy: 0.9850 - val_decoder_output_2_accuracy: 0.8520 - val_decoder_output_3_accuracy: 0.9530 - val_decoder_output_4_accuracy: 1.0000 - val_decoder_output_5_accuracy: 0.9780 - val_decoder_output_6_accuracy: 0.9460 - val_decoder_output_7_accuracy: 1.0000 - val_decoder_output_8_accuracy: 0.9330 - val_decoder_output_9_accuracy: 0.9160\n",
      "Epoch 111/120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 1s 162ms/step - loss: 0.9420 - decoder_output_loss: 0.0303 - decoder_output_1_loss: 0.0300 - decoder_output_2_loss: 0.2251 - decoder_output_3_loss: 0.1708 - decoder_output_4_loss: 0.0020 - decoder_output_5_loss: 0.0287 - decoder_output_6_loss: 0.1448 - decoder_output_7_loss: 0.0019 - decoder_output_8_loss: 0.1423 - decoder_output_9_loss: 0.1660 - decoder_output_accuracy: 0.9841 - decoder_output_1_accuracy: 0.9846 - decoder_output_2_accuracy: 0.8797 - decoder_output_3_accuracy: 0.9607 - decoder_output_4_accuracy: 1.0000 - decoder_output_5_accuracy: 0.9897 - decoder_output_6_accuracy: 0.9579 - decoder_output_7_accuracy: 1.0000 - decoder_output_8_accuracy: 0.9484 - decoder_output_9_accuracy: 0.9421 - val_loss: 1.0644 - val_decoder_output_loss: 0.0291 - val_decoder_output_1_loss: 0.0305 - val_decoder_output_2_loss: 0.2435 - val_decoder_output_3_loss: 0.1721 - val_decoder_output_4_loss: 0.0018 - val_decoder_output_5_loss: 0.0485 - val_decoder_output_6_loss: 0.1699 - val_decoder_output_7_loss: 0.0018 - val_decoder_output_8_loss: 0.1534 - val_decoder_output_9_loss: 0.2138 - val_decoder_output_accuracy: 0.9880 - val_decoder_output_1_accuracy: 0.9850 - val_decoder_output_2_accuracy: 0.8530 - val_decoder_output_3_accuracy: 0.9610 - val_decoder_output_4_accuracy: 1.0000 - val_decoder_output_5_accuracy: 0.9820 - val_decoder_output_6_accuracy: 0.9530 - val_decoder_output_7_accuracy: 1.0000 - val_decoder_output_8_accuracy: 0.9440 - val_decoder_output_9_accuracy: 0.9240\n",
      "Epoch 112/120\n",
      "5/5 [==============================] - 1s 184ms/step - loss: 0.9298 - decoder_output_loss: 0.0301 - decoder_output_1_loss: 0.0300 - decoder_output_2_loss: 0.2224 - decoder_output_3_loss: 0.1680 - decoder_output_4_loss: 0.0021 - decoder_output_5_loss: 0.0287 - decoder_output_6_loss: 0.1425 - decoder_output_7_loss: 0.0022 - decoder_output_8_loss: 0.1410 - decoder_output_9_loss: 0.1627 - decoder_output_accuracy: 0.9843 - decoder_output_1_accuracy: 0.9856 - decoder_output_2_accuracy: 0.8786 - decoder_output_3_accuracy: 0.9601 - decoder_output_4_accuracy: 1.0000 - decoder_output_5_accuracy: 0.9890 - decoder_output_6_accuracy: 0.9582 - decoder_output_7_accuracy: 1.0000 - decoder_output_8_accuracy: 0.9493 - decoder_output_9_accuracy: 0.9420 - val_loss: 1.0560 - val_decoder_output_loss: 0.0289 - val_decoder_output_1_loss: 0.0303 - val_decoder_output_2_loss: 0.2421 - val_decoder_output_3_loss: 0.1693 - val_decoder_output_4_loss: 0.0019 - val_decoder_output_5_loss: 0.0449 - val_decoder_output_6_loss: 0.1700 - val_decoder_output_7_loss: 0.0019 - val_decoder_output_8_loss: 0.1533 - val_decoder_output_9_loss: 0.2134 - val_decoder_output_accuracy: 0.9890 - val_decoder_output_1_accuracy: 0.9850 - val_decoder_output_2_accuracy: 0.8520 - val_decoder_output_3_accuracy: 0.9600 - val_decoder_output_4_accuracy: 1.0000 - val_decoder_output_5_accuracy: 0.9790 - val_decoder_output_6_accuracy: 0.9470 - val_decoder_output_7_accuracy: 1.0000 - val_decoder_output_8_accuracy: 0.9380 - val_decoder_output_9_accuracy: 0.9260\n",
      "Epoch 113/120\n",
      "5/5 [==============================] - 1s 164ms/step - loss: 0.9118 - decoder_output_loss: 0.0298 - decoder_output_1_loss: 0.0296 - decoder_output_2_loss: 0.2201 - decoder_output_3_loss: 0.1643 - decoder_output_4_loss: 0.0019 - decoder_output_5_loss: 0.0274 - decoder_output_6_loss: 0.1410 - decoder_output_7_loss: 0.0020 - decoder_output_8_loss: 0.1360 - decoder_output_9_loss: 0.1596 - decoder_output_accuracy: 0.9847 - decoder_output_1_accuracy: 0.9857 - decoder_output_2_accuracy: 0.8810 - decoder_output_3_accuracy: 0.9603 - decoder_output_4_accuracy: 1.0000 - decoder_output_5_accuracy: 0.9892 - decoder_output_6_accuracy: 0.9583 - decoder_output_7_accuracy: 1.0000 - decoder_output_8_accuracy: 0.9520 - decoder_output_9_accuracy: 0.9456 - val_loss: 1.0547 - val_decoder_output_loss: 0.0284 - val_decoder_output_1_loss: 0.0298 - val_decoder_output_2_loss: 0.2425 - val_decoder_output_3_loss: 0.1676 - val_decoder_output_4_loss: 0.0019 - val_decoder_output_5_loss: 0.0430 - val_decoder_output_6_loss: 0.1694 - val_decoder_output_7_loss: 0.0019 - val_decoder_output_8_loss: 0.1527 - val_decoder_output_9_loss: 0.2176 - val_decoder_output_accuracy: 0.9880 - val_decoder_output_1_accuracy: 0.9870 - val_decoder_output_2_accuracy: 0.8560 - val_decoder_output_3_accuracy: 0.9590 - val_decoder_output_4_accuracy: 1.0000 - val_decoder_output_5_accuracy: 0.9800 - val_decoder_output_6_accuracy: 0.9470 - val_decoder_output_7_accuracy: 1.0000 - val_decoder_output_8_accuracy: 0.9380 - val_decoder_output_9_accuracy: 0.9150\n",
      "Epoch 114/120\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.9051 - decoder_output_loss: 0.0299 - decoder_output_1_loss: 0.0295 - decoder_output_2_loss: 0.2179 - decoder_output_3_loss: 0.1638 - decoder_output_4_loss: 0.0020 - decoder_output_5_loss: 0.0274 - decoder_output_6_loss: 0.1381 - decoder_output_7_loss: 0.0020 - decoder_output_8_loss: 0.1354 - decoder_output_9_loss: 0.1592 - decoder_output_accuracy: 0.9843 - decoder_output_1_accuracy: 0.9852 - decoder_output_2_accuracy: 0.8856 - decoder_output_3_accuracy: 0.9618 - decoder_output_4_accuracy: 1.0000 - decoder_output_5_accuracy: 0.9891 - decoder_output_6_accuracy: 0.9582 - decoder_output_7_accuracy: 1.0000 - decoder_output_8_accuracy: 0.9539 - decoder_output_9_accuracy: 0.9427 - val_loss: 1.0380 - val_decoder_output_loss: 0.0284 - val_decoder_output_1_loss: 0.0296 - val_decoder_output_2_loss: 0.2384 - val_decoder_output_3_loss: 0.1711 - val_decoder_output_4_loss: 0.0019 - val_decoder_output_5_loss: 0.0421 - val_decoder_output_6_loss: 0.1685 - val_decoder_output_7_loss: 0.0021 - val_decoder_output_8_loss: 0.1534 - val_decoder_output_9_loss: 0.2026 - val_decoder_output_accuracy: 0.9880 - val_decoder_output_1_accuracy: 0.9870 - val_decoder_output_2_accuracy: 0.8600 - val_decoder_output_3_accuracy: 0.9520 - val_decoder_output_4_accuracy: 1.0000 - val_decoder_output_5_accuracy: 0.9790 - val_decoder_output_6_accuracy: 0.9520 - val_decoder_output_7_accuracy: 1.0000 - val_decoder_output_8_accuracy: 0.9440 - val_decoder_output_9_accuracy: 0.9230\n",
      "Epoch 115/120\n",
      "5/5 [==============================] - 1s 194ms/step - loss: 0.8960 - decoder_output_loss: 0.0289 - decoder_output_1_loss: 0.0288 - decoder_output_2_loss: 0.2161 - decoder_output_3_loss: 0.1636 - decoder_output_4_loss: 0.0019 - decoder_output_5_loss: 0.0284 - decoder_output_6_loss: 0.1371 - decoder_output_7_loss: 0.0019 - decoder_output_8_loss: 0.1345 - decoder_output_9_loss: 0.1547 - decoder_output_accuracy: 0.9854 - decoder_output_1_accuracy: 0.9862 - decoder_output_2_accuracy: 0.8861 - decoder_output_3_accuracy: 0.9609 - decoder_output_4_accuracy: 1.0000 - decoder_output_5_accuracy: 0.9900 - decoder_output_6_accuracy: 0.9597 - decoder_output_7_accuracy: 1.0000 - decoder_output_8_accuracy: 0.9543 - decoder_output_9_accuracy: 0.9481 - val_loss: 1.0150 - val_decoder_output_loss: 0.0284 - val_decoder_output_1_loss: 0.0294 - val_decoder_output_2_loss: 0.2342 - val_decoder_output_3_loss: 0.1630 - val_decoder_output_4_loss: 0.0018 - val_decoder_output_5_loss: 0.0438 - val_decoder_output_6_loss: 0.1647 - val_decoder_output_7_loss: 0.0019 - val_decoder_output_8_loss: 0.1458 - val_decoder_output_9_loss: 0.2020 - val_decoder_output_accuracy: 0.9900 - val_decoder_output_1_accuracy: 0.9860 - val_decoder_output_2_accuracy: 0.8590 - val_decoder_output_3_accuracy: 0.9580 - val_decoder_output_4_accuracy: 1.0000 - val_decoder_output_5_accuracy: 0.9810 - val_decoder_output_6_accuracy: 0.9500 - val_decoder_output_7_accuracy: 1.0000 - val_decoder_output_8_accuracy: 0.9430 - val_decoder_output_9_accuracy: 0.9290\n",
      "Epoch 116/120\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.8809 - decoder_output_loss: 0.0291 - decoder_output_1_loss: 0.0289 - decoder_output_2_loss: 0.2120 - decoder_output_3_loss: 0.1600 - decoder_output_4_loss: 0.0019 - decoder_output_5_loss: 0.0272 - decoder_output_6_loss: 0.1367 - decoder_output_7_loss: 0.0020 - decoder_output_8_loss: 0.1287 - decoder_output_9_loss: 0.1545 - decoder_output_accuracy: 0.9851 - decoder_output_1_accuracy: 0.9859 - decoder_output_2_accuracy: 0.8917 - decoder_output_3_accuracy: 0.9628 - decoder_output_4_accuracy: 1.0000 - decoder_output_5_accuracy: 0.9894 - decoder_output_6_accuracy: 0.9594 - decoder_output_7_accuracy: 1.0000 - decoder_output_8_accuracy: 0.9562 - decoder_output_9_accuracy: 0.9462 - val_loss: 1.0400 - val_decoder_output_loss: 0.0283 - val_decoder_output_1_loss: 0.0289 - val_decoder_output_2_loss: 0.2340 - val_decoder_output_3_loss: 0.1635 - val_decoder_output_4_loss: 0.0017 - val_decoder_output_5_loss: 0.0563 - val_decoder_output_6_loss: 0.1828 - val_decoder_output_7_loss: 0.0016 - val_decoder_output_8_loss: 0.1415 - val_decoder_output_9_loss: 0.2014 - val_decoder_output_accuracy: 0.9890 - val_decoder_output_1_accuracy: 0.9860 - val_decoder_output_2_accuracy: 0.8560 - val_decoder_output_3_accuracy: 0.9620 - val_decoder_output_4_accuracy: 1.0000 - val_decoder_output_5_accuracy: 0.9790 - val_decoder_output_6_accuracy: 0.9420 - val_decoder_output_7_accuracy: 1.0000 - val_decoder_output_8_accuracy: 0.9480 - val_decoder_output_9_accuracy: 0.9280\n",
      "Epoch 117/120\n",
      "5/5 [==============================] - 1s 172ms/step - loss: 0.8988 - decoder_output_loss: 0.0287 - decoder_output_1_loss: 0.0285 - decoder_output_2_loss: 0.2102 - decoder_output_3_loss: 0.1577 - decoder_output_4_loss: 0.0018 - decoder_output_5_loss: 0.0288 - decoder_output_6_loss: 0.1426 - decoder_output_7_loss: 0.0019 - decoder_output_8_loss: 0.1363 - decoder_output_9_loss: 0.1623 - decoder_output_accuracy: 0.9861 - decoder_output_1_accuracy: 0.9862 - decoder_output_2_accuracy: 0.8907 - decoder_output_3_accuracy: 0.9641 - decoder_output_4_accuracy: 1.0000 - decoder_output_5_accuracy: 0.9900 - decoder_output_6_accuracy: 0.9573 - decoder_output_7_accuracy: 1.0000 - decoder_output_8_accuracy: 0.9496 - decoder_output_9_accuracy: 0.9432 - val_loss: 1.0542 - val_decoder_output_loss: 0.0282 - val_decoder_output_1_loss: 0.0291 - val_decoder_output_2_loss: 0.2328 - val_decoder_output_3_loss: 0.1671 - val_decoder_output_4_loss: 0.0020 - val_decoder_output_5_loss: 0.0431 - val_decoder_output_6_loss: 0.1723 - val_decoder_output_7_loss: 0.0019 - val_decoder_output_8_loss: 0.1616 - val_decoder_output_9_loss: 0.2162 - val_decoder_output_accuracy: 0.9890 - val_decoder_output_1_accuracy: 0.9860 - val_decoder_output_2_accuracy: 0.8650 - val_decoder_output_3_accuracy: 0.9530 - val_decoder_output_4_accuracy: 1.0000 - val_decoder_output_5_accuracy: 0.9790 - val_decoder_output_6_accuracy: 0.9450 - val_decoder_output_7_accuracy: 1.0000 - val_decoder_output_8_accuracy: 0.9350 - val_decoder_output_9_accuracy: 0.9170\n",
      "Epoch 118/120\n",
      "5/5 [==============================] - 1s 173ms/step - loss: 0.8909 - decoder_output_loss: 0.0284 - decoder_output_1_loss: 0.0280 - decoder_output_2_loss: 0.2093 - decoder_output_3_loss: 0.1571 - decoder_output_4_loss: 0.0019 - decoder_output_5_loss: 0.0282 - decoder_output_6_loss: 0.1374 - decoder_output_7_loss: 0.0019 - decoder_output_8_loss: 0.1371 - decoder_output_9_loss: 0.1617 - decoder_output_accuracy: 0.9864 - decoder_output_1_accuracy: 0.9867 - decoder_output_2_accuracy: 0.8922 - decoder_output_3_accuracy: 0.9643 - decoder_output_4_accuracy: 1.0000 - decoder_output_5_accuracy: 0.9897 - decoder_output_6_accuracy: 0.9597 - decoder_output_7_accuracy: 1.0000 - decoder_output_8_accuracy: 0.9470 - decoder_output_9_accuracy: 0.9403 - val_loss: 1.0095 - val_decoder_output_loss: 0.0280 - val_decoder_output_1_loss: 0.0289 - val_decoder_output_2_loss: 0.2297 - val_decoder_output_3_loss: 0.1556 - val_decoder_output_4_loss: 0.0017 - val_decoder_output_5_loss: 0.0427 - val_decoder_output_6_loss: 0.1607 - val_decoder_output_7_loss: 0.0016 - val_decoder_output_8_loss: 0.1499 - val_decoder_output_9_loss: 0.2108 - val_decoder_output_accuracy: 0.9890 - val_decoder_output_1_accuracy: 0.9860 - val_decoder_output_2_accuracy: 0.8630 - val_decoder_output_3_accuracy: 0.9620 - val_decoder_output_4_accuracy: 1.0000 - val_decoder_output_5_accuracy: 0.9810 - val_decoder_output_6_accuracy: 0.9500 - val_decoder_output_7_accuracy: 1.0000 - val_decoder_output_8_accuracy: 0.9400 - val_decoder_output_9_accuracy: 0.9160\n",
      "Epoch 119/120\n",
      "5/5 [==============================] - 1s 163ms/step - loss: 0.8660 - decoder_output_loss: 0.0283 - decoder_output_1_loss: 0.0278 - decoder_output_2_loss: 0.2080 - decoder_output_3_loss: 0.1534 - decoder_output_4_loss: 0.0018 - decoder_output_5_loss: 0.0270 - decoder_output_6_loss: 0.1311 - decoder_output_7_loss: 0.0018 - decoder_output_8_loss: 0.1314 - decoder_output_9_loss: 0.1554 - decoder_output_accuracy: 0.9860 - decoder_output_1_accuracy: 0.9861 - decoder_output_2_accuracy: 0.8930 - decoder_output_3_accuracy: 0.9654 - decoder_output_4_accuracy: 1.0000 - decoder_output_5_accuracy: 0.9902 - decoder_output_6_accuracy: 0.9627 - decoder_output_7_accuracy: 1.0000 - decoder_output_8_accuracy: 0.9521 - decoder_output_9_accuracy: 0.9459 - val_loss: 1.0252 - val_decoder_output_loss: 0.0276 - val_decoder_output_1_loss: 0.0278 - val_decoder_output_2_loss: 0.2272 - val_decoder_output_3_loss: 0.1579 - val_decoder_output_4_loss: 0.0017 - val_decoder_output_5_loss: 0.0519 - val_decoder_output_6_loss: 0.1727 - val_decoder_output_7_loss: 0.0016 - val_decoder_output_8_loss: 0.1493 - val_decoder_output_9_loss: 0.2076 - val_decoder_output_accuracy: 0.9890 - val_decoder_output_1_accuracy: 0.9880 - val_decoder_output_2_accuracy: 0.8660 - val_decoder_output_3_accuracy: 0.9650 - val_decoder_output_4_accuracy: 1.0000 - val_decoder_output_5_accuracy: 0.9800 - val_decoder_output_6_accuracy: 0.9520 - val_decoder_output_7_accuracy: 1.0000 - val_decoder_output_8_accuracy: 0.9360 - val_decoder_output_9_accuracy: 0.9230\n",
      "Epoch 120/120\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.8543 - decoder_output_loss: 0.0280 - decoder_output_1_loss: 0.0275 - decoder_output_2_loss: 0.2036 - decoder_output_3_loss: 0.1515 - decoder_output_4_loss: 0.0018 - decoder_output_5_loss: 0.0259 - decoder_output_6_loss: 0.1345 - decoder_output_7_loss: 0.0019 - decoder_output_8_loss: 0.1285 - decoder_output_9_loss: 0.1512 - decoder_output_accuracy: 0.9863 - decoder_output_1_accuracy: 0.9864 - decoder_output_2_accuracy: 0.8952 - decoder_output_3_accuracy: 0.9659 - decoder_output_4_accuracy: 1.0000 - decoder_output_5_accuracy: 0.9908 - decoder_output_6_accuracy: 0.9596 - decoder_output_7_accuracy: 1.0000 - decoder_output_8_accuracy: 0.9542 - decoder_output_9_accuracy: 0.9467 - val_loss: 1.0108 - val_decoder_output_loss: 0.0280 - val_decoder_output_1_loss: 0.0283 - val_decoder_output_2_loss: 0.2263 - val_decoder_output_3_loss: 0.1527 - val_decoder_output_4_loss: 0.0017 - val_decoder_output_5_loss: 0.0425 - val_decoder_output_6_loss: 0.1617 - val_decoder_output_7_loss: 0.0014 - val_decoder_output_8_loss: 0.1490 - val_decoder_output_9_loss: 0.2193 - val_decoder_output_accuracy: 0.9890 - val_decoder_output_1_accuracy: 0.9860 - val_decoder_output_2_accuracy: 0.8610 - val_decoder_output_3_accuracy: 0.9620 - val_decoder_output_4_accuracy: 1.0000 - val_decoder_output_5_accuracy: 0.9820 - val_decoder_output_6_accuracy: 0.9460 - val_decoder_output_7_accuracy: 1.0000 - val_decoder_output_8_accuracy: 0.9430 - val_decoder_output_9_accuracy: 0.9170\n"
     ]
    }
   ],
   "source": [
    "#for model2\n",
    "history2=model2.fit([Xoh, pred0], outputs, epochs=120, batch_size=2048,validation_split=0.1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While training you can see the loss as well as the accuracy on each of the 10 positions of the output. The table below gives you an example of what the accuracies could be if the batch had 2 examples: \n",
    "\n",
    "<img src=\"images/table.png\" style=\"width:700;height:200px;\"> <br>\n",
    "<caption><center>Thus, `dense_2_acc_8: 0.89` means that you are predicting the 7th character of the output correctly 89% of the time in the current batch of data. </center></caption>\n",
    "\n",
    "\n",
    "We have run this model for longer, and saved the weights. Run the next cell to load our weights. (By training a model for several minutes, you should be able to obtain a model of similar accuracy, but loading our model will save you time.) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将模型保存到文件 my_model.h5\n",
    "model.save('models/xrh_model2.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将模型保存到文件 my_model.h5\n",
    "model2.save('models/xrh_model2.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#载入模型 \n",
    "model2.load_weights('models/xrh_model2.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#查看 训练完成的 模型 里面的参数\n",
    "all_configs=model2.get_config()\n",
    "all_configs['input_layers']\n",
    "all_configs['output_layers']\n",
    "all_configs['layers'][11]\n",
    "weights = model2.layers[11].get_weights() # Getting params\n",
    "# model.layers[i].set_weights(weights) # Setting par\n",
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    " \n",
    " \n",
    "a=np.array(\n",
    "[[1,2,3,4,5],\n",
    "[1,2,3,4,5],\n",
    "[1,2,3,4,5]]    \n",
    ")\n",
    "# a=np.array([[2.29982214e-10,1.05035841e-03,1.04089566e-04,9.98845458e-01,\n",
    "#   5.10124494e-08,5.03688757e-10,2.52189380e-10,1.18713073e-09,\n",
    "#   2.30988277e-08,2.21948682e-08,1.48340121e-07]])\n",
    "\n",
    "\n",
    "\n",
    "input = tf.constant(a)\n",
    "k = 3\n",
    "output = tf.nn.top_k(input, k).indices\n",
    "\n",
    "\n",
    "# one_hot=one_hot_tensor(output,11)\n",
    "# one_hot=one_hot[0]\n",
    "# # one_hot[0].shape\n",
    "# one_hot=K.reshape(one_hot,(1,one_hot.shape[0],one_hot.shape[1]))\n",
    "# # one_hot.shape\n",
    "# one_hot_permute=K.permute_dimensions(one_hot,(1,0,2))\n",
    "# one_hot_permute.shape\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    print(sess.run(input))\n",
    "    print(sess.run(output))\n",
    "#     print(sess.run(one_hot))\n",
    "#     print(sess.run(one_hot_permute))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  tf.nn.top_k 输出每一行 的topk 我们希望能输出整个矩阵的 topk\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    " \n",
    "\n",
    "def all_top_k(input,k):\n",
    "\n",
    "    flatten=K.flatten(input)\n",
    "    global_top_k=tf.nn.top_k(flatten, k)\n",
    "    print('global topk values:',K.eval(global_top_k.values))\n",
    "    print('glaobal topk indices:',K.eval(global_top_k.indices))\n",
    "    indices=global_top_k.indices\n",
    "\n",
    "    indices_row= K.cast(tf.floor(indices/input.shape[-1]),dtype='int32') \n",
    "#     K.eval(indices_row)\n",
    "\n",
    "\n",
    "    indices_col=indices%input.shape[-1] # dtype='int32'\n",
    "    # indices_col=tf.mod(indices,a.shape[-1]) #  tensorflow 的数学运算 https://blog.csdn.net/zywvvd/article/details/78593618\n",
    "\n",
    "#     K.eval(indices_col)\n",
    "\n",
    "    indices=K.concatenate( [K.reshape(indices_row,(1,indices_row.shape[0])) , K.reshape(indices_col,(1,indices_col.shape[0]))] , axis=0)\n",
    "    indices=K.transpose(indices)\n",
    "    \n",
    "    return indices\n",
    " \n",
    "    \n",
    "a=np.array(\n",
    "[[1,2,3,4,5],\n",
    "[1,2,2,2,2],\n",
    "[1,3,3,3,6]]    \n",
    ")\n",
    "\n",
    "\n",
    "k = 3\n",
    "# result=K.eval(all_top_k(a,k))\n",
    "# result\n",
    "# result.shape\n",
    "\n",
    "\n",
    "decoder_result=[]\n",
    "# decoder_result=np.zeros((k,Ty))\n",
    "r0=np.array([3, 1, 2])\n",
    "r0=np.reshape(r0,(3,1))\n",
    "\n",
    "# decoder_result[:,0]=b\n",
    "decoder_result.append(r0)\n",
    "decoder_result\n",
    "\n",
    "r=np.array([[0, 1],\n",
    "             [2, 2],\n",
    "             [1, 1]])\n",
    "\n",
    "# r0=decoder_result[0]\n",
    "\n",
    "r_pre=decoder_result[0]\n",
    "print('r_pre:',r_pre)\n",
    "\n",
    "r1=K.cast(K.zeros((k,2)),dtype='int32')\n",
    "\n",
    "#TODO:  build a empty tensor: r1\n",
    "\n",
    "#TODO:  少在 tensor 和 numpy 之间的来回转换 可以提升速度？ 全部用tensor 进行计算 \n",
    "\n",
    "for i in range(k):\n",
    "    a=K.reshape(r_pre[r[i][0]],(1,r_pre.shape[1])) \n",
    "    b=K.reshape(r[i][1],(1,1))\n",
    "    \n",
    "    c=K.concatenate( [a,b]  ,axis=1 )\n",
    "    print(c)\n",
    "#     r1[i,:].assign( K.concatenate( [a,b]  ,axis=0 ) ) #ValueError: Sliced assignment is only supported for variables\n",
    "# TODO: 两个 tensor 之间的切片 赋值   \n",
    "    \n",
    "    \n",
    "decoder_result.append(r1)\n",
    "decoder_result\n",
    "\n",
    "# r_pre=decoder_result[1]\n",
    "# print('r_pre:',r_pre)\n",
    "\n",
    "# r=np.array([[0, 4],\n",
    "#              [0, 2],\n",
    "#              [0, 3]])\n",
    "\n",
    "# r2=np.zeros((k,3))\n",
    "\n",
    "# for i in range(k):\n",
    "#     r2[i,:]=np.concatenate( ( r_pre[r[i][0]],[r[i][1]] ),axis=0 )\n",
    "\n",
    "\n",
    "# decoder_result.append(r2)\n",
    "# decoder_result\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "##--part1--: 使用 numpy 复现 tf.nn.topk  ## \n",
    "\n",
    "# k=3\n",
    "# arr = np.array([1, 98, 2, 99, 100])\n",
    "# idx=arr.argsort()[::-1][0:k]\n",
    "# idx\n",
    "# arr[idx]# 最大的三个元素 (已排序)\n",
    "\n",
    "# idx=np.argpartition(arr, k)[0:k]\n",
    "# arr[idx]#最小的 三个元素\n",
    "\n",
    "# idx = np.argpartition(arr, -k)[-k:]\n",
    "# idx\n",
    "# arr[idx]#最大的 三个元素 (未排序)\n",
    "\n",
    "# a=np.array(\n",
    "# [[1,2,3,4,5],\n",
    "# [1,2,8,2,2],\n",
    "# [9,3,3,3,6]]    \n",
    "# )\n",
    "# np.argpartition(a, -k)\n",
    "# idx = np.argpartition(a, -k)[ :,-k:]\n",
    "# idx\n",
    "\n",
    "\n",
    "def topk_array(matrix, k, axis=1):\n",
    "    \"\"\"\n",
    "    perform topK based on np.argsort\n",
    "    :param matrix: to be sorted\n",
    "    :param K: select and sort the top K items\n",
    "    :param axis: dimension to be sorted.\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    full_sort = np.argsort(matrix, axis=axis)\n",
    "    return full_sort[ :,-k:]\n",
    "\n",
    "def partition_topk_array(matrix, K, axis=1):\n",
    "    \"\"\"\n",
    "    perform topK based on np.argpartition\n",
    "    :param matrix: to be sorted\n",
    "    :param K: select and sort the top K items\n",
    "    :param axis: 0 or 1. dimension to be sorted.\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    a_part = np.argpartition(matrix, -K, axis=axis)\n",
    "    if axis == 0:\n",
    "        row_index = np.arange(matrix.shape[1 - axis])\n",
    "        a_sec_argsort_K = np.argsort(matrix[a_part[-K:, :], row_index], axis=axis)\n",
    "        return a_part[-K:, :][a_sec_argsort_K, row_index]\n",
    "    else:\n",
    "        column_index = np.arange(matrix.shape[1 - axis])[:, None]\n",
    "#         print('column_index ',column_index)\n",
    "#         print('matrix[column_index, a_part[:, -K:]] ',matrix[column_index, a_part[:, -K:]]) #选取矩阵中的一组元素\n",
    "        a_sec_argsort_K = np.argsort(matrix[column_index, a_part[:, -K:]], axis=axis)\n",
    "#         print('a_sec_argsort_K ',a_sec_argsort_K)\n",
    "        return a_part[:, -K:][column_index, a_sec_argsort_K] # 乾坤大挪移，变换矩阵中的元素位置\n",
    "\n",
    "    \n",
    "\n",
    "# arr = np.array([[1, 98, 2, 99, 100]])\n",
    "\n",
    "a=np.array(\n",
    "[[1,2,3,4,5],\n",
    "[1,2,8,2,2],\n",
    "[9,3,3,3,6]]    \n",
    ")\n",
    "k=3\n",
    "# partition_topk_array(a, k, axis=1) \n",
    "\n",
    "# partition_topk_array(arr, k, axis=1)# 最大的 三个元素 (已排序)\n",
    "\n",
    "\n",
    "## --ref: \n",
    "# https://stackoverflow.com/questions/6910641/how-do-i-get-indices-of-n-maximum-values-in-a-numpy-array\n",
    "# https://stackoverflow.com/questions/41484104/how-numpy-partition-work\n",
    "# https://blog.csdn.net/SoftPoeter/article/details/86629329\n",
    "##--part1-- end --##\n",
    "\n",
    "##--part2--: arg_topK 输出 二维矩阵的 每一行 的topk，我们希望能输出整个矩阵的 topk\n",
    "\n",
    "def whole_topk_array(input,k):\n",
    "    \"\"\"\n",
    "    输出 input 中所有元素中的 k 个最大的元素的下标，但是这k个元素并不会按照大小排序\n",
    "    \"\"\"\n",
    "\n",
    "    flatten=input.flatten()\n",
    "#     print(flatten)\n",
    "    \n",
    "    global_top_k=np.argpartition(flatten, -k)[-k:]\n",
    "    \n",
    "    indices=global_top_k\n",
    "\n",
    "    indices_row= np.floor(indices/input.shape[-1])\n",
    "    \n",
    "\n",
    "    indices_col=indices%input.shape[-1] # dtype='int32'\n",
    "\n",
    "\n",
    "    indices=np.concatenate( [np.reshape(indices_row,(1,indices_row.shape[0])) , np.reshape(indices_col,(1,indices_col.shape[0]))] , axis=0)\n",
    "    indices=np.transpose(indices)\n",
    "    \n",
    "    return indices.astype(np.int32) # numpy 数据类型转换 ；查看数据类型： arr.dtype\n",
    "\n",
    "\n",
    "# whole_topk_array(a,k)\n",
    "\n",
    "##--part2-- end --##\n",
    "\n",
    "##--part3--: 使用 numpy 复现   tf.one_hot()\n",
    "\n",
    "# a.reshape(-1,3) # 固定3列 (-1)=多少行不知道，numpy自己算去吧\n",
    "# a.reshape(-1) #  flatten a \n",
    "\n",
    "def one_hot_array(targets, nb_classes):\n",
    "    res = np.eye(nb_classes)[np.array(targets).reshape(-1)]\n",
    "#     print(res.shape)\n",
    "    return res.reshape(list(targets.shape)+[nb_classes])\n",
    "\n",
    "# a.shape\n",
    "# b=one_hot_array(a,11)\n",
    "# b.shape\n",
    "\n",
    "##--ref: \n",
    "# https://stackoverflow.com/questions/38592324/one-hot-encoding-using-numpy\n",
    "##--part3-- end --##\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 30, 37)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'one_step_attention' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n",
      "\u001b[1;32m<timed exec>\u001b[0m in \u001b[0;36mbeamsearch_v1\u001b[1;34m(source_oh, Tx, Ty, n_a, n_s, human_vocab_size, machine_vocab_size, k)\u001b[0m\n",
      "\u001b[1;32m<timed exec>\u001b[0m in \u001b[0;36mmodel2_onestep_decode\u001b[1;34m(Tx, Ty, timestep, n_a, n_s, human_vocab_size, machine_vocab_size)\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'one_step_attention' is not defined"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# By XRH in 2019.9.10\n",
    "# beamsearch 的两种实现方式，并比较时间开销  \n",
    "\n",
    "# by : https://stackoverflow.com/questions/48374905/how-can-i-use-argsort-in-keras\n",
    "def top_k(input, k):\n",
    "  # Can also use `.values` to return a sorted tensor\n",
    "  return tf.nn.top_k(input, k=k, sorted=True)\n",
    "\n",
    "\n",
    "def all_top_k(input,k):\n",
    "    \"\"\"\n",
    "     tf.nn.top_k 输出每一行 的topk 我们希望能输出整个矩阵的 topk\n",
    "    \"\"\"\n",
    "\n",
    "    flatten=K.flatten(input)\n",
    "    global_top_k=tf.nn.top_k(flatten, k)\n",
    "#     print('global topk values:',K.eval(global_top_k.values))\n",
    "#     print('glaobal topk indices:',K.eval(global_top_k.indices))\n",
    "    indices=global_top_k.indices\n",
    "\n",
    "    indices_row= K.cast(tf.floor(indices/input.shape[-1]),dtype='int32') \n",
    "#     K.eval(indices_row)\n",
    "\n",
    "\n",
    "    indices_col=indices%input.shape[-1] # dtype='int32'\n",
    "    # indices_col=tf.mod(indices,a.shape[-1]) #  tensorflow 的数学运算 https://blog.csdn.net/zywvvd/article/details/78593618\n",
    "\n",
    "#     K.eval(indices_col)\n",
    "\n",
    "    indices=K.concatenate( [K.reshape(indices_row,(1,indices_row.shape[0])) , K.reshape(indices_col,(1,indices_col.shape[0]))] , axis=0)\n",
    "    indices=K.transpose(indices)\n",
    "    \n",
    "    return indices\n",
    "\n",
    "\n",
    "def model2_onestep_decode(Tx, Ty,timestep, n_a, n_s, human_vocab_size, machine_vocab_size):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    Tx -- length of the input sequence\n",
    "    timestep -- timestep of decoder \n",
    "    Ty -- length of the output sequence\n",
    "    n_a -- hidden state size of the Bi-LSTM\n",
    "    n_s -- hidden state size of the post-attention LSTM\n",
    "    human_vocab_size -- size of the python dictionary \"human_vocab\"\n",
    "    machine_vocab_size -- size of the python dictionary \"machine_vocab\"\n",
    "\n",
    "    Returns:\n",
    "    model -- Keras model instance\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    X = Input(shape=(Tx, human_vocab_size)) # shape: (m,Tx,human_vocab_size)\n",
    "    s0 = Input(shape=(n_s,), name='s')  # shape of s:  (m, 64)\n",
    "    c0 = Input(shape=(n_s,), name='c')  # shape of c:  (m, 64)\n",
    "    \n",
    "    pred0=Input(shape=(1,len(machine_vocab)), name='pred')  # shape of pred (m ,1, 11)\n",
    "    \n",
    "    s=s0 # unmutable object: a new tensor is generated \n",
    "    c=c0\n",
    "    pred=pred0\n",
    "    \n",
    "    \n",
    "#     print('pred: after Input',pred)\n",
    "\n",
    "    \n",
    "    a, forward_h, forward_c, backward_h, backward_c= pre_activation_LSTM_cell(inputs=X) #  shape of a : (m,Tx, 2*n_a) \n",
    "    #TODO：这一步的推理是多余的，可以把 encoder 和 decoder 彻底解耦\n",
    "\n",
    "    \n",
    "    if timestep==0: # decoder 的第一个时间步\n",
    "\n",
    "            s = concatenate_s([forward_h, backward_h]) # shape of s:  (m, 64)\n",
    "            c = concatenate_c([forward_c, backward_c])\n",
    "            \n",
    "            context = one_step_attention(a, s) # shape of context :  (m, 1, 128)\n",
    "       \n",
    "            context=concatenate_context([context,pred])# shape of context: (m,128+11=139)\n",
    "\n",
    "            s, _, c = post_activation_LSTM_cell(inputs=context,initial_state=[s, c])\n",
    "\n",
    "            out = output_layer(s)   \n",
    "    else:\n",
    "            \n",
    "            context = one_step_attention(a, s) # shape of context :  (m, 1, 128)\n",
    "       \n",
    "            context=concatenate_context([context,pred])# shape of context: (m,128+11=139)\n",
    "\n",
    "            s, _, c = post_activation_LSTM_cell(inputs=context,initial_state=[s, c])\n",
    "\n",
    "            out = output_layer(s)    \n",
    "    \n",
    "    outputs=[s,c,out] # 输出 s c out 作为下一个时间步使用\n",
    "        \n",
    "          \n",
    "    model =  Model(inputs=[X, s0, c0 ,pred0], outputs=outputs) \n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "def beamsearch(Tx, Ty, n_a, n_s, human_vocab_size, machine_vocab_size,k=3):\n",
    "    \"\"\"\n",
    "    @deprecated: too slow\n",
    "    cost time: 1min30s\n",
    "    \"\"\"\n",
    "    \n",
    "    s0 = np.zeros((k, n_s))\n",
    "    c0 = np.zeros((k, n_s))\n",
    "    pred0=np.zeros((k,1,len(machine_vocab)))\n",
    "    \n",
    "    s=s0\n",
    "    c=c0\n",
    "    pred=pred0\n",
    "    \n",
    "    decoder_result=[]\n",
    "    \n",
    "    for timestep in range(Ty):\n",
    "        \n",
    "        onestep_decode = model2_onestep_decode(Tx, Ty,timestep, n_a, n_s, len(human_vocab), len(machine_vocab))\n",
    "        s,c,out=onestep_decode.predict([source_oh, s, c,pred])\n",
    "        \n",
    "        \n",
    "        if timestep==0:\n",
    "            print('timestep :', timestep)\n",
    "\n",
    "#             print ('out:',out) # shape:(3, 11)\n",
    "\n",
    "            out_top_K=top_k(out, k).indices  #  shape:(3,3) \n",
    "           \n",
    "            top_K_indices=K.eval(out_top_K) # cost much time\n",
    "            \n",
    "            r0=top_K_indices[0]\n",
    "            \n",
    "            r0=np.reshape(r0,(k,1))\n",
    "            decoder_result=r0\n",
    "            \n",
    "            one_hot=one_hot_tensor(out_top_K,machine_vocab_size )\n",
    "#             print (K.eval(one_hot)) # tensor shape:(3,3,11)  \n",
    "            \n",
    "            one_hot=one_hot[0]\n",
    "#             print(K.eval(one_hot)) #  shape:(3, 11) for debug, get the value of tensor\n",
    "            one_hot=K.reshape(one_hot,(1,one_hot.shape[0],one_hot.shape[1])) #shape:(1,3, 11)\n",
    "            one_hot_permute=K.permute_dimensions(one_hot,(1,0,2)) #shape: (3,1,11)\n",
    "            \n",
    "            pred=K.eval(one_hot_permute) # tensor -> numpy array  \n",
    "#             print('pred shape:',pred.shape)\n",
    "        \n",
    "        else:\n",
    "            print('timestep :', timestep)\n",
    "#             print ('out:',out)\n",
    "            \n",
    "            out_top_K=all_top_k(out,k)\n",
    "            \n",
    "            r=K.eval(out_top_K)\n",
    "#             print('r:',r)\n",
    "            \n",
    "            r_pre=decoder_result\n",
    "    \n",
    "#             print('r_pre:',r_pre)\n",
    "\n",
    "            rt=np.zeros((k,timestep+1))\n",
    "\n",
    "            for i in range(k):\n",
    "\n",
    "                rt[i,:]=np.concatenate( ( r_pre[r[i][0]],[r[i][1]] ) , axis=0 )\n",
    "\n",
    "            decoder_result=rt\n",
    "            \n",
    "            \n",
    "            one_hot=one_hot_tensor(r[:,1],machine_vocab_size )\n",
    "#             print (K.eval(one_hot)) # shape:(3, 11)\n",
    "            \n",
    "            one_hot=K.reshape(one_hot,(1,one_hot.shape[0],one_hot.shape[1])) #shape:(1,3, 11)\n",
    "#             print('one_hot shape:',one_hot.shape)\n",
    "            one_hot_permute=K.permute_dimensions(one_hot,(1,0,2)) #shape: (3,1,11)\n",
    "            \n",
    "            pred=K.eval(one_hot_permute) # tensor -> numpy array  \n",
    "#             print('pred shape:',pred.shape)\n",
    "            \n",
    "            \n",
    "        print('decoder_result',decoder_result) \n",
    "    \n",
    "    return   decoder_result  \n",
    "        \n",
    "\n",
    "def beamsearch_v1(source_oh,Tx, Ty, n_a, n_s, human_vocab_size, machine_vocab_size,k):\n",
    "    \"\"\"\n",
    "    np.array -> tensor 很自然 但是 tensor -> np.array 的方法： K.eval(tensor) 非常耗费时间；\n",
    "    beamsearch_v1 尝试尽量多用 numpy 库的函数，以减少 tensor 和 np.array 的转换的次数。\n",
    "    cost time: 6.13 s\n",
    "    \"\"\"\n",
    "    \n",
    "    s0 = np.zeros((k, n_s))\n",
    "    c0 = np.zeros((k, n_s))\n",
    "    pred0=np.zeros((k,1,len(machine_vocab)))\n",
    "    \n",
    "    s=s0\n",
    "    c=c0\n",
    "    pred=pred0\n",
    "    \n",
    "    decoder_result=[]\n",
    "    \n",
    "    for timestep in range(Ty):\n",
    "        \n",
    "        onestep_decode = model2_onestep_decode(Tx, Ty,timestep, n_a, n_s, len(human_vocab), len(machine_vocab))\n",
    "        s,c,out=onestep_decode.predict([source_oh, s, c,pred]) \n",
    "        #source_oh shape：(3, 30, 37) \n",
    "        #每次都对 3个相同的样本（k=3）进行 推理，但是每一个 样本对应的 pred 不同 ；\n",
    "        #这实现了beamsearch 中，每一个时间步都会根据上一步的 onestep_decoder 输出结果中 选择最好的k个, 输入 onestep_decoder \n",
    "        \n",
    "        if timestep==0:\n",
    "            print('timestep :', timestep)\n",
    "\n",
    "#             print ('out:',out) # shape:(3, 11)\n",
    "\n",
    "            out_top_K=partition_topk_array(out, k)  #  shape:(3,3) \n",
    "            print(out_top_K)\n",
    "           \n",
    "            top_K_indices=out_top_K \n",
    "            \n",
    "            r0=top_K_indices[0]\n",
    "            \n",
    "            r0=np.reshape(r0,(k,1))\n",
    "            decoder_result=r0\n",
    "            \n",
    "            one_hot=one_hot_array(out_top_K,machine_vocab_size )#  shape:(3,3,11)\n",
    "            \n",
    "            one_hot=one_hot[0]#  shape:(3, 11) \n",
    "            one_hot=np.reshape(one_hot,(1,one_hot.shape[0],one_hot.shape[1])) #shape:(1,3, 11)\n",
    "            \n",
    "            one_hot_permute=one_hot.transpose((1,0,2)) #shape: (3,1,11)\n",
    "            \n",
    "            pred=one_hot_permute   \n",
    "        \n",
    "        else:\n",
    "            print('timestep :', timestep)\n",
    "#             print ('out:',out)\n",
    "            \n",
    "            out_top_K=whole_topk_array(out,k)\n",
    "            \n",
    "            r=out_top_K\n",
    "            \n",
    "            r_pre=decoder_result\n",
    "    \n",
    "\n",
    "            rt=np.zeros((k,timestep+1))\n",
    "\n",
    "            for i in range(k):\n",
    "                \n",
    "\n",
    "                rt[i,:]=np.concatenate( ( r_pre[r[i][0]],[r[i][1]] ) , axis=0 )\n",
    "\n",
    "            decoder_result=rt\n",
    "            \n",
    "            \n",
    "            one_hot=one_hot_array(r[:,1],machine_vocab_size ) # shape:(3, 11)\n",
    "            \n",
    "            one_hot=np.reshape(one_hot,(1,one_hot.shape[0],one_hot.shape[1])) #shape:(1,3, 11)\n",
    "            one_hot_permute=one_hot.transpose((1,0,2)) #shape: (3,1,11)\n",
    "            \n",
    "            pred=one_hot_permute\n",
    "            \n",
    "            \n",
    "        print('decoder_result',decoder_result) \n",
    "    \n",
    "    return   decoder_result  \n",
    "        \n",
    "    \n",
    "example = \"3rd of March 2002\"\n",
    "source = np.array(string_to_int(example, Tx, human_vocab))\n",
    "source_oh = np.array(list(map(lambda x: to_categorical(x, num_classes=len(human_vocab)), source)))\n",
    "\n",
    "k=3\n",
    "source_oh=source_oh.reshape(1,source_oh.shape[0],source_oh.shape[1])   \n",
    "# print(source_oh.shape) \n",
    "source_oh=np.repeat(source_oh, k, axis=0) \n",
    "print(source_oh.shape) #(3, 30, 37) m=3 一个样本 复制为三个, 输入模型进行推理\n",
    "\n",
    "  \n",
    "decoder_result=beamsearch_v1(source_oh,Tx, Ty, n_a, n_s, len(human_vocab), len(machine_vocab),k) \n",
    "\n",
    "for prediction in decoder_result:\n",
    "    output = int_to_string(prediction, inv_machine_vocab)\n",
    "    print(\"source:\", example)\n",
    "    print(\"output:\", ''.join(output))\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "#   By XRH in 2019.9.17\n",
    "#  对 encoder 和 decoder 进行解耦\n",
    "\n",
    "def model2_onestep_decoder_v2(Tx, Ty,timestep, n_a, n_s, human_vocab_size, machine_vocab_size):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    Tx -- length of the input sequence\n",
    "    timestep -- timestep of decoder \n",
    "    Ty -- length of the output sequence\n",
    "    n_a -- hidden state size of the Bi-LSTM\n",
    "    n_s -- hidden state size of the post-attention LSTM\n",
    "    human_vocab_size -- size of the python dictionary \"human_vocab\"\n",
    "    machine_vocab_size -- size of the python dictionary \"machine_vocab\"\n",
    "\n",
    "    Returns:\n",
    "    model -- Keras model instance\n",
    "    \"\"\"\n",
    "    \n",
    "    context0=Input(shape=(1,n_s), name='context')\n",
    "    \n",
    "    s0 = Input(shape=(n_s,), name='s')  # shape of s:  (m, 64)\n",
    "    c0 = Input(shape=(n_s,), name='c')  # shape of c:  (m, 64)\n",
    "    \n",
    "    pred0=Input(shape=(1,len(machine_vocab)), name='pred')  # shape of pred (m ,1, 11)\n",
    "    \n",
    "    context=context0\n",
    "    s=s0 # unmutable object: a new tensor is generated \n",
    "    c=c0\n",
    "    pred=pred0\n",
    "     \n",
    "#     print('pred: after Input',pred)\n",
    "\n",
    "    context=concatenate_context([context,pred])# shape of context: (m,128+11=139)\n",
    "\n",
    "    s, _, c = post_activation_LSTM_cell(inputs=context,initial_state=[s, c])\n",
    "\n",
    "    out = output_layer(s)    \n",
    "    \n",
    "    outputs=[s,c,out] # 输出 s c out 作为下一个时间步使用\n",
    "        \n",
    "          \n",
    "    model =  Model(inputs=[context0 ,s0, c0 ,pred0], outputs=outputs) \n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "def model2_encoder(Tx, human_vocab_size):\n",
    "    \n",
    "    X = Input(shape=(Tx, human_vocab_size)) # shape: (m,Tx,human_vocab_size)\n",
    "    \n",
    "    a, forward_h, forward_c, backward_h, backward_c= pre_activation_LSTM_cell(inputs=X) #  shape of a : (m,Tx, 2*n_a) \n",
    " \n",
    "    s = concatenate_s([forward_h, backward_h]) # shape of s:  (m, 64+64)\n",
    "    c = concatenate_c([forward_c, backward_c])\n",
    "    \n",
    "    context = one_step_attention_M2(a, s) # shape of context :  (m, 1, 128)\n",
    "    \n",
    "    outputs=[context,s,c]\n",
    "    \n",
    "    model =  Model(inputs=[X], outputs=outputs) \n",
    "    \n",
    "    return model\n",
    "    \n",
    "\n",
    "def beamsearch_v2(source_oh,Tx, Ty, n_a, n_s, human_vocab_size, machine_vocab_size,k=3):\n",
    "    \"\"\"\n",
    "    np.array -> tensor 很自然 但是 tensor -> np.array 的方式： K.eval(tensor) 非常耗费时间；\n",
    "    beamsearch_v1 尝试尽量多用 numpy 的函数，以减少 tensor 和 np 的转换的次数。\n",
    "    cost time: 6.13 s\n",
    "    \"\"\"\n",
    "    \n",
    "    s0 = np.zeros((k, n_s))\n",
    "    c0 = np.zeros((k, n_s))\n",
    "    pred0=np.zeros((k,1,len(machine_vocab)))\n",
    "    \n",
    "    s=s0\n",
    "    c=c0\n",
    "    pred=pred0\n",
    "    \n",
    "    decoder_result=[]\n",
    "\n",
    "#--encoder 和 decoder 的解耦\n",
    "#bearm serach encoder：\n",
    "# M1: \n",
    "#     encoder_output = Model(inputs=model2.input,  \n",
    "#         outputs=[ model2.get_layer('decoder_output').get_output_at(1) ]) #TODO\n",
    "#     a,s,c= encoder_output.predict([source_oh, s0, c0,pred0])\n",
    "\n",
    "# M2:\n",
    "\n",
    "    encoder=model2_encoder(Tx, human_vocab_size)\n",
    "    context,s,c=encoder.predict([source_oh])\n",
    "\n",
    "#bearm serach decoder：\n",
    "\n",
    "    for timestep in range(Ty):\n",
    "        \n",
    "        onestep_decoder = model2_onestep_decoder_v2(Tx, Ty,timestep, n_a, n_s, len(human_vocab), len(machine_vocab))\n",
    "        s,c,out=onestep_decoder.predict([context, s, c,pred]) \n",
    "        #source_oh shape：(3, 30, 37) \n",
    "        #每次都对 3个相同的样本（k=3）进行 推理，但是每一个 样本对应的 pred 不同 ；\n",
    "        #这实现了beamsearch 中，每一个时间步都会根据上一步的 onestep_decoder 输出结果中 选择最好的k个, 输入 onestep_decoder \n",
    "        \n",
    "        if timestep==0:\n",
    "            print('timestep :', timestep)\n",
    "\n",
    "#             print ('out:',out) # shape:(3, 11) softmax 层输出的为 11 unit 的概率；输入的样本数量为3\n",
    "\n",
    "            out_top_K=partition_topk_array(out, k)  #  shape:(3,3) 从每个样本 的 11unit 中选出最大的k个 \n",
    "            print(out_top_K)\n",
    "           \n",
    "            top_K_indices=out_top_K \n",
    "            \n",
    "            r0=top_K_indices[0] #  shape:(1,3) 因为3个输入样本是一样的，取其中一个即可 \n",
    "            \n",
    "            r0=np.reshape(r0,(k,1)) # shape:(3,1)\n",
    "            decoder_result=r0\n",
    "            \n",
    "            one_hot=one_hot_array(out_top_K,machine_vocab_size )#  shape:(3,3,11)\n",
    "            # 把 out_top_K shape:(3,3) 最后一个维度 变为 one-hot 向量\n",
    "            \n",
    "            one_hot=one_hot[0]#  shape:(3, 11) ；只要取一个即可\n",
    "            one_hot=np.reshape(one_hot,(1,one_hot.shape[0],one_hot.shape[1])) #shape:(1,3, 11)\n",
    "            \n",
    "            one_hot_permute=one_hot.transpose((1,0,2)) #shape: (3,1,11) ；\n",
    "            #交换 第0维 和 第1维，相当于3个不同的 pred 同时输入下一个时间步的 onestep_decoder\n",
    "            pred=one_hot_permute   \n",
    "        \n",
    "        else:\n",
    "            print('timestep :', timestep)\n",
    "#             print ('out:',out) # shape:(3, 11)\n",
    "            \n",
    "            out_top_K=whole_topk_array(out,k)  #  shape:(3, 2) 找出 3*11 个元素中的k个最大的 元素的标号 \n",
    "            \n",
    "            r=out_top_K \n",
    "            print('r:',r) \n",
    "#             [[1 1]    \n",
    "#              [0 1]\n",
    "#              [2 1]] 元素的标号为 [2,1] ，代表 第3个输入的pred 所输出的11个uints中的第1个unit\n",
    "            \n",
    "            r_pre=decoder_result # shape:(k,timestep) 上一步 解码的结果 即是 这一步的输入 \n",
    "            # [[2]\n",
    "            #  [1]\n",
    "            #  [3]]\n",
    "    \n",
    "            rt=np.zeros((k,timestep+1)) #这一步 会在上一步 已有的解码序列的基础上 增加1个 解码位\n",
    "\n",
    "            for i in range(k):\n",
    "                \n",
    "                rt[i,:]=np.concatenate( ( r_pre[r[i][0]],[r[i][1]] ) , axis=0 )\n",
    "                # r[2][0]=2 说明是第二个输入的pred，前一步的解码情况为： r_pre[r[2][0]]=[2] ，\n",
    "                #再连接上这一步的解码位  r[2][1]=1 得到 解码序列：[2,1]\n",
    "                # 一共k 个解码序列 组成 rt\n",
    "                \n",
    "            decoder_result=rt.astype(np.int32) \n",
    "#               rt:\n",
    "#              [[1. 1.] \n",
    "#              [2. 1.]\n",
    "#              [3. 1.]]\n",
    "\n",
    "#              decoder_result:\n",
    "#              [[1 1] \n",
    "#              [2 1]\n",
    "#              [3 1]]\n",
    "            \n",
    "            \n",
    "            one_hot=one_hot_array(decoder_result[:,-1],machine_vocab_size ) # shape:(3, 11)\n",
    "#             print(one_hot.shape)\n",
    "            \n",
    "            one_hot=np.reshape(one_hot,(1,one_hot.shape[0],one_hot.shape[1])) #shape:(1,3, 11)\n",
    "            one_hot_permute=one_hot.transpose((1,0,2)) #shape: (3,1,11)\n",
    "            \n",
    "            pred=one_hot_permute\n",
    "            \n",
    "        print('decoder_result',decoder_result) \n",
    "    \n",
    "    return   decoder_result  \n",
    "        \n",
    "    \n",
    "example = \"3rd of March 2002\"\n",
    "source = np.array(string_to_int(example, Tx, human_vocab))\n",
    "source_oh = np.array(list(map(lambda x: to_categorical(x, num_classes=len(human_vocab)), source)))\n",
    "\n",
    "k=3\n",
    "source_oh=source_oh.reshape(1,source_oh.shape[0],source_oh.shape[1])   \n",
    "# print(source_oh.shape) \n",
    "source_oh=np.repeat(source_oh, k, axis=0) \n",
    "# print(source_oh.shape) #(3, 30, 37) m=3 一个样本 复制为三个输入模型进行推理\n",
    "\n",
    "  \n",
    "decoder_result=beamsearch_v2(source_oh,Tx, Ty, n_a, n_s, len(human_vocab), len(machine_vocab),k) \n",
    "\n",
    "for prediction in decoder_result:\n",
    "    output = int_to_string(prediction, inv_machine_vocab)\n",
    "    print(\"source:\", example)\n",
    "    print(\"output:\", ''.join(output))\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Draw pictures to show the training process and the accuracy in the verification set and training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#by model \n",
    "\n",
    "\n",
    "#---对 decoder 的10个时间步的 准确率求均值 --#\n",
    "#--start--#\n",
    "# Ty = 10\n",
    "Epoch_num=(history.history['dense_3_acc'])\n",
    "\n",
    "acc0=np.array(history.history['dense_3_acc'])\n",
    "acc0=acc0.reshape(40,1)\n",
    "\n",
    "acc=acc0\n",
    "\n",
    "for i in range(Ty):\n",
    "    if i != 0:\n",
    "        \n",
    "        acc_t=np.array(history.history['dense_3_acc_'+str(i)])\n",
    "        acc_t=acc_t.reshape(40,1)\n",
    "        acc=np.concatenate([acc,acc_t],axis=1)\n",
    "  \n",
    "\n",
    "acc=np.mean(acc,axis=1)\n",
    "print('acc.shape:',acc.shape)   \n",
    "\n",
    "val_acc0=np.array(history.history['val_dense_3_acc'])\n",
    "val_acc0=val_acc0.reshape(40,1)\n",
    "\n",
    "val_acc=val_acc0\n",
    "\n",
    "for i in range(Ty):\n",
    "    if i != 0:\n",
    "        \n",
    "        val_acc_t=np.array(history.history['val_dense_3_acc_'+str(i)])\n",
    "        val_acc_t=val_acc_t.reshape(40,1)\n",
    "        val_acc=np.concatenate([val_acc,val_acc_t],axis=1)  \n",
    "\n",
    "val_acc=np.mean(val_acc,axis=1)\n",
    "print(val_acc.shape)  \n",
    "#--- end --#\n",
    "\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs = range(1, len(acc) + 1)\n",
    "\n",
    "plt.figure( figsize=(8,4), dpi=100 )\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#by model2\n",
    "#---对 decoder 的10个时间步的 准确率求均值 --#\n",
    "#--start--#\n",
    "# Ty = 10\n",
    "Epoch_num=len(history2.history['decoder_output_acc'])\n",
    "\n",
    "acc0=np.array(history2.history['decoder_output_acc'])\n",
    "acc0=acc0.reshape(Epoch_num,1)\n",
    "\n",
    "acc=acc0\n",
    "\n",
    "for i in range(Ty):\n",
    "    if i != 0:\n",
    "        \n",
    "        acc_t=np.array(history2.history['decoder_output_acc_'+str(i)])\n",
    "        acc_t=acc_t.reshape(Epoch_num,1)\n",
    "        acc=np.concatenate([acc,acc_t],axis=1)\n",
    "  \n",
    "\n",
    "acc=np.mean(acc,axis=1)\n",
    "print('acc.shape:',acc.shape)   \n",
    "\n",
    "val_acc0=np.array(history2.history['val_decoder_output_acc'])\n",
    "val_acc0=val_acc0.reshape(Epoch_num,1)\n",
    "\n",
    "val_acc=val_acc0\n",
    "\n",
    "for i in range(Ty):\n",
    "    if i != 0:\n",
    "        \n",
    "        val_acc_t=np.array(history2.history['val_decoder_output_acc_'+str(i)])\n",
    "        val_acc_t=val_acc_t.reshape(Epoch_num,1)\n",
    "        val_acc=np.concatenate([val_acc,val_acc_t],axis=1)  \n",
    "\n",
    "val_acc=np.mean(val_acc,axis=1)\n",
    "print(val_acc.shape)  \n",
    "#--- end --#\n",
    "\n",
    "loss = history2.history['loss']\n",
    "val_loss = history2.history['val_loss']\n",
    "\n",
    "epochs = range(1, len(acc) + 1)\n",
    "\n",
    "plt.figure( figsize=(25,15), dpi=200 )\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can now see the results on new examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "example = \"3rd of March 2002\"\n",
    "source = np.array(string_to_int(example, Tx, human_vocab))\n",
    "\n",
    "\n",
    "# model input: [Xoh, s0, c0]\n",
    "\n",
    "print(\"Xoh.shape:\", Xoh.shape)\n",
    "print(\"source.shape:\", source.shape)\n",
    "\n",
    "source_oh = np.array(list(map(lambda x: to_categorical(x, num_classes=len(human_vocab)), source)))\n",
    "print(\"source_oh shape:\", source_oh.shape)\n",
    "source_oh=source_oh.reshape(1,source_oh.shape[0],source_oh.shape[1])\n",
    "print(\"source_oh shape after reshape:\", source_oh.shape)\n",
    "\n",
    "prediction = model2.predict([source_oh, s0, c0,pred00])\n",
    "# prediction = model.predict([source_oh, s0, c0])\n",
    "\n",
    "prediction=np.array(prediction)\n",
    "print('prediction.shape:',prediction.shape)\n",
    "\n",
    "prediction=prediction.swapaxes(0,1)\n",
    "\n",
    "\n",
    "print(\"Yoh.shape:\", Yoh.shape)\n",
    "print(\"prediction.shape:\", prediction.shape)\n",
    "prediction = np.argmax(prediction[0], axis = -1)\n",
    "prediction\n",
    "output = int_to_string(prediction, inv_machine_vocab)\n",
    "print(\"source:\", example)\n",
    "print(\"output:\", ''.join(output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#输出 模型 中间层的计算结果\n",
    "# M1  \n",
    "# get_layer12_output_timestep_9 = K.function([model2.layers[0].input],\n",
    "#                                   [model2.layers[12].get_output_at(9)])\n",
    "# layer_output = get_layer12_output_timestep_9([source_oh, s0, c0,pred0])[0] #TODO exisits error \n",
    "\n",
    "# M2\n",
    "\n",
    "\n",
    "example = \"3rd of March 2002\"\n",
    "source = np.array(string_to_int(example, Tx, human_vocab))\n",
    "\n",
    "print(\"Xoh.shape:\", Xoh.shape)\n",
    "print(\"source.shape:\", source.shape)\n",
    "\n",
    "source_oh = np.array(list(map(lambda x: to_categorical(x, num_classes=len(human_vocab)), source)))\n",
    "print(\"source_oh shape:\", source_oh.shape)\n",
    "source_oh=source_oh.reshape(1,source_oh.shape[0],source_oh.shape[1])\n",
    "print(\"source_oh shape after reshape:\", source_oh.shape)\n",
    "\n",
    "\n",
    "layer_timestep_0_1 = Model(inputs=model2.input,\n",
    "                                     outputs=[model2.get_layer('encoder_lstm').get_output_at(0)])\n",
    "#TODO: error : Output tensors to a Model must be the output of a Keras `Layer`\n",
    "#如何通过 get_output_at 拿到lstm layer 的输出\n",
    "\n",
    "\n",
    "layer_timestep_0_1 = layer_timestep_0_1.predict([source_oh, s0, c0,pred0]) \n",
    "# dense3_output_timestep_9.shape\n",
    "layer_timestep_0_1.shape\n",
    "\n",
    "\n",
    "model2.get_layer('encoder_lstm')\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 - BLEU score\n",
    "\n",
    "In this last part, you are going to implement the BLEU score to assess the effectiveness "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1.0, [1.0, 1.0, 1.0, 1.0], 1.0, 2.0, 4, 2)\n",
      "Cumulative 1-gram: 0.750000\n",
      "Cumulative 2-gram: 0.500000\n",
      "Cumulative 3-gram: 0.000000\n",
      "Cumulative 4-gram: 0.000000\n"
     ]
    }
   ],
   "source": [
    "from lib.bleu import compute_bleu\n",
    "reference = [['this', 'is', 'small', 'test']]\n",
    "candidate = ['this', 'is', 'a', 'test']\n",
    "\n",
    "score = compute_bleu(reference, candidate)\n",
    "print (score)\n",
    "\n",
    "\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "\n",
    "print('Cumulative 1-gram: %f' % sentence_bleu(reference, candidate, weights=(1, 0, 0, 0)))\n",
    "print('Cumulative 2-gram: %f' % sentence_bleu(reference, candidate, weights=(0.5, 0.5, 0, 0)))\n",
    "print('Cumulative 3-gram: %f' % sentence_bleu(reference, candidate, weights=(0.33, 0.33, 0.33, 0)))\n",
    "print('Cumulative 4-gram: %f' % sentence_bleu(reference, candidate, weights=(0.25, 0.25, 0.25, 0.25)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0, [0.9, 0.0, 0.0, 0.0], 1.0, 1.0, 10, 10)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score = compute_bleu(\"2002-03-03\", \"0002-03-03\")\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 10000/10000 [00:00<00:00, 20704.04it/s]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "m = 10000  # 数据集中的样本总数\n",
    "dataset, human_vocab, machine_vocab, inv_machine_vocab = load_dataset(m)\n",
    "\n",
    "Tx = 30\n",
    "Ty = 10\n",
    "X, Y, Xoh, Yoh = preprocess_data(dataset, human_vocab, machine_vocab, Tx, Ty)\n",
    "\n",
    "\n",
    "train_dataset,test_dataset =train_test_split(dataset, test_size=0.2, random_state=1024)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8000, 2)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-68eb343a8528>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;31m#     print(np.shape(source))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m     \u001b[0mprediction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msource\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mXoh\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ms0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mc0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m \u001b[1;31m#     print(np.shape(prediction))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m     \u001b[0mprediction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprediction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#作用于 最后一维的特征\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "from lib.bleu import compute_bleu,_get_ngrams\n",
    "\n",
    "\n",
    "# EXAMPLES = ['3 May 1979', '5 April 09', '21th of August 2016', 'Tue 10 Jul 2007', 'Saturday May 9 2018', 'March 3 2001', 'March 3rd 2001', '1 March 2001']\n",
    "# print('shape Xoh:',np.shape(Xoh))\n",
    "\n",
    "One_EXAMPLES = ['3 May 1979']\n",
    "EXAMPLES = ['3 May 1979', '5 Apr 09', '20th February 2016', 'Wed 10 Jul 2007']\n",
    "GROUND_TRUTH = ['1979-05-03', '2009-04-05', '2016-02-20', '2007-07-10']\n",
    "\n",
    "\n",
    "for index,example in enumerate(EXAMPLES):\n",
    "    \n",
    "    source = string_to_int(example, Tx, human_vocab)\n",
    "#     print (source)\n",
    "    source = np.array(list(map(lambda x: to_categorical(x, num_classes=len(human_vocab)), source)))#.swapaxes(0,1)\n",
    "#     print(np.shape(source))\n",
    "    \n",
    "    prediction = model.predict([np.reshape(source,[1,np.shape(Xoh)[1],-1]), s0, c0])\n",
    "#     print(np.shape(prediction))\n",
    "    prediction = np.argmax(prediction, axis = -1) #作用于 最后一维的特征\n",
    "#     print (np.shape(prediction))\n",
    "#     print (prediction)\n",
    "    output = [inv_machine_vocab[int(i)] for i in prediction]\n",
    "    output=''.join(output)\n",
    "    print(\"source:\", example)\n",
    "    print(\"output:\", output)\n",
    "    \n",
    "    target=GROUND_TRUTH[index]\n",
    "    print(\"target:\", target)\n",
    "    \n",
    "    \n",
    "    print(\"BLEU score: \", compute_bleu([target.split('-')], output.split('-'))[0])\n",
    "    \n",
    "#     print(\"BLEU score: \", compute_bleu([[ch for ch in target]],[ch for ch in output] )[0])\n",
    "    \n",
    "   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also change these examples to test with your own examples. The next part will give you a better sense on what the attention mechanism is doing--i.e., what part of the input the network is paying attention to when generating a particular output character. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 - Visualizing Attention (Optional / Ungraded)\n",
    "\n",
    "Since the problem has a fixed output length of 10, it is also possible to carry out this task using 10 different softmax units to generate the 10 characters of the output. But one advantage of the attention model is that each part of the output (say the month) knows it needs to depend only on a small part of the input (the characters in the input giving the month). We can  visualize what part of the output is looking at what part of the input.\n",
    "\n",
    "Consider the task of translating \"Saturday 9 May 2018\" to \"2018-05-09\". If we visualize the computed $\\alpha^{\\langle t, t' \\rangle}$ we get this: \n",
    "\n",
    "<img src=\"images/date_attention.png\" style=\"width:600;height:300px;\"> <br>\n",
    "<caption><center> **Figure 8**: Full Attention Map</center></caption>\n",
    "\n",
    "Notice how the output ignores the \"Saturday\" portion of the input. None of the output timesteps are paying much attention to that portion of the input. We see also that 9 has been translated as 09 and May has been correctly translated into 05, with the output paying attention to the parts of the input it needs to to make the translation. The year mostly requires it to pay attention to the input's \"18\" in order to generate \"2018.\" \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 - Getting the activations from the network\n",
    "\n",
    "Lets now visualize the attention values in your network. We'll propagate an example through the network, then visualize the values of $\\alpha^{\\langle t, t' \\rangle}$. \n",
    "\n",
    "To figure out where the attention values are located, let's start by printing a summary of the model ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Navigate through the output of `model.summary()` above. You can see that the layer named `attention_weights` outputs the `alphas` of shape (m, 30, 1) before `dot_2` computes the context vector for every time step $t = 0, \\ldots, T_y-1$. Lets get the activations from this layer.\n",
    "\n",
    "The function `attention_map()` pulls out the attention values from your model and plots them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "attention_map = plot_attention_map(model, human_vocab, inv_machine_vocab, \"Tuesday April 08 1993\", num = 6, n_s = 128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the generated plot you can observe the values of the attention weights for each character of the predicted output. Examine this plot and check that where the network is paying attention makes sense to you.\n",
    "\n",
    "In the date translation application, you will observe that most of the time attention helps predict the year, and hasn't much impact on predicting the day/month."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Congratulations!\n",
    "\n",
    "\n",
    "You have come to the end of this assignment \n",
    "\n",
    "<font color='blue'> **Here's what you should remember from this notebook**:\n",
    "\n",
    "- Machine translation models can be used to map from one sequence to another. They are useful not just for translating human languages (like French->English) but also for tasks like date format translation. \n",
    "- An attention mechanism allows a network to focus on the most relevant parts of the input when producing a specific part of the output. \n",
    "- A network using an attention mechanism can translate from inputs of length $T_x$ to outputs of length $T_y$, where $T_x$ and $T_y$ can be different. \n",
    "- You can visualize attention weights $\\alpha^{\\langle t,t' \\rangle}$ to see what the network is paying attention to while generating each output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations on finishing this assignment! You are now able to implement an attention model and use it to learn complex mappings from one sequence to another. "
   ]
  }
 ],
 "metadata": {
  "coursera": {
   "course_slug": "nlp-sequence-models",
   "graded_item_id": "n16CQ",
   "launcher_item_id": "npjGi"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
