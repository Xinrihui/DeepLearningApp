# value 全部是字符串, 注意类型转换

[DEFAULT]

# 数据集根目录
base_dir= dataset/WMT-14-English-Germa

# 训练源语料
train_source_corpus = train.en

# 训练目标语料
train_target_corpus = train.de

# 验证源语料
valid_source_corpus = newstest2013.en

# 验证目标语料
valid_target_corpus = newstest2013.de

# 测试源语料
test_source_corpus = newstest2014.en

# 测试目标语料
test_target_corpus = newstest2014.de

# 预处理的模式( 返回最终的数据集: final, 返回中间状态的数据集: mid)
preprocess_mode = final

# 是否将源序列倒置(倒置: 1, 否则: 0)
reverse_source = 1

# 语料库的标准化模式( 不进行标准化: none,  标准化: normalize )
normalize_mode = none

# 预处理后的数据集
cache_data_folder= cache_data

# 源语言的词典大小
n_vocab_source=50000

# 目标语言的词典大小
n_vocab_target=50000

# 最大序列的长度(训练集和验证集)
max_seq_length=50

# 最大序列的长度(测试集)
test_max_seq_length=100


# 词嵌入的维度
n_embedding = 1000

# 编码器 和 解码器的隐状态维度
n_h = 1000


# _null_str: 末尾填充的空
_null_str=

# _start_str: 句子的开始
_start_str= [START]

# _end_str: 句子的结束
_end_str= [END]

# _unk_str: 未登录词
_unk_str= [UNK]

# 各个层的 dropout 弃置率
dropout_rates = [0.2,0.2,0.2,0.2,0.2]

# 预训练模型的路径(用于推理)
model_path = models/cache/XXX

# 模型的推理结果文件
candidate_file = outs/candidates.txt

# build_mode: 建立 训练计算图的方式
# - 'Session' 手工建立计算图
# - 'Eager' 利用框架自动从代码中抽取出计算图
build_mode = Session


# save_mode: 模型持久化的形式
# - 'hdf5' 使用 hdf5 保存整个模型
# - 'SavedModel' 使用 SavedModel 保存整个模型
# - 'weight' 只保存权重
save_mode = hdf5

# checkpoint 模型路径
checkpoint_models_path = models/cache/

# 选择 min-Batch梯度下降时, 每一次输入模型的样本个数
batch_size = 256

# 配置tensorflow 的混合精度 (开启降低精度: mixed_float16, 不开启: float32)
mixed_precision = mixed_float16


# shuffle 的窗口大小
buffer_size = 1000000


# 模型训练的 epoch 个数,  一般训练集所有的样本模型都见过一遍才算一个 epoch
epoch_num = 15

# 推理使用 CPU/GPU ( 使用 CPU: /CPU:0 , 使用 GPU-0: /device:GPU:0 )
infer_device = /device:GPU:0


[TEST]

# 数据集根目录
base_dir= dataset/WMT-14-English-Germa

# 训练源语料
train_source_corpus = newstest2012.en

# 训练目标语料
train_target_corpus = newstest2012.de

# 验证源语料
valid_source_corpus = newstest2013.en

# 验证目标语料
valid_target_corpus = newstest2013.de

# 测试源语料
test_source_corpus = newstest2014.en

# 测试目标语料
test_target_corpus = newstest2014.de

# 预处理的模式 ( 返回最终的数据集: final, 返回中间状态的数据集: mid)
preprocess_mode = final

# 是否将源序列倒置(倒置: 1, 否则: 0)
reverse_source = 1

# 语料库的标准化模式( 不进行标准化: none, 标准化: normalize )
normalize_mode = none

# 预处理后的数据集
cache_data_folder= cache_small_data

# 源语言的词典大小
n_vocab_source=5000

# 目标语言的词典大小
n_vocab_target=5000

# 最大序列的长度(训练集和验证集)
max_seq_length=30

# 最大序列的长度(测试集)
test_max_seq_length=30


# 词嵌入的维度
n_embedding = 100

# 编码器 和 解码器的隐状态维度
n_h = 100

# _null_str: 末尾填充的空
_null_str=

# _start_str: 句子的开始
_start_str=[START]

# _end_str: 句子的结束
_end_str= [END]

# _unk_str: 未登录词
_unk_str= [UNK]

# 各个层的 dropout 弃置率
dropout_rates = [0.2,0.2,0.2,0.2,0.2]

# 预训练模型的路径(用于推理)
model_path = models/cache/model.01-2.9549

# 模型的推理结果文件
candidate_file = outs/candidates.txt

# build_mode: 建立 训练计算图的方式
# - 'Session' 手工建立计算图
# - 'Eager' 利用框架自动从代码中抽取出计算图
build_mode = Session

# save_mode: 模型持久化的形式
# - 'hdf5' 使用 hdf5 保存整个模型
# - 'SavedModel' 使用 SavedModel 保存整个模型
# - 'weight' 只保存权重
save_mode = hdf5

# checkpoint 模型路径
checkpoint_models_path = models/cache/

# 选择 min-Batch梯度下降时, 每一次输入模型的样本个数
batch_size = 32

# 配置tensorflow 的混合精度 (开启降低精度: mixed_float16, 不开启: float32)
mixed_precision = mixed_float16

# shuffle 的窗口大小
buffer_size = 1000

# 模型训练的 epoch 个数,  一般训练集所有的样本模型都见过一遍才算一个 epoch
epoch_num = 2

# 推理使用 CPU/GPU ( 使用 CPU: /CPU:0 , 使用 GPU-0: /device:GPU:0 )
infer_device = /device:GPU:0
