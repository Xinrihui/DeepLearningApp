# value 全部是字符串, 注意类型转换

[DEFAULT]

# 数据集根目录
base_dir= dataset/WMT-14-English-Germa

# 训练源语料
train_source_corpus = train.en

# 训练目标语料
train_target_corpus = train.de

# 验证源语料
valid_source_corpus = newstest2013.en

# 验证目标语料
valid_target_corpus = newstest2013.de

# 测试源语料
test_source_corpus = newstest2014.en

# 测试目标语料
test_target_corpus = newstest2014.de

# 分词的模式
# - space: 直接通过空格分词
# - subword: 通过 subword 算法分词
tokenize_mode = space

# 生成持久化数据集的模式
# - fixed_length: 返回已符号化的句子,并保证所有 batch 中序列的长度均相同, row = (source_vector, target_in), target_out
# - tokenized: 返回已符号化的句子, row = (source_vector, target_vector)
# - dynamic_batch: 返回符号化后的句子, 并动态划分 batch, row = (source_vector, target_vector)
# - text: 返回未符号化, 还是文本形式的数据集, row = (source, target)
return_mode = fixed_length

# 动态划分 batch 时, 1个 batch 的 token 个数
token_in_batch = 4096

# 训练数据集的混洗模式
# - row: 混洗的粒度为行(row)
# - batch: 混洗的粒度为 batch
shuffle_mode = row


# 训练数据集的预处理模式
# - none: 不做任何动作 (适用于 subword 分词器)
# - normalize: unicode 标准化 (适用于 subword 分词器)
# - add_control_token: 只对句子加入控制字符 (适用于 space 分词器)
# - all: 包括标准化在内的所有操作 (适用于 space 分词器)
preprocess_mode = add_control_token

# 是否将源序列倒置(倒置: 1, 否则: 0)
reverse_source = 1

# 预处理后的数据集
cache_data_folder= cache_data

# 源语言的词典大小
n_vocab_source=50000

# 目标语言的词典大小
n_vocab_target=50000

# 原始序列的最大长度(训练集和验证集)
org_seq_length=50

# 原始序列的最大长度(测试集)
test_org_seq_length=100

# 将原始序列标记化为固定的长度
fixed_seq_length = 52

# 推理时的最大序列长度
infer_target_length = 102

# 词嵌入的维度
n_embedding = 1000

# 编码器 和 解码器的隐状态维度
n_h = 1000


# _null_str: 末尾填充的空
_null_str=

# _start_str: 句子的开始
_start_str= [START]

# _end_str: 句子的结束
_end_str= [END]

# _unk_str: 未登录词
_unk_str= [UNK]

# 各个层的 dropout 弃置率
dropout_rates = [0.2,0.2,0.2,0.2,0.2]

# 预训练模型的路径(用于推理)
model_path = models/cache/XXX
;model_path = models/prev/attention_seq2seq

# 输出翻译结果文件
candidate_file = outs/candidate.txt

# 输出对照语料文件名前缀
reference_dir = outs/reference

# build_mode: 建立 训练计算图的方式
# - 'Session' 手工建立计算图
# - 'Eager' 利用框架自动从代码中抽取出计算图
build_mode = Session


# save_mode: 模型持久化的形式
# - 'hdf5' 使用 hdf5 保存整个模型
# - 'SavedModel' 使用 SavedModel 保存整个模型
# - 'weight' 只保存权重
save_mode = weight

# checkpoint 模型路径
checkpoint_models_path = models/cache/

# 选择 min-Batch梯度下降时, 每一次输入模型的样本个数
batch_size = 256

# 配置tensorflow 的混合精度 (开启降低精度: mixed_float16, 不开启: float32)
mixed_precision = mixed_float16


# shuffle 的窗口大小
buffer_size = 1000000


# 模型训练的 epoch 个数,  一般训练集所有的样本模型都见过一遍才算一个 epoch
epoch_num = 10

# 推理使用 CPU/GPU ( 使用 CPU: /CPU:0 , 使用 GPU-0: /device:GPU:0 )
infer_device = /device:GPU:0


[TEST]

# 数据集根目录
base_dir= dataset/WMT-14-English-Germa

# 训练源语料
train_source_corpus = newstest2012.en

# 训练目标语料
train_target_corpus = newstest2012.de

# 验证源语料
valid_source_corpus = newstest2013.en

# 验证目标语料
valid_target_corpus = newstest2013.de

# 测试源语料
test_source_corpus = newstest2014.en

# 测试目标语料
test_target_corpus = newstest2014.de

# 分词的模式
# - space: 直接通过空格分词
# - subword: 通过 subword 算法分词
tokenize_mode = space

# 生成持久化数据集的模式
# - fixed_length: 返回已符号化的句子,并保证所有 batch 中序列的长度均相同, row = (source_vector, target_in), target_out
# - tokenized: 返回已符号化的句子, row = (source_vector, target_vector)
# - dynamic_batch: 返回符号化后的句子, 并动态划分 batch, row = (source_vector, target_vector)
# - text: 返回未符号化, 还是文本形式的数据集, row = (source, target)
return_mode = fixed_length

# 动态划分 batch 时, 1个 batch 的 token 个数
token_in_batch = 4096

# 训练数据集的混洗模式
# - row: 混洗的粒度为行(row)
# - batch: 混洗的粒度为 batch
shuffle_mode = row


# 训练数据集的预处理模式
# - none: 不做任何动作 (适用于 subword 分词器)
# - normalize: unicode 标准化 (适用于 subword 分词器)
# - add_control_token: 只对句子加入控制字符 (适用于 space 分词器)
# - all: 包括标准化在内的所有操作 (适用于 space 分词器)
preprocess_mode = add_control_token

# 是否将源序列倒置(倒置: 1, 否则: 0)
reverse_source = 1

# 预处理后的数据集
cache_data_folder= cache_small_data

# 源语言的词典大小
n_vocab_source=5000

# 目标语言的词典大小
n_vocab_target=5000

# 原始序列的最大长度(训练集和验证集)
org_seq_length=30

# 原始序列的最大长度(测试集)
test_org_seq_length=30

# 将原始序列标记化为固定的长度
fixed_seq_length = 32

# 推理时的最大序列长度
infer_target_length = 32

# 词嵌入的维度
n_embedding = 100

# 编码器 和 解码器的隐状态维度
n_h = 100

# _null_str: 末尾填充的空
_null_str=

# _start_str: 句子的开始
_start_str=[START]

# _end_str: 句子的结束
_end_str= [END]

# _unk_str: 未登录词
_unk_str= [UNK]

# 各个层的 dropout 弃置率
dropout_rates = [0.2,0.2,0.2,0.2,0.2]

# 预训练模型的路径(用于推理)
model_path = models/cache/model.02-2.7839

# 输出翻译结果文件
candidate_file = outs/candidate.txt

# 输出对照语料文件名前缀
reference_dir = outs/reference

# build_mode: 建立 训练计算图的方式
# - 'Session' 手工建立计算图
# - 'Eager' 利用框架自动从代码中抽取出计算图
build_mode = Eager

# save_mode: 模型持久化的形式
# - 'hdf5' 使用 hdf5 保存整个模型
# - 'SavedModel' 使用 SavedModel 保存整个模型
# - 'weight' 只保存权重
save_mode = weight

# checkpoint 模型路径
checkpoint_models_path = models/cache/

# 选择 min-Batch梯度下降时, 每一次输入模型的样本个数
batch_size = 32

# 配置tensorflow 的混合精度 (开启降低精度: mixed_float16, 不开启: float32)
mixed_precision = mixed_float16

# shuffle 的窗口大小
buffer_size = 1000

# 模型训练的 epoch 个数,  一般训练集所有的样本模型都见过一遍才算一个 epoch
epoch_num = 10

# 推理使用 CPU/GPU ( 使用 CPU: /CPU:0 , 使用 GPU-0: /device:GPU:0 )
infer_device = /device:GPU:0
