# value 全部是字符串, 注意类型转换

[DEFAULT]

# 数据集根目录
base_dir= dataset/WMT-14-English-Germa

# 训练源语料
train_source_corpus = train.en

# 训练目标语料
train_target_corpus = train.de

# 验证源语料
valid_source_corpus = newstest2013.en

# 验证目标语料
valid_target_corpus = newstest2013.de

# 测试源语料
test_source_corpus = newstest2014.en

# 测试目标语料
test_target_corpus = newstest2014.de

# 分词的模式
# - space: 直接通过空格分词
# - subword: 通过 subword 算法分词
# - subword_union: 通过 subword 算法分词, 并且源语言和目标语言共用一个词典, 词典大小通过 n_vocab_target 配置
tokenize_mode = subword_union

# 生成持久化数据集的模式
# - fixed_length: 返回已符号化的句子,并保证所有 batch 中序列的长度均相同, row = (source_vector, target_in), target_out
# - tokenized: 返回已符号化的句子, row = (source_vector, target_vector)
# - dynamic_batch: 返回符号化后的句子, 并动态划分 batch, row = (source_vector, target_vector)
# - text: 返回未符号化, 还是文本形式的数据集, row = (source, target)
return_mode = dynamic_batch

# 动态划分 batch 时, 1个 batch 的 token 个数
token_in_batch = 12288
# 推荐取值 4096, 8192, 12288
# RTX3090(24GB)可选最大 12288

# 训练数据集的混洗模式
# - row: 混洗的粒度为行(row)
# - batch: 混洗的粒度为 batch
shuffle_mode = batch

# 训练数据集的预处理模式
# - none: 不做任何动作 (适用于 subword 分词器)
# - normalize: unicode 标准化 (适用于 subword 分词器)
# - add_control_token: 只对句子加入控制字符 (适用于 space 分词器)
# - all: 包括标准化在内的所有操作 (适用于 space 分词器)
train_preprocess_mode = none

# 测试数据集的预处理模式
# - none: 不做任何动作 (适用于 subword 分词器)
# - add_control_token: 只对句子加入控制字符 (适用于 space 分词器)
test_preprocess_mode = none

# 是否将源序列倒置(倒置: 1, 否则: 0)
reverse_source = 0

# 预处理后的数据集
cache_data_folder= cache_data

# 源语言的词典大小
n_vocab_source=37000

# 目标语言的词典大小
n_vocab_target=37000

# 原始序列的最大长度(训练集和验证集)
org_seq_length=64
# transformer 源码(tensor2tensor)中配置为 256

# 原始序列的最大长度(测试集)
test_org_seq_length=100

# 将原始序列标记化为固定的长度
fixed_seq_length = 200

# 推理时的最大序列长度
infer_target_length = 200

# 源句子的可能最大长度
maximum_position_source = 10000

# 目标句子的可能最大长度
maximum_position_target = 10000

# 堆叠的编码器(解码器)的层数
num_layers = 6

# 模型整体的隐藏层的维度
d_model = 512

#并行注意力层的个数(头数)
num_heads = 8

#Position-wise Feed-Forward 的中间层的维度
dff = 2048


# _null_str: 末尾填充的空
_null_str=

# _start_str: 句子的开始
_start_str= [START]

# _end_str: 句子的结束
_end_str= [END]

# _unk_str: 未登录词
_unk_str= [UNK]

# 各个层的 dropout 弃置率
dropout_rates = [0.1,0.1,0.1,0.1,0.1]

# 预训练模型的路径(用于推理)
model_path = models/cache/
;model_path = models/prev/attention_seq2seq

# 输出翻译结果文件
candidate_file = outs/candidate.txt

# 输出对照语料文件名前缀
reference_dir = outs/reference

# build_mode: 建立 训练计算图的方式
# - 'Session' 手工建立计算图
# - 'Eager' 利用框架自动从代码中抽取出计算图
build_mode = Eager


# save_mode: 模型持久化的形式
# - 'hdf5' 使用 hdf5 保存整个模型
# - 'SavedModel' 使用 SavedModel 保存整个模型
# - 'weight' 只保存权重
save_mode = weight

# checkpoint 模型路径
checkpoint_models_path = models/cache/

# 选择 min-Batch梯度下降时, 每一次输入模型的样本个数
batch_size = 256

# 标签平滑
# 取值范围 [0, 1]
# - 0 不开启标签平滑
label_smoothing=0

# 优化器学习率的预热步骤
warmup_steps = 32000
# 推荐取值 16000, 32000, 48000

# 配置tensorflow 的混合精度 (开启降低精度: mixed_float16, 不开启: float32)
mixed_precision = float32

# shuffle 的窗口大小
buffer_size = 20000


# 模型训练的 epoch 个数,  一般训练集所有的样本模型都见过一遍才算一个 epoch
epoch_num = 12

# 推理使用 CPU/GPU ( 使用 CPU: /CPU:0 , 使用 GPU-0: /device:GPU:0 )
infer_device = /device:GPU:0


[TEST]

# 数据集根目录
base_dir= dataset/WMT-14-English-Germa

# 训练源语料
train_source_corpus = newstest2012.en

# 训练目标语料
train_target_corpus = newstest2012.de

# 验证源语料
valid_source_corpus = newstest2013.en

# 验证目标语料
valid_target_corpus = newstest2013.de

# 测试源语料
test_source_corpus = newstest2014.en

# 测试目标语料
test_target_corpus = newstest2014.de


# 分词的模式
# - space: 直接通过空格分词
# - subword: 通过 subword 算法分词
# - subword_union: 通过 subword 算法分词, 并且源语言和目标语言共用一个词典, 词典大小通过 n_vocab_target 配置
tokenize_mode = subword_union

# 生成持久化数据集的模式
# - fixed_length: 返回已符号化的句子,并保证所有 batch 中序列的长度均相同, row = (source_vector, target_in), target_out
# - tokenized: 返回已符号化的句子, row = (source_vector, target_vector)
# - dynamic_batch: 返回符号化后的句子, 并动态划分 batch, row = (source_vector, target_vector)
# - text: 返回未符号化, 还是文本形式的数据集, row = (source, target)
return_mode = dynamic_batch

# 动态划分 batch 时, 1个 batch 的 token 个数
token_in_batch = 4096

# 训练数据集的混洗模式
# - row: 混洗的粒度为行(row)
# - batch: 混洗的粒度为 batch
shuffle_mode = batch

# 训练数据集的预处理模式
# - none: 不做任何动作 (适用于 subword 分词器)
# - normalize: unicode 标准化 (适用于 subword 分词器)
# - add_control_token: 只对句子加入控制字符 (适用于 space 分词器)
# - all: 包括标准化在内的所有操作 (适用于 space 分词器)
train_preprocess_mode = none

# 测试数据集的预处理模式
# - none: 不做任何动作 (适用于 subword 分词器)
# - add_control_token: 只对句子加入控制字符 (适用于 space 分词器)
test_preprocess_mode = none

# 是否将源序列倒置(倒置: 1, 否则: 0)
reverse_source = 0

# 预处理后的数据集
cache_data_folder= cache_small_data

# 源语言的词典大小
n_vocab_source=5000

# 目标语言的词典大小
n_vocab_target=5000

# 原始序列的最大长度(训练集和验证集)
org_seq_length=30

# 原始序列的最大长度(测试集)
test_org_seq_length=30

# 将原始序列标记化为固定的长度
fixed_seq_length = 60

# 推理时的最大序列长度
infer_target_length = 60

# 源句子的可能最大长度
maximum_position_source = 1000

# 目标句子的可能最大长度
maximum_position_target = 1000

# 堆叠的编码器(解码器)的层数
num_layers = 2

# 模型整体的隐藏层的维度
d_model = 128

#并行注意力层的个数(头数)
num_heads = 8

#Position-wise Feed-Forward 的中间层的维度
dff = 256


# _null_str: 末尾填充的空
_null_str=

# _start_str: 句子的开始
_start_str=[START]

# _end_str: 句子的结束
_end_str= [END]

# _unk_str: 未登录词
_unk_str= [UNK]

# 各个层的 dropout 弃置率
dropout_rates = [0.1,0.1,0.1,0.1,0.1]

# 预训练模型的路径(用于推理)
model_path = models/cache/model.09-4.1815

# 输出翻译结果文件
candidate_file = outs/candidate.txt

# 输出对照语料文件名前缀
reference_dir = outs/reference

# build_mode: 建立训练计算图的方式
# - 'Session' 手工建立计算图
# - 'Eager' 利用框架自动从代码中抽取出计算图
build_mode = Eager

# save_mode: 模型持久化的形式
# - 'hdf5' 使用 hdf5 保存整个模型
# - 'SavedModel' 使用 SavedModel 保存整个模型
# - 'weight' 只保存权重
save_mode = weight

# checkpoint 模型路径
checkpoint_models_path = models/cache/

# 选择 min-Batch梯度下降时, 每一次输入模型的样本个数
batch_size = 32

# 标签平滑
# 取值范围 [0, 1]
# - 0 不开启标签平滑
label_smoothing=0

# 优化器学习率的预热步骤
warmup_steps = 4000

# 配置tensorflow 的混合精度 (开启降低精度: mixed_float16, 不开启: float32)
mixed_precision = mixed_float16

# shuffle 的窗口大小
buffer_size = 1000

# 模型训练的 epoch 个数,  一般训练集所有的样本模型都见过一遍才算一个 epoch
epoch_num = 10

# 推理使用 CPU/GPU ( 使用 CPU: /CPU:0 , 使用 GPU-0: /device:GPU:0 )
infer_device = /device:GPU:0

[TEST-1]

# 数据集根目录
base_dir= dataset/TED-Portuguese-English

# 训练源语料
train_source_corpus = pt.train

# 训练目标语料
train_target_corpus = en.train

# 验证源语料
valid_source_corpus = pt.dev

# 验证目标语料
valid_target_corpus = en.dev

# 测试源语料
test_source_corpus = pt.test

# 测试目标语料
test_target_corpus = en.test


# 分词的模式
# - space: 直接通过空格分词
# - subword: 通过 subword 算法分词
# - subword_union: 通过 subword 算法分词, 并且源语言和目标语言共用一个词典, 词典大小通过 n_vocab_target 配置
tokenize_mode = subword_union

# 生成持久化数据集的模式
# - fixed_length: 返回已符号化的句子,并保证所有 batch 中序列的长度均相同, row = (source_vector, target_in), target_out
# - tokenized: 返回已符号化的句子, row = (source_vector, target_vector)
# - dynamic_batch: 返回符号化后的句子, 并动态划分 batch, row = (source_vector, target_vector)
# - text: 返回未符号化, 还是文本形式的数据集, row = (source, target)
return_mode = text

# 动态划分 batch 时, 1个 batch 的 token 个数
token_in_batch = 2048

# 训练数据集的混洗模式
# - row: 混洗的粒度为行(row), 仅适用于 return_mode = text
# - batch: 混洗的粒度为 batch
shuffle_mode = row

# 训练数据集的预处理模式
# - none: 不做任何动作 (适用于 subword 分词器)
# - normalize: unicode 标准化 (适用于 subword 分词器)
# - add_control_token: 只对句子加入控制字符 (适用于 space 分词器)
# - all: 包括标准化在内的所有操作 (适用于 space 分词器)
train_preprocess_mode = none

# 测试数据集的预处理模式
# - none: 不做任何动作 (适用于 subword 分词器)
# - add_control_token: 只对句子加入控制字符 (适用于 space 分词器)
test_preprocess_mode = none

# 是否将源序列倒置(倒置: 1, 否则: 0)
reverse_source = 0

# 预处理后的数据集
cache_data_folder= cache_data

# 源语言的词典大小
n_vocab_source=8000

# 目标语言的词典大小
n_vocab_target=8000

# 原始序列的最大长度(训练集和验证集)
org_seq_length=205

# 原始序列的最大长度(测试集)
test_org_seq_length=205

# 将原始序列标记化为固定的长度
fixed_seq_length = 250

# 推理时的最大序列长度
infer_target_length = 250

# 源句子的最大编码位置
maximum_position_source = 1000

# 目标句子的最大编码位置
maximum_position_target = 1000

# 堆叠的编码器(解码器)的层数
num_layers = 4

# 模型整体的隐藏层的维度
d_model = 128

#并行注意力层的个数(头数)
num_heads = 8

#Position-wise Feed-Forward 的中间层的维度
dff = 512


# _null_str: 末尾填充的空
_null_str=

# _start_str: 句子的开始
_start_str=[START]

# _end_str: 句子的结束
_end_str= [END]

# _unk_str: 未登录词
_unk_str= [UNK]

# 各个层的 dropout 弃置率
dropout_rates = [0.1,0.1,0.1,0.1,0.1]

# 预训练模型的路径(用于推理)
model_path = models/cache/model.04-0.9588

# 输出翻译结果文件
candidate_file = outs/candidate.txt

# 输出对照语料文件名前缀
reference_dir = outs/reference

# build_mode: 建立训练计算图的方式
# - 'Session' 手工建立计算图
# - 'Eager' 利用框架自动从代码中抽取出计算图
build_mode = Eager

# save_mode: 模型持久化的形式
# - 'hdf5' 使用 hdf5 保存整个模型
# - 'SavedModel' 使用 SavedModel 保存整个模型
# - 'weight' 只保存权重
save_mode = weight

# checkpoint 模型路径
checkpoint_models_path = models/cache/

# 选择 min-Batch梯度下降时, 每一次输入模型的样本个数
batch_size = 64

# 标签平滑
# 取值范围 [0, 1]
# - 0 不开启标签平滑
label_smoothing=0

# 优化器学习率的预热步骤
warmup_steps = 4000

# 配置tensorflow 的混合精度 (开启降低精度: mixed_float16, 不开启: float32)
mixed_precision = float32

# shuffle 的窗口大小
buffer_size = 20000

# 模型训练的 epoch 个数,  一般训练集所有的样本模型都见过一遍才算一个 epoch
epoch_num = 20

# 推理使用 CPU/GPU ( 使用 CPU: /CPU:0 , 使用 GPU-0: /device:GPU:0 )
infer_device = /device:GPU:0

