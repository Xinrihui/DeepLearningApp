# value 全部是字符串, 注意类型转换

[DEFAULT]

# 数据集根目录
base_dir= dataset/WMT-14-English-Germa

# 训练源语料
train_source_corpus = train.en

# 训练目标语料
train_target_corpus = train.de

# 验证源语料
valid_source_corpus = newstest2013.en

# 验证目标语料
valid_target_corpus = newstest2013.de

# 测试源语料
test_source_corpus = newstest2014.en

# 测试目标语料
test_target_corpus = newstest2014.de

# 分词的模式
# - space: 直接通过空格分词
# - subword: 通过 subword 算法分词
tokenize_mode = subword

# 生成数据集的模式
# - symbolic: 返回已符号化的数据集, row = (source_vector, target_in), target_out
# - text: 返回未符号化, 还是文本形式的数据集, row = (source, target)
# - dynamic_batch: 返回符号化后的句子, 并动态划分 batch, row = (source_vector, target_vector)
return_mode = dynamic_batch

# 动态划分 batch 时, 1个 batch 的 token 个数
token_in_batch = 4096


# 训练数据集的混洗模式
# - row: 混洗的粒度为行(row)
# - batch: 混洗的粒度为 batch
shuffle_mode = batch

# 训练数据集的预处理模式
# - none: 不做任何动作
# - add_control_token: 只对句子加入控制字符
# - normalize: unicode 标准化
# - all: 包括标准化在内的所有操作
preprocess_mode = normalize

# 是否将源序列倒置(倒置: 1, 否则: 0)
reverse_source = 0

# 预处理后的数据集
cache_data_folder= cache_data

# 源语言的词典大小
n_vocab_source=37000

# 目标语言的词典大小
n_vocab_target=37000

# 最大序列的长度(训练集和验证集)
max_seq_length=50

# 最大序列的长度(测试集)
test_max_seq_length=100

# 最大序列长度的增加量
increment = 50

# 源句子的可能最大长度
maximum_position_source = 10000

# 目标句子的可能最大长度
maximum_position_target = 10000

# 堆叠的编码器(解码器)的层数
num_layers = 6

# 模型整体的隐藏层的维度
d_model = 512

#并行注意力层的个数(头数)
num_heads = 8

#Position-wise Feed-Forward 的中间层的维度
dff = 2048


# _null_str: 末尾填充的空
_null_str=

# _start_str: 句子的开始
_start_str= [START]

# _end_str: 句子的结束
_end_str= [END]

# _unk_str: 未登录词
_unk_str= [UNK]

# 各个层的 dropout 弃置率
dropout_rates = [0.1,0.1,0.1,0.1,0.1]

# 预训练模型的路径(用于推理)
model_path = models/cache/XXX
;model_path = models/prev/attention_seq2seq

# 输出翻译结果文件
candidate_file = outs/candidate.txt

# 输出对照语料文件名前缀
reference_dir = outs/reference

# build_mode: 建立 训练计算图的方式
# - 'Session' 手工建立计算图
# - 'Eager' 利用框架自动从代码中抽取出计算图
build_mode = Eager


# save_mode: 模型持久化的形式
# - 'hdf5' 使用 hdf5 保存整个模型
# - 'SavedModel' 使用 SavedModel 保存整个模型
# - 'weight' 只保存权重
save_mode = weight

# checkpoint 模型路径
checkpoint_models_path = models/cache/

# 选择 min-Batch梯度下降时, 每一次输入模型的样本个数
batch_size = 128

# 标签平滑
label_smoothing=0.1

# 配置tensorflow 的混合精度 (开启降低精度: mixed_float16, 不开启: float32)
mixed_precision = mixed_float16

# shuffle 的窗口大小
buffer_size = 100000


# 模型训练的 epoch 个数,  一般训练集所有的样本模型都见过一遍才算一个 epoch
epoch_num = 5

# 推理使用 CPU/GPU ( 使用 CPU: /CPU:0 , 使用 GPU-0: /device:GPU:0 )
infer_device = /device:GPU:0


[TEST]

# 数据集根目录
base_dir= dataset/WMT-14-English-Germa

# 训练源语料
train_source_corpus = newstest2012.en

# 训练目标语料
train_target_corpus = newstest2012.de

# 验证源语料
valid_source_corpus = newstest2013.en

# 验证目标语料
valid_target_corpus = newstest2013.de

# 测试源语料
test_source_corpus = newstest2014.en

# 测试目标语料
test_target_corpus = newstest2014.de


# 分词的模式
# - space: 直接通过空格分词
# - subword: 通过 subword 算法分词
tokenize_mode = subword

# 生成数据集的模式
# - symbolic: 返回已符号化的数据集, row = (source_vector, target_in), target_out
# - text: 返回未符号化, 还是文本形式的数据集, row = (source, target)
# - dynamic_batch: 返回符号化后的句子, 并动态划分 batch, row = (source_vector, target_vector)
return_mode = dynamic_batch

# 动态划分 batch 时, 1个 batch 的 token 个数
token_in_batch = 4096

# 训练数据集的混洗模式
# - row: 混洗的粒度为行(row)
# - batch: 混洗的粒度为 batch
shuffle_mode = batch

# 训练数据集的预处理模式
# - none: 不做任何动作
# - add_control_token: 只对句子加入控制字符
# - normalize: unicode 标准化
# - all: 包括标准化在内的所有操作
preprocess_mode = none

# 是否将源序列倒置(倒置: 1, 否则: 0)
reverse_source = 0

# 预处理后的数据集
cache_data_folder= cache_small_data

# 源语言的词典大小
n_vocab_source=5000

# 目标语言的词典大小
n_vocab_target=5000

# 最大序列的长度(训练集和验证集)
max_seq_length=30

# 最大序列的长度(测试集)
test_max_seq_length=30

# 最大序列长度的增加量
increment = 30

# 源句子的可能最大长度
maximum_position_source = 1000

# 目标句子的可能最大长度
maximum_position_target = 1000

# 堆叠的编码器(解码器)的层数
num_layers = 2

# 模型整体的隐藏层的维度
d_model = 128

#并行注意力层的个数(头数)
num_heads = 8

#Position-wise Feed-Forward 的中间层的维度
dff = 256


# _null_str: 末尾填充的空
_null_str=

# _start_str: 句子的开始
_start_str=[START]

# _end_str: 句子的结束
_end_str= [END]

# _unk_str: 未登录词
_unk_str= [UNK]

# 各个层的 dropout 弃置率
dropout_rates = [0.1,0.1,0.1,0.1,0.1]

# 预训练模型的路径(用于推理)
model_path = models/cache/model.01-3.3536

# 输出翻译结果文件
candidate_file = outs/candidate.txt

# 输出对照语料文件名前缀
reference_dir = outs/reference

# build_mode: 建立训练计算图的方式
# - 'Session' 手工建立计算图
# - 'Eager' 利用框架自动从代码中抽取出计算图
build_mode = Eager

# save_mode: 模型持久化的形式
# - 'hdf5' 使用 hdf5 保存整个模型
# - 'SavedModel' 使用 SavedModel 保存整个模型
# - 'weight' 只保存权重
save_mode = weight

# checkpoint 模型路径
checkpoint_models_path = models/cache/

# 选择 min-Batch梯度下降时, 每一次输入模型的样本个数
batch_size = 32

# 标签平滑
label_smoothing=0.1

# 配置tensorflow 的混合精度 (开启降低精度: mixed_float16, 不开启: float32)
mixed_precision = mixed_float16

# shuffle 的窗口大小
buffer_size = 1000

# 模型训练的 epoch 个数,  一般训练集所有的样本模型都见过一遍才算一个 epoch
epoch_num = 3

# 推理使用 CPU/GPU ( 使用 CPU: /CPU:0 , 使用 GPU-0: /device:GPU:0 )
infer_device = /device:GPU:0
