{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "t2KbLji6JriC"
   },
   "outputs": [],
   "source": [
    "# -*- coding: UTF-8 -*-\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "\n",
    "from tensorflow.keras.layers import Layer, LSTM, Input, Embedding, Dense, Activation, Flatten,concatenate\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "from tensorflow.keras.activations import *\n",
    "\n",
    "from tensorflow.keras.models import Sequential,Model\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from tensorflow.data import Dataset\n",
    "\n",
    "# import keras_tuner as kt\n",
    "\n",
    "from tensorflow.keras.layers import TextVectorization, StringLookup\n",
    "\n",
    "import tensorflow_text as tf_text\n",
    "\n",
    "import os\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow version 2.7.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(\"Tensorflow version \" + tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-ZvSw_owHbfR"
   },
   "source": [
    "# 下载数据集"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  keras.dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 233
    },
    "colab_type": "code",
    "id": "nWuDkqLRKRd9",
    "outputId": "de57cbb8-9cf7-4871-f4fa-cdc6d32ca62b"
   },
   "outputs": [],
   "source": [
    "data = keras.datasets.fashion_mnist\n",
    "\n",
    "(x_train, y_train),(x_test, y_test) = data.load_data()\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "\n",
    "print( x_train.shape )\n",
    "print( x_test.shape )\n",
    "print( y_train.shape )\n",
    "print( y_test.shape )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "NvZUwoVbNVgO",
    "outputId": "86068f24-1990-4078-a409-be8feae34eb5"
   },
   "outputs": [],
   "source": [
    "class_names = [\"T-shirt/top\", \"Trouser\", \"Pullover\", \"Dress\", \"Coat\",\n",
    "               \"Sandal\", \"Shirt\", \"Sneaker\", \"Bag\", \"Ankle boot\"]\n",
    "\n",
    "class_names[y_train[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 287
    },
    "colab_type": "code",
    "id": "ND_ykm8YPINQ",
    "outputId": "9f6fac88-23ab-4892-babf-a9b878a2270d"
   },
   "outputs": [],
   "source": [
    "plt.imshow(x_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  tensorflow_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "# Construct a tf.data.Dataset\n",
    "ds = tfds.load('mnist', split='train', shuffle_files=True)\n",
    "\n",
    "# Build your input pipeline\n",
    "# ds = ds.shuffle(1024).batch(32).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "for example in ds.take(1):\n",
    "  image, label = example[\"image\"], example[\"label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(image.shape)\n",
    "\n",
    "# image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## hugging dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.processors import TemplateProcessing\n",
    "from tokenizers import normalizers\n",
    "from tokenizers.normalizers import Lowercase, NFD, StripAccents\n",
    "from tokenizers.trainers import BpeTrainer, WordLevelTrainer\n",
    "from tokenizers.models import WordLevel, BPE\n",
    "from tokenizers.pre_tokenizers import Whitespace,WhitespaceSplit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config=dict(\n",
    "    # RUN CONFIG:\n",
    "    RUN_NAME='unofficial_single_gpu_run',\n",
    "    RUN_DESCRIPTION='No description',\n",
    "    RUNS_FOLDER_PTH='../runs',\n",
    "    # DATA CONFIG:\n",
    "    DATASET_SIZE=30000, # WARN: This is too small but makes it easy to go through the notebook fast\n",
    "    TEST_PROPORTION=0.01,\n",
    "    MAX_SEQ_LEN=40,\n",
    "    VOCAB_SIZE=60000,\n",
    "    TOKENIZER_TYPE='wordlevel',\n",
    "    # TRAINING CONFIG:\n",
    "    BATCH_SIZE=48, \n",
    "    GRAD_ACCUMULATION_STEPS=2048//48,\n",
    "    WORKER_COUNT=10,\n",
    "    EPOCHS=100,\n",
    "    # OPTIMIZER CONFIG:\n",
    "    BETAS=(0.9, 0.98),\n",
    "    EPS=1e-9,\n",
    "    # SCHEDULER CONFIG:\n",
    "    N_WARMUP_STEPS=4000, \n",
    "    # MODEL CONFIG:\n",
    "    D_MODEL=512,\n",
    "    N_BLOCKS=6,\n",
    "    N_HEADS=8,\n",
    "    D_FF=2048,\n",
    "    DROPOUT_PROBA=0.1,\n",
    "    # OTHER:\n",
    "    MODEL_SAVE_EPOCH_CNT=10,\n",
    "    DEVICE='gpu',\n",
    "    LABEL_SMOOTHING=0.1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset_builder\n",
    "dataset_builder = load_dataset_builder('wmt14', 'de-en')\n",
    "print(dataset_builder.cache_dir)\n",
    "print(dataset_builder.info.features)\n",
    "print(dataset_builder.info.splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cache_dir='dataset/wmt14/de-en/1.0.0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=load_dataset('wmt14','de-en',split='train')#.shuffle(seed=42)\n",
    "\n",
    "# data=load_dataset('dataset/wmt14/de-en/1.0.0',split='train')\n",
    "\n",
    "data=data.select(range(30000)) \n",
    "data=data.flatten()\n",
    "data=data.rename_column('translation.de','translation_trg')\n",
    "data=data.rename_column('translation.en','translation_src')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数据预处理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 操作 tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 转换 Numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.constant(10)\n",
    "\n",
    "tf.constant(10).numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 累加"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = tf.constant(1)\n",
    "\n",
    "a += 1\n",
    "\n",
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### reshape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a =  [[[1, 3], [2, 3]],\n",
    "        [[2, 1], [1, 2]],\n",
    "        [[3, 3], [3, 2]]]\n",
    "\n",
    "\n",
    "# a.numpy()\n",
    "\n",
    "\n",
    "tf.shape(a)\n",
    "\n",
    "b = tf.reshape(a, [-1,2])\n",
    "\n",
    "b.numpy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 按照索引取元素"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### tf.gather"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 3])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=np.array([1,2,3,4])\n",
    "\n",
    "a[[0,2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['ab the', 'b', 'c'], dtype='<U6')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "['ab the', 'b', 'c']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.array(['ab the','b','c'])\n",
    "\n",
    "a\n",
    "\n",
    "list(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['a', 'b'], dtype='<U1')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.array(['a','b','c'])\n",
    "\n",
    "a[[0,1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = tf.constant(['a','b','c'])\n",
    "\n",
    "a[0]\n",
    "\n",
    "# a[[0,1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'p3'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "b'p3'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = tf.constant(['p0', 'p1', 'p2', 'p3', 'p4', 'p5'])\n",
    "params[3].numpy()\n",
    "\n",
    "tf.gather(params, 3).numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([b'p2', b'p0', b'p2', b'p5'], dtype=object)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices = [2, 0, 2, 5]\n",
    "tf.gather(params, indices).numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[30., 31., 32.],\n",
       "       [10., 11., 12.]], dtype=float32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([[ 2.,  1.],\n",
       "       [12., 11.],\n",
       "       [22., 21.],\n",
       "       [32., 31.]], dtype=float32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = tf.constant([[0, 1.0, 2.0],\n",
    "                      [10.0, 11.0, 12.0],\n",
    "                      [20.0, 21.0, 22.0],\n",
    "                      [30.0, 31.0, 32.0]])\n",
    "\n",
    "tf.gather(params, indices=[3,1]).numpy()\n",
    "\n",
    "\n",
    "tf.gather(params, indices=[2,1], axis=1).numpy()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### tf.nn.embedding_lookup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 3), dtype=float32, numpy=\n",
       "array([[ 0.,  1.,  2.],\n",
       "       [10., 11., 12.],\n",
       "       [20., 21., 22.]], dtype=float32)>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "params = tf.constant([[0, 1.0, 2.0],\n",
    "                      [10.0, 11.0, 12.0],\n",
    "                      [20.0, 21.0, 22.0],\n",
    "                      [30.0, 31.0, 32.0]])\n",
    "\n",
    "\n",
    "ids = [0, 1, 2]\n",
    "\n",
    "out = tf.nn.embedding_lookup(params, ids)\n",
    "\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 2, 3), dtype=float32, numpy=\n",
       "array([[[ 0.,  1.,  2.],\n",
       "        [10., 11., 12.]],\n",
       "\n",
       "       [[10., 11., 12.],\n",
       "        [20., 21., 22.]]], dtype=float32)>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "ids = tf.constant( [\n",
    "                    [0, 1],\n",
    "                    [1, 2]\n",
    "                    ])\n",
    "\n",
    "out = tf.nn.embedding_lookup(params, ids)\n",
    "\n",
    "out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 数组 (list append)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ta = tf.TensorArray(tf.float32, size=3, clear_after_read=False)\n",
    "\n",
    "ta = tf.TensorArray(tf.float32, size=3)\n",
    "\n",
    "\n",
    "ta = ta.write(0, 10)\n",
    "ta = ta.write(1, 20)\n",
    "ta = ta.write(2, 30)\n",
    "\n",
    "ta.read(0)\n",
    "\n",
    "ta.read(1)\n",
    "\n",
    "ta.read(2)\n",
    "\n",
    "ta.stack()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "array = tf.TensorArray(tf.float32, size=0, dynamic_size=True)\n",
    "\n",
    "slice0 = tf.ones((3,))\n",
    "\n",
    "array = array.write(0, slice0)\n",
    "\n",
    "\n",
    "array.stack()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed = Embedding(20,10)\n",
    "\n",
    "x = embed(array.stack())\n",
    "\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "x[:, -1, :]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 切片赋值\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size, time, width, height, channels = 13,11,7,5,3\n",
    "video_batch = tf.zeros([batch_size, time, width, height, channels])\n",
    "\n",
    "indices = [[0],[1]]\n",
    "new_clips = tf.ones([2, time, width, height, channels])\n",
    "\n",
    "\n",
    "# tf.tensor_scatter_nd_update(video_batch, indices, new_clips)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "indices = [[0, 0], [1, 0], [2, 0]] # num_updates=3, index_depth=2\n",
    "new_images = tf.ones([\n",
    "  # num_updates=3, inner_shape=(width, height, channels)\n",
    "  3, width, height, channels])\n",
    "# tf.tensor_scatter_nd_update(video_batch, indices, new_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_batch = tf.constant(2)\n",
    "\n",
    "target_length = 3\n",
    "\n",
    "n_vocab_target = 4 \n",
    "\n",
    "\n",
    "probs = tf.zeros((int(target_length), int(N_batch), n_vocab_target))\n",
    "\n",
    "indices = [[1]]\n",
    "\n",
    "new_clips = tf.ones([1, N_batch, n_vocab_target])\n",
    "\n",
    "probs_new =  tf.tensor_scatter_nd_update(probs, indices, new_clips)\n",
    "\n",
    "probs_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# my_var = my_var[4:8].assign(tf.zeros(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### one hot 相互转换"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = [[0, 2, 0], \n",
    "           [1, 0, 1]]\n",
    "depth = 3\n",
    "\n",
    "one_hot = tf.one_hot(indices, depth,\n",
    "           on_value=1, off_value=0,  dtype=tf.int64,\n",
    "           axis=-1)\n",
    "\n",
    "one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dense = tf.argmax(one_hot, axis=-1)\n",
    "\n",
    "dense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tf.where"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.where([True, False, False, True], [1,2,3,4], [100,200,300,400])\n",
    "\n",
    "\n",
    "tf.where([True, False, False, True], [1,2,3,4], [100])\n",
    "\n",
    "\n",
    "tf.where([True, False, False, True], [1,2,3,4], 100)\n",
    "\n",
    "\n",
    "tf.where([True, False, False, True], 1, 100)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tf.data 数据流水线 ( Dataset )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 分批次遍历 (batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Two tensors can be combined into one Dataset object.\n",
    "features = tf.constant([[1, 3], \n",
    "                        [2, 1], \n",
    "                        [3, 3]]) # ==> 3x2 tensor\n",
    "labels = tf.constant(['A', 'B', 'A']) # ==> 3x1 tensor\n",
    "dataset = Dataset.from_tensor_slices((features, labels))  # 把tensor 的第一个维度作为样本 id 维度, 按照此维度拼接样本\n",
    "\n",
    "list(dataset.as_numpy_iterator()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Both the features and the labels tensors can be converted\n",
    "# to a Dataset object separately and combined after.\n",
    "features_dataset = Dataset.from_tensor_slices(features)\n",
    "labels_dataset = Dataset.from_tensor_slices(labels)\n",
    "dataset = Dataset.zip((features_dataset, labels_dataset))\n",
    "\n",
    "list(dataset.as_numpy_iterator())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# A batched feature and label set can be converted to a Dataset\n",
    "# in similar fashion.\n",
    "batched_features = tf.constant([[[1, 3], \n",
    "                                 [2, 3]],\n",
    "                                [[2, 1], \n",
    "                                 [1, 2]],\n",
    "                                [[3, 3], \n",
    "                                 [3, 2]]], shape=(3, 2, 2))\n",
    "batched_labels = tf.constant([['A', 'A'],\n",
    "                              ['B', 'B'],\n",
    "                              ['A', 'B']], shape=(3, 2))\n",
    "dataset = Dataset.from_tensor_slices((batched_features, batched_labels))\n",
    "\n",
    "dataset_batch2 = dataset.batch(2)\n",
    "\n",
    "for i,element in enumerate(dataset):\n",
    "    \n",
    "    print(i)\n",
    "    \n",
    "    print(element[0])\n",
    "    \n",
    "#     print(element[1].shape)\n",
    "    \n",
    "    print(element[1].numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,element in enumerate(dataset_batch2):\n",
    "    \n",
    "    print(i)\n",
    "    print(element)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dataset_batch1 = dataset.batch(1)\n",
    "\n",
    "for i,element in enumerate(dataset_batch1):\n",
    "    \n",
    "    print(i)\n",
    "    \n",
    "    print(element)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_unbatch = dataset_batch2.unbatch()\n",
    "\n",
    "for i,element in enumerate(dataset_unbatch):\n",
    "    \n",
    "    print(i)\n",
    "    \n",
    "    print(element)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label2 = tf.constant([[1],\n",
    "                      [2],\n",
    "                      [3]])\n",
    "\n",
    "\n",
    "for feature,label,label2 in zip(batched_features, batched_labels, label2):\n",
    "    \n",
    "    print(feature)\n",
    "    \n",
    "    print(label)\n",
    "    \n",
    "    print(label2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 分批次映射 (map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = tf.constant( [\n",
    "                        [1, 3, 2, 3],\n",
    "                        [2, 1, 1, 2],\n",
    "                        [3, 3, 3, 2],\n",
    "                        [0, 0, 0, 2]\n",
    "                        ], shape=(4, 4))\n",
    "\n",
    "dataset = Dataset.from_tensor_slices(features).batch(2) # 后面每一次遍历的单位都是 batch \n",
    "\n",
    "source_in = dataset.map(lambda batch_text: batch_text[:, ::-1], num_parallel_calls=tf.data.AUTOTUNE)\n",
    "\n",
    "target_out = dataset.map(lambda batch_text: batch_text[:, 1:], num_parallel_calls=tf.data.AUTOTUNE)\n",
    "\n",
    "target_in = dataset.map(lambda batch_text: batch_text[:, :-1], num_parallel_calls=tf.data.AUTOTUNE)\n",
    "\n",
    "\n",
    "print('source_in')\n",
    "\n",
    "for ele in source_in:\n",
    "    \n",
    "    print(ele)\n",
    "\n",
    "\n",
    "print('target_out')\n",
    "\n",
    "for ele in target_out:\n",
    "    \n",
    "    print(ele)\n",
    "\n",
    "print('target_in')\n",
    "    \n",
    "for ele in target_in:\n",
    "    \n",
    "    print(ele)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 访问列 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices([[1,2,3,4], \n",
    "                                              [5,6,7,8]])\n",
    "dataset_filter = dataset.map(lambda x: tf.gather(x, [0, 2], axis=0))\n",
    "result = list(dataset_filter.as_numpy_iterator())\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "features = tf.constant( [\n",
    "                        [1, 3, 2, 3],\n",
    "                        [2, 1, 1, 2],\n",
    "                        [3, 3, 3, 2],\n",
    "                        [0, 0, 0, 2]\n",
    "                        ], shape=(4, 4))\n",
    "\n",
    "labels = tf.constant(['A', 'B', 'A', 'A'])\n",
    "\n",
    "dataset = Dataset.from_tensor_slices((features, labels)).batch(2)\n",
    "\n",
    "slice0 = dataset.map(lambda batch_feature, batch_label : batch_feature)\n",
    "\n",
    "slice1 = dataset.map(lambda batch_feature, batch_label : batch_label)\n",
    "\n",
    "for ele in slice0:\n",
    "    \n",
    "    print(ele)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 按照序列长度划分 batch\n",
    "\n",
    "ref:\n",
    "\n",
    "[1] https://www.tensorflow.org/api_docs/python/tf/data/Dataset#bucket_by_sequence_length\n",
    "\n",
    "[2] https://www.jianshu.com/p/2e83d11c7ae6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import six\n",
    "\n",
    "def example_length(example):\n",
    "  length = 0\n",
    "  # Length of the example is the maximum length of the feature lengths\n",
    "  for _, v in sorted(six.iteritems(example)):\n",
    "    # For images the sequence length is the size of the spatial dimensions.\n",
    "    feature_length = tf.shape(v)[0]\n",
    "    if len(v.get_shape()) > 2:\n",
    "      feature_length = tf.shape(v)[0] * tf.shape(v)[1]\n",
    "    length = tf.maximum(length, feature_length)\n",
    "  return length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _bucket_boundaries(max_length, min_length=8, length_bucket_step=1.1):\n",
    "  \"\"\"A default set of length-bucket boundaries.\"\"\"\n",
    "  assert length_bucket_step > 1.0\n",
    "  x = min_length\n",
    "  boundaries = []\n",
    "  while x < max_length:\n",
    "    boundaries.append(x)\n",
    "    x = max(x + 1, int(x * length_bucket_step))\n",
    "  return boundaries\n",
    "\n",
    "\n",
    "def batching_scheme(batch_size,\n",
    "                    max_length,\n",
    "                    min_length_bucket,\n",
    "                    length_bucket_step,\n",
    "                    drop_long_sequences=False,\n",
    "                    shard_multiplier=1,\n",
    "                    length_multiplier=1,\n",
    "                    min_length=0):\n",
    "  \"\"\"A batching scheme based on model hyperparameters.\n",
    "\n",
    "  Every batch contains a number of sequences divisible by `shard_multiplier`.\n",
    "\n",
    "  Args:\n",
    "    batch_size: int, total number of tokens in a batch.\n",
    "    max_length: int, sequences longer than this will be skipped. Defaults to\n",
    "      batch_size.\n",
    "    min_length_bucket: int\n",
    "    length_bucket_step: float greater than 1.0\n",
    "    drop_long_sequences: bool, if True, then sequences longer than\n",
    "      `max_length` are dropped.  This prevents generating batches with\n",
    "      more than the usual number of tokens, which can cause out-of-memory\n",
    "      errors.\n",
    "    shard_multiplier: an integer increasing the batch_size to suit splitting\n",
    "      across datashards.\n",
    "    length_multiplier: an integer multiplier that is used to increase the\n",
    "      batch sizes and sequence length tolerance.\n",
    "    min_length: int, sequences shorter than this will be skipped.\n",
    "\n",
    "  Returns:\n",
    "     A dictionary with parameters that can be passed to input_pipeline:\n",
    "       * boundaries: list of bucket boundaries\n",
    "       * batch_sizes: list of batch sizes for each length bucket\n",
    "       * max_length: int, maximum length of an example\n",
    "\n",
    "  Raises:\n",
    "    ValueError: If min_length > max_length\n",
    "  \"\"\"\n",
    "  max_length = max_length or batch_size\n",
    "  if max_length < min_length:\n",
    "    raise ValueError(\"max_length must be greater or equal to min_length\")\n",
    "\n",
    "  boundaries = _bucket_boundaries(max_length, min_length_bucket,\n",
    "                                  length_bucket_step)\n",
    "  boundaries = [boundary * length_multiplier for boundary in boundaries]\n",
    "  max_length *= length_multiplier\n",
    "\n",
    "  batch_sizes = [\n",
    "      max(1, batch_size // length) for length in boundaries + [max_length]\n",
    "  ]\n",
    "  max_batch_size = max(batch_sizes)\n",
    "  # Since the Datasets API only allows a single constant for window_size,\n",
    "  # and it needs divide all bucket_batch_sizes, we pick a highly-composite\n",
    "  # window size and then round down all batch sizes to divisors of that window\n",
    "  # size, so that a window can always be divided evenly into batches.\n",
    "  # TODO(noam): remove this when Dataset API improves.\n",
    "  highly_composite_numbers = [\n",
    "      1, 2, 4, 6, 12, 24, 36, 48, 60, 120, 180, 240, 360, 720, 840, 1260, 1680,\n",
    "      2520, 5040, 7560, 10080, 15120, 20160, 25200, 27720, 45360, 50400, 55440,\n",
    "      83160, 110880, 166320, 221760, 277200, 332640, 498960, 554400, 665280,\n",
    "      720720, 1081080, 1441440, 2162160, 2882880, 3603600, 4324320, 6486480,\n",
    "      7207200, 8648640, 10810800, 14414400, 17297280, 21621600, 32432400,\n",
    "      36756720, 43243200, 61261200, 73513440, 110270160\n",
    "  ]\n",
    "  window_size = max(\n",
    "      [i for i in highly_composite_numbers if i <= 3 * max_batch_size])\n",
    "  divisors = [i for i in range(1, window_size + 1) if window_size % i == 0]\n",
    "  batch_sizes = [max([d for d in divisors if d <= bs]) for bs in batch_sizes]\n",
    "  window_size *= shard_multiplier\n",
    "  batch_sizes = [bs * shard_multiplier for bs in batch_sizes]\n",
    "  # The Datasets API splits one window into multiple batches, which\n",
    "  # produces runs of many consecutive batches of the same size.  This\n",
    "  # is bad for training.  To solve this, we will shuffle the batches\n",
    "  # using a queue which must be several times as large as the maximum\n",
    "  # number of batches per window.\n",
    "  max_batches_per_window = window_size // min(batch_sizes)\n",
    "  shuffle_queue_size = max_batches_per_window * 3\n",
    "\n",
    "  ret = {\n",
    "      \"boundaries\": boundaries,\n",
    "      \"batch_sizes\": batch_sizes,\n",
    "      \"min_length\": min_length,\n",
    "      \"max_length\": (max_length if drop_long_sequences else 10**9),\n",
    "      \"shuffle_queue_size\": shuffle_queue_size,\n",
    "  }\n",
    "  return ret\n",
    "\n",
    "batch_size = 4096\n",
    "max_length = 256\n",
    "min_length_bucket= 8\n",
    "length_bucket_step= 1.1\n",
    "\n",
    "rt = batching_scheme(batch_size,max_length,min_length_bucket, length_bucket_step)\n",
    "\n",
    "rt['boundaries']\n",
    "rt['batch_sizes']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 从生成器(from_generator)中构建"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elements = [\n",
    "  [0], \n",
    "  [1, 2, 3, 4], \n",
    "  [5, 6, 7],\n",
    "  [7, 8, 9, 10, 11], \n",
    "  [13, 14, 15, 16, 19, 20], \n",
    "  [21, 22]]\n",
    "\n",
    "dataset = tf.data.Dataset.from_generator(\n",
    "    lambda: elements, tf.int32, output_shapes=[None])\n",
    "\n",
    "for batch in dataset:\n",
    "    print(batch)\n",
    "\n",
    "# dataset = dataset.bucket_by_sequence_length(\n",
    "#         element_length_func=lambda elem: tf.shape(elem)[0],\n",
    "#         bucket_boundaries=[3, 5],\n",
    "#         bucket_batch_sizes=[2, 2, 2]) # bucket_batch_sizes 定义了各个 batch 的比例大小(不是真实大小)\n",
    "\n",
    "dataset = dataset.bucket_by_sequence_length(\n",
    "        element_length_func=lambda elem: tf.shape(elem)[0],\n",
    "        bucket_boundaries=[3, 5],\n",
    "        bucket_batch_sizes=[4, 4, 4]) # bucket_batch_sizes 定义了各个 batch 的比例大小(不是真实大小)\n",
    "\n",
    "for batch in dataset:\n",
    "    print(batch)\n",
    "\n",
    "\n",
    "# tf.Tensor(\n",
    "# [[1 2 3 4]\n",
    "#  [5 6 7 0]], shape=(2, 4), dtype=int64)\n",
    "\n",
    "# tf.Tensor(\n",
    "# [[ 7  8  9 10 11  0]\n",
    "#  [13 14 15 16 19 20]], shape=(2, 6), dtype=int64)\n",
    "\n",
    "# tf.Tensor(\n",
    "# [[ 0  0]\n",
    "#  [21 22]], shape=(2, 2), dtype=int64)\n",
    "    \n",
    "# bucket_boundaries=[3, 5] : batch 中的序列的长度分别为 2, 4, 6\n",
    "# 2 满足第一个区间的长度要求 (小于3)\n",
    "# 4 满足第二个区间的长度要求 (大于等于3 小于5)\n",
    "# 6 满足第二个区间的长度要求 (大于等于5 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elements1 = [\n",
    "  [0], \n",
    "  [1, 2, 3, 4], \n",
    "  [5, 6, 7],\n",
    "  [7, 8, 9, 10, 11], \n",
    "  [13, 14, 15, 16, 19, 20], \n",
    "  [21, 22]\n",
    "]\n",
    "\n",
    "elements2 = [\n",
    "  [0,99], \n",
    "  [1, 2, 3, 4], \n",
    "  [5, 6, 7],\n",
    "  [7, 8, 9, 10, 11], \n",
    "  [13, 14, 15, 16, 19, 20], \n",
    "  [21, 22]\n",
    "]\n",
    "\n",
    "dataset1 = tf.data.Dataset.from_generator(\n",
    "    lambda: elements1, tf.int32, output_shapes=[None])\n",
    "\n",
    "dataset2 = tf.data.Dataset.from_generator(\n",
    "    lambda: elements2, tf.int32, output_shapes=[None])\n",
    "\n",
    "\n",
    "dataset = tf.data.Dataset.zip((dataset1, dataset2))\n",
    "\n",
    "\n",
    "dataset = dataset.bucket_by_sequence_length(\n",
    "        element_length_func=lambda ele1, ele2: tf.shape(ele1)[0],\n",
    "        bucket_boundaries=[3, 5],\n",
    "        bucket_batch_sizes=[2, 2, 2])\n",
    "\n",
    "# dataset = dataset.bucket_by_sequence_length(\n",
    "#         element_length_func=lambda ele1, ele2: tf.shape(ele1)[0],\n",
    "#         bucket_boundaries= rt['boundaries'],\n",
    "#         bucket_batch_sizes= rt['batch_sizes'])\n",
    "\n",
    "\n",
    "for batch in dataset:\n",
    "    print(batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset 转换为生成器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elements1 = [\n",
    "  [0], \n",
    "  [1, 2, 3, 4], \n",
    "  [5, 6, 7],\n",
    "  [7, 8, 9, 10, 11], \n",
    "  [13, 14, 15, 16, 19, 20], \n",
    "  [21, 22]\n",
    "]\n",
    "\n",
    "elements2 = [\n",
    "  [0,99], \n",
    "  [1, 2], \n",
    "  [5, 6, 7],\n",
    "  [7, 8, 9, 10, 11, 12, 13], \n",
    "  [13, 14, 15, 16], \n",
    "  [21, 22, 23]\n",
    "]\n",
    "\n",
    "ragged1 = tf.ragged.constant(elements1)\n",
    "ragged2 = tf.ragged.constant(elements2)\n",
    "\n",
    "dataset1 = Dataset.from_tensor_slices(ragged1)\n",
    "dataset2 = Dataset.from_tensor_slices(ragged2)\n",
    "\n",
    "dataset = tf.data.Dataset.zip((dataset1, dataset2))\n",
    "\n",
    "list(dataset)[0]\n",
    "\n",
    "\n",
    "dataset = dataset.map(lambda x,y: (x,y)) # 转换为生成器\n",
    "\n",
    "dataset_backet = dataset.bucket_by_sequence_length(\n",
    "        element_length_func=lambda x,y: tf.cast(tf.maximum(tf.shape(x)[0],tf.shape(y)[0]), tf.int32),\n",
    "        bucket_boundaries=[3, 5],\n",
    "        bucket_batch_sizes=[4, 4, 4],\n",
    "        )\n",
    "\n",
    "# bucket_fn = tf.data.experimental.bucket_by_sequence_length(\n",
    "#     lambda x,y: tf.cast(tf.shape(x)[0], tf.int32), \n",
    "#     bucket_boundaries=[3, 5], \n",
    "#     bucket_batch_sizes=[1, 4, 4], \n",
    "# )\n",
    "\n",
    "# dataset_backet = dataset.map(lambda x,y: (x,y)).apply(bucket_fn)\n",
    "\n",
    "\n",
    "for data in dataset_backet:\n",
    "    \n",
    "    print(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_corpus_data( corpus_file_dir):\n",
    "    \"\"\"\n",
    "    读取 图片描述文本, 并将它们和对应的图片进行映射\n",
    "\n",
    "    :param corpus_file_dir: 语料文本\n",
    "\n",
    "    :return:\n",
    "\n",
    "    text_data: 所有句子的列表\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    with open(corpus_file_dir, encoding='utf-8') as corpus_file:\n",
    "\n",
    "        lines = corpus_file.readlines()\n",
    "        text_data = []\n",
    "\n",
    "        for line in lines:\n",
    "#             sentence = line.rstrip(\"\\n\")\n",
    "\n",
    "            sentence = line.strip()\n",
    "\n",
    "            sentence = '[START] ' + sentence + ' [END]'\n",
    "\n",
    "            text_data.append(sentence)\n",
    "\n",
    "        return text_data\n",
    "    \n",
    "corpus = load_corpus_data('dataset/newstest2012.de')\n",
    "corpus_dataset = Dataset.from_tensor_slices(corpus).batch(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "corpus_dataset_row = corpus_dataset.unbatch()\n",
    "\n",
    "list(corpus_dataset_row)[0]\n",
    "\n",
    "list(corpus_dataset_row)[0].numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_tokenizer = SpaceTokenizer(corpus) \n",
    "\n",
    "corpus_vector = corpus_dataset.map(lambda batch_text: model_tokenizer.tokenize(batch_text))\n",
    "\n",
    "list(corpus_vector)[0]\n",
    "\n",
    "corpus_vector_row = corpus_vector.unbatch()\n",
    "\n",
    "list(corpus_vector_row)[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.shape(list(corpus_vector_row)[0])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# corpus_vector_backet = corpus_vector_row.bucket_by_sequence_length(\n",
    "#         element_length_func=lambda ele1: tf.shape(ele1)[0],\n",
    "#         bucket_boundaries= rt['boundaries'],\n",
    "#         bucket_batch_sizes= rt['batch_sizes'])\n",
    "\n",
    "bucket_fn = tf.data.experimental.bucket_by_sequence_length(\n",
    "    lambda x: tf.cast(tf.shape(x)[0], tf.int32), \n",
    "    bucket_boundaries=rt['boundaries'], \n",
    "    bucket_batch_sizes=rt['batch_sizes'], \n",
    ")\n",
    "\n",
    "# Apply bucketing\n",
    "corpus_vector_backet = corpus_vector_row.map(lambda x: x).apply(bucket_fn)\n",
    "\n",
    "\n",
    "for batch in corpus_vector_backet.take(3):\n",
    "    print(batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 混洗(shuffle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.range(3)\n",
    "dataset = dataset.shuffle(3)\n",
    "list(dataset.as_numpy_iterator())\n",
    "# [1, 0, 2]\n",
    "list(dataset.as_numpy_iterator())\n",
    "# [1, 2, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.range(3)\n",
    "dataset = dataset.shuffle(3, reshuffle_each_iteration=False)\n",
    "list(dataset.as_numpy_iterator())\n",
    "# [1, 0, 2]\n",
    "list(dataset.as_numpy_iterator())\n",
    "# [1, 0, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = tf.constant( [\n",
    "                        [1, 3, 2, 3,1,1,1],\n",
    "                        [2, 1, 1, 2,0,0,0],\n",
    "    \n",
    "                        [3, 3, 3, 2,2],\n",
    "                        [0, 0, 0, 2,0],\n",
    "    \n",
    "                        [1, 3, 2, 3,3,3],\n",
    "                        [2, 1, 1, 2,2,3],\n",
    "    \n",
    "                        [3, 3, 3, 2],\n",
    "                        [0, 0, 0, 2]\n",
    "    \n",
    "                        ], shape=(8, None))\n",
    "\n",
    "dataset = Dataset.from_tensor_slices(features).shuffle(4).batch(2)  \n",
    "\n",
    "\n",
    "\n",
    "for i,ele in enumerate(dataset):\n",
    "    \n",
    "    print(i)\n",
    "    \n",
    "    print(ele)\n",
    "\n",
    "for i,ele in enumerate(dataset):\n",
    "    \n",
    "    print(i)\n",
    "    \n",
    "    print(ele)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = tf.constant( [\n",
    "                        [1, 3, 2, 3],\n",
    "                        [2, 1, 1, 2],\n",
    "                        [3, 3, 3, 2],\n",
    "                        [0, 0, 0, 2]\n",
    "                        ], shape=(4, 4))\n",
    "\n",
    "dataset = Dataset.from_tensor_slices(features).shuffle(4).batch(2) \n",
    "\n",
    "target_out = dataset.map(lambda batch_text: batch_text[:, 1:], num_parallel_calls=tf.data.AUTOTUNE)\n",
    "\n",
    "target_in = dataset.map(lambda batch_text: batch_text[:, :-1], num_parallel_calls=tf.data.AUTOTUNE)\n",
    "\n",
    "print('target_out')\n",
    "# 每次遍历都会打乱顺序\n",
    "for ele in target_out:\n",
    "    \n",
    "    print(ele)\n",
    "\n",
    "print('target_in')\n",
    "    \n",
    "for ele in target_in:\n",
    "    \n",
    "    print(ele)\n",
    "\n",
    "    \n",
    "# target_out\n",
    "# tf.Tensor(\n",
    "# [[3 2 3]\n",
    "#  [0 0 2]], shape=(2, 3), dtype=int32)\n",
    "\n",
    "# target_in\n",
    "# tf.Tensor(\n",
    "# [[2 1 1]\n",
    "#  [0 0 0]], shape=(2, 3), dtype=int32)\n",
    "\n",
    "# 元素不再一一对应"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "features = tf.constant( [\n",
    "                        [1, 3, 2, 3],\n",
    "                        [2, 1, 1, 2],\n",
    "                        [3, 3, 3, 2],\n",
    "                        [0, 0, 0, 2]\n",
    "                        ], shape=(4, 4))\n",
    "\n",
    "labels = tf.constant(['A', 'B', 'C', 'D'])\n",
    "\n",
    "dataset = Dataset.from_tensor_slices((features, labels)).batch(2)\n",
    "\n",
    "dataset = dataset.unbatch().shuffle(4).batch(2)\n",
    "\n",
    "\n",
    "for ele in dataset:\n",
    "    \n",
    "    print(ele)\n",
    "\n",
    "\n",
    "for ele in dataset:\n",
    "    \n",
    "    print(ele)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 两个数据集连接"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 5, 6, 7]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "TypeError",
     "evalue": "Incompatible types of input datasets: <dtype: 'int64'> vs. (tf.int64, tf.int64).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-6de9b7c1484f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;31m# compatible element specs.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[0ma\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py\u001b[0m in \u001b[0;36mconcatenate\u001b[1;34m(self, dataset, name)\u001b[0m\n\u001b[0;32m   1244\u001b[0m       \u001b[0mDataset\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mA\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1245\u001b[0m     \"\"\"\n\u001b[1;32m-> 1246\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mConcatenateDataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1247\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1248\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mprefetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbuffer_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, input_dataset, dataset_to_concatenate, name)\u001b[0m\n\u001b[0;32m   4843\u001b[0m     \u001b[0moutput_types\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_legacy_output_types\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_dataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4844\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0moutput_types\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mget_legacy_output_types\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset_to_concatenate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4845\u001b[1;33m       raise TypeError(f\"Incompatible types of input datasets: {output_types} \"\n\u001b[0m\u001b[0;32m   4846\u001b[0m                       f\"vs. {get_legacy_output_types(dataset_to_concatenate)}.\")\n\u001b[0;32m   4847\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: Incompatible types of input datasets: <dtype: 'int64'> vs. (tf.int64, tf.int64)."
     ]
    }
   ],
   "source": [
    "a = tf.data.Dataset.range(1, 4)  # ==> [ 1, 2, 3 ]\n",
    "b = tf.data.Dataset.range(4, 8)  # ==> [ 4, 5, 6, 7 ]\n",
    "ds = a.concatenate(b)\n",
    "list(ds.as_numpy_iterator())\n",
    "\n",
    "# The input dataset and dataset to be concatenated should have\n",
    "# compatible element specs.\n",
    "c = tf.data.Dataset.zip((a, b))\n",
    "a.concatenate(c)\n",
    "\n",
    "\n",
    "\n",
    "d = tf.data.Dataset.from_tensor_slices([\"a\", \"b\", \"c\"])\n",
    "a.concatenate(d)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  使用 tensorflow_text 文本预处理工具 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 字符串 (Strings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_utf8 = tf.constant(u\"中\")\n",
    "\n",
    "text_utf8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unicode 编码\n",
    "hex(ord('中'))\n",
    "\n",
    "# utf-8 编码\n",
    "'中'.encode('utf-8')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_utf8 = tf.constant(\"语言处理\")\n",
    "\n",
    "text_utf8\n",
    "\n",
    "text_utf8.numpy().decode('utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 正则表达式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ord('„') # unicode 编码\n",
    "\n",
    "ord('“')\n",
    "\n",
    "chr(8220)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 匹配 unicode 范围\n",
    "\n",
    "**Ref**\n",
    "\n",
    "[1] https://segmentfault.com/a/1190000021141670\n",
    "\n",
    "[2] https://github.com/google/re2/wiki/Syntax#perl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "[^**]\t表示不匹配此字符集中的任何一个字符\n",
    "\\u4e00-\\u9fa5\t汉字的unicode范围\n",
    "\\u0030-\\u0039\t数字的unicode范围\n",
    "\\u0041-\\u005a\t大写字母unicode范围\n",
    "\\u0061-\\u007a\t小写字母unicode范围\n",
    "\\u0021-\\u007e   英文字母unicode范围(含数字与符号)\n",
    "\\u00C0-\\u00FF   德文/法文的unicode范围\n",
    "\\uAC00-\\uD7AF\t韩文的unicode范围\n",
    "\\u3040-\\u31FF\t日文的unicode范围\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text =' a b c , d . , '\n",
    "\n",
    "remove_unk = r'[^[:alnum:]]' \n",
    "# [[:alnum:]]  (≡ [0-9A-Za-z] )\n",
    "\n",
    "text = tf.strings.regex_replace(text, remove_unk, ' ')\n",
    "\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re \n",
    "\n",
    "text = ['我 Ämter Aushärtung für Autostraßen Öfen , abc-10 Pro 50,813 words and  “ format articles „ .']\n",
    "\n",
    "\n",
    "# 删除在各个语系外的字符\n",
    "remove_unk =   r'[^\\p{Latin}|[:print:]]' \n",
    "\n",
    "# \\p{Han} 匹配中文语系\n",
    "# \\p{Latin} 匹配拉丁语系\n",
    "# [[:alnum:]]  (≡ [0-9A-Za-z] )\n",
    "# [[:print:]]  (≡ [A-Za-z0-9!\"#$%&'()*+,\\-./:;<=>?@[\\\\\\]^_`{|}~] )\n",
    "\n",
    "# ref\n",
    "# [1] https://segmentfault.com/a/1190000021141670\n",
    "# [2] https://github.com/google/re2/wiki/Syntax#perl\n",
    "\n",
    "\n",
    "text = tf.strings.regex_replace(text, remove_unk, ' ')\n",
    "\n",
    "\n",
    "print(text[0])\n",
    "\n",
    "print(text[0].numpy().decode('utf-8'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 匹配独立出现的数字"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = '11 999 avc 10 abc-10 10-abc abc10 50 813 aa 4x4 20'\n",
    "\n",
    "\n",
    "#  A=( \\d+) 空格+至少1个数字 为匹配字符A ,\n",
    "#  B='A+ ' 至少1个A + 空格 为匹配字符B\n",
    "# remove_digits = r'( \\d+)+ '\n",
    "\n",
    "# 考虑数字可能出现在句首\n",
    "# remove_digits = r'^(\\d+ )+|( \\d+)+ '\n",
    "\n",
    "# 考虑数字可能出现在句首和句末 \n",
    "remove_digits = r'^(\\d+ )+|( \\d+)+ |(\\d+)$'\n",
    "\n",
    "text = tf.strings.regex_replace(text, remove_digits, ' ')\n",
    "\n",
    "print(text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 去除控制字符"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' A group of race horses [UNK] [UNK]  run down a track carrying jockeys  '"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import re \n",
    "\n",
    "_null_str=''\n",
    "_start_str=re.escape('[START]') \n",
    "_end_str=re.escape('[END]')\n",
    "_unk_str=re.escape('[UNK]')\n",
    "\n",
    "\n",
    "remove_word_re = re.compile(\n",
    "            r'{}|{}|{}'.format(_null_str, _start_str, _end_str))\n",
    "\n",
    "\n",
    "text = '[START] A group of race horses [UNK] [UNK]  run down a track carrying jockeys  [END]'\n",
    "\n",
    "text_clean = remove_word_re.sub('', text)\n",
    "\n",
    "text_clean\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_word_re = re.compile(\n",
    "            r'{}|{}|{}|{}'.format(_null_str, _start_str, _end_str, _unk_str))\n",
    "\n",
    "text_clean = remove_word_re.sub('', text)\n",
    "\n",
    "\n",
    "text_clean.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text2 = '[UNK] und [UNK] [UNK] immer noch [UNK] [END]'\n",
    "\n",
    "text2_clean = remove_word_re.sub('', text2)\n",
    "\n",
    "text2_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 去除匹配子串中的空格"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import re \n",
    "\n",
    "dash_str = '# # AT # # - # # AT # #' # 破折号\n",
    "double_quotation_str = '& quot ;' # 双引号\n",
    "quotation_str = '& apos ;' # 单引号\n",
    "\n",
    "\n",
    "dash_re = re.compile(r'{}'.format(re.escape(dash_str)))\n",
    "\n",
    "double_quotation_re = re.compile(r'{}'.format(re.escape(double_quotation_str)))\n",
    "\n",
    "quotation_re = re.compile(r'{}'.format(re.escape(quotation_str)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'EU ##AT##-##AT## Botschafterin &quot; Ich wurde stets &quot; &apos; Obani '"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = 'EU # # AT # # - # # AT # # Botschafterin & quot ; Ich wurde stets & quot ; & apos ; Obani '\n",
    "\n",
    "text = dash_re.sub(dash_str.replace(' ', ''), text)\n",
    "\n",
    "text = double_quotation_re.sub(double_quotation_str.replace(' ', ''), text)\n",
    "\n",
    "text = quotation_re.sub(quotation_str.replace(' ', ''), text)\n",
    "\n",
    "text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 文本标准化\n",
    "\n",
    "ref: \n",
    "\n",
    "[1]https://www.tensorflow.org/text/api_docs/python/text/normalize_utf8\n",
    "\n",
    "[2] https://www.elastic.co/guide/cn/elasticsearch/guide/current/unicode-normalization.html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### NFC 和 NFD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "\n",
    "\n",
    "s1='\\u00f1'                                                            \n",
    " \n",
    "s2='n\\u0303'                                                            \n",
    " \n",
    "print(s1)\n",
    "print(s2)\n",
    "\n",
    "s1 == s2\n",
    "                                                              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a= b'schl\\xc3\\xa4gt'\n",
    "\n",
    "print(a.decode('utf-8'))\n",
    "\n",
    "\n",
    "b = 'schlägt'\n",
    "print(b)\n",
    "\n",
    "a==b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**第一种 \\u00f1 为整体表示法 (NFC)**\n",
    "\n",
    "**第二种 n\\u0303 为组合表示法，是n和字符~的组合字符 (NFD)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = unicodedata.normalize('NFC', s1)\n",
    "t2 = unicodedata.normalize('NFC', s2)\n",
    "\n",
    "t1.encode('raw_unicode_escape')\n",
    "\n",
    "t2.encode('raw_unicode_escape')\n",
    "\n",
    "t1==t2  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t3 = unicodedata.normalize('NFD', s1)\n",
    "t4 = unicodedata.normalize('NFD', s2)\n",
    "\n",
    "t3.encode('raw_unicode_escape')\n",
    "\n",
    "t4.encode('raw_unicode_escape')\n",
    "\n",
    "t3==t4  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(t1)\n",
    "print(t3)\n",
    "\n",
    "t1==t3\n",
    "\n",
    "t1.encode('utf-8')\n",
    "\n",
    "t3.encode('utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 'Aushärtung für Autostraßen Öfen nämlich'\n",
    "\n",
    "print(a.encode('utf-8'))\n",
    "\n",
    "normal_nfkd = tf_text.normalize_utf8(a, 'NFKD')\n",
    "\n",
    "print('nfkd: ',normal_nfkd)\n",
    "\n",
    "normal_nfkc = tf_text.normalize_utf8(a, 'NFKC')\n",
    "\n",
    "print('nfkc:', normal_nfkc)\n",
    "\n",
    "\n",
    "print(normal_nfkd.numpy().decode('utf-8'))\n",
    "\n",
    "# b = 'Weißkopfseeadler WEISSKOPFSEEADLER'\n",
    "\n",
    "# normal_b = tf_text.normalize_utf8(b, 'NFKD')\n",
    "\n",
    "# print(normal_b.numpy().decode('utf-8'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [\"株式会社\", \"ＫＡＤＯＫＡＷＡ\"]\n",
    "\n",
    "a = \"ＫＡＤＯＫＡＷＡ\"\n",
    "\n",
    "print(a.encode('utf-8'))\n",
    "\n",
    "normal_a = tf_text.normalize_utf8(a)\n",
    "\n",
    "print(normal_a)\n",
    "\n",
    "print(normal_a.numpy().decode('utf-8'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 小写化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = ['Ämter Aushärtung für Autostraßen Öfen Weißkopfseeadler WEISSKOPFSEEADLER ',\n",
    "        'Dadurch sind die Hiesigen eine Sorge los - nämlich die Vertreibung der Kreuzfahrer ']\n",
    "\n",
    "text = tf_text.normalize_utf8(text, 'NFKD')\n",
    "\n",
    "# text = tf_text.normalize_utf8(text, 'NFKC')\n",
    "\n",
    "text = tf.strings.lower(text)\n",
    "\n",
    "text\n",
    "\n",
    "text.numpy()[0].decode('utf-8')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 大小写折叠\n",
    "\n",
    "lower函数只支持ascill表中的字符，而casefold则支持很多不同种类的语言。\n",
    "比如说β，lower 只能显示出原形而casefold则能显示他的小写--ss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1='Autostraßen'\n",
    "\n",
    "s1.lower()\n",
    "\n",
    "s1.casefold()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = ['Ämter Aushärtung für Autostraßen Öfen Weißkopfseeadler WEISSKOPFSEEADLER ']\n",
    "\n",
    "text = tf_text.case_fold_utf8(text)\n",
    "\n",
    "text\n",
    "\n",
    "text.numpy()[0].decode('utf-8')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1='\\u00f1'                                                            \n",
    " \n",
    "s2='n\\u0303'                                                            \n",
    " \n",
    "print(s1)\n",
    "print(s2)\n",
    "\n",
    "print(s1.encode('utf-8'))\n",
    "\n",
    "print(s2.encode('utf-8'))\n",
    "\n",
    "text = ['{} - {}'.format(s1, s2)]\n",
    "\n",
    "text = tf_text.case_fold_utf8(text)\n",
    "\n",
    "text\n",
    "\n",
    "# text.numpy()[0].decode('utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 最终结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re \n",
    "import string\n",
    "\n",
    "text = '„ &apos 我 English &quot ##AT##-##AT## abc10 abc-10 4x4  ,, Weißkopfseeadler Ämter Aushärtung für Autostraßen Öfen ,  50,813 words “   .'\n",
    "\n",
    "\n",
    "# 删除单词的规则\n",
    "remove_words = r'(##AT##-##AT##|&apos|&quot)'\n",
    "\n",
    "# 删除在各个语系外的字符\n",
    "remove_unk =   r'[^\\p{Latin}|[:print:]]' \n",
    "\n",
    "# \\p{Latin} 匹配拉丁语系\n",
    "# [[:print:]]  (≡ [A-Za-z0-9!\"#$%&'()*+,\\-./:;<=>?@[\\\\\\]^_`{|}~] )\n",
    "\n",
    "\n",
    "# 需要删除的标点符号, 排除 \"'\" 和 \"-\"\n",
    "punctuation = string.punctuation\n",
    "punctuation = punctuation.replace(\"'\", \"\")  # 英语中有', 不删除 '\n",
    "punctuation = punctuation.replace(\"-\", \"\")  #\n",
    "remove_punc = r'[%s]' % re.escape(punctuation)\n",
    "\n",
    "# 删除单独的数字字符\n",
    "# eg. 'avc 10 abc-10 abc10' -> 'avc  abc-10 abc10'\n",
    "\n",
    "remove_digits = r'^(\\d+ )+|( \\d+)+ |(\\d+)$'\n",
    "\n",
    "\n",
    "# 删除在各个语系外的字符\n",
    "text = tf.strings.regex_replace(text, remove_unk, '')\n",
    "\n",
    "# 清除指定单词\n",
    "text = tf.strings.regex_replace(text, remove_words, '')\n",
    "\n",
    "# NKFC unicode 标准化 +  大小写折叠\n",
    "text = tf_text.case_fold_utf8(text)\n",
    "\n",
    "# 清除句子中的标点符号\n",
    "text = tf.strings.regex_replace(text, remove_punc, ' ')  # 空1格\n",
    "\n",
    "# 清除句子中的独立数字\n",
    "text = tf.strings.regex_replace(text, remove_digits, '')  \n",
    "\n",
    "# 清除左右端点的空格\n",
    "text = tf.strings.strip(text)\n",
    "\n",
    "print(text)\n",
    "\n",
    "print(text.numpy().decode('utf-8'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 分词器"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TextVectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_corpus_data( corpus_file_dir):\n",
    "    \"\"\"\n",
    "    读取 图片描述文本, 并将它们和对应的图片进行映射\n",
    "\n",
    "    :param corpus_file_dir: 语料文本\n",
    "\n",
    "    :return:\n",
    "\n",
    "    text_data: 所有句子的列表\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    with open(corpus_file_dir, encoding='utf-8') as corpus_file:\n",
    "\n",
    "        lines = corpus_file.readlines()\n",
    "        text_data = []\n",
    "\n",
    "        for line in lines:\n",
    "#             sentence = line.rstrip(\"\\n\")\n",
    "\n",
    "            sentence = line.strip()\n",
    "\n",
    "            sentence = '[START] ' + sentence + ' [END]'\n",
    "\n",
    "            text_data.append(sentence)\n",
    "\n",
    "        return text_data\n",
    "    \n",
    "corpus = load_corpus_data('dataset/newstest2012.de')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def statistic_seq_length( seq_list):\n",
    "    \"\"\"\n",
    "    统计序列的长度\n",
    "\n",
    "    :param seq_list:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "\n",
    "    seq_length = {}\n",
    "\n",
    "    for i, seq in enumerate(seq_list):\n",
    "\n",
    "        seq_arr = seq.split(\" \")\n",
    "\n",
    "        seq_length[i] = len(seq_arr)\n",
    "\n",
    "    return seq_length\n",
    "\n",
    "seq_length = statistic_seq_length(corpus)\n",
    "# seq_length = { 句子标号: 句子的长度,  }\n",
    "\n",
    "seq_length_counter =  Counter(seq_length.values())\n",
    "# seq_length_counter = { 句子的长度: 该长度句子的个数 }\n",
    "\n",
    "# seq_length_counter.most_common(10)\n",
    "\n",
    "# sorted(seq_length_counter.items(), key=lambda d: d[0], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "corpus = tf_text.normalize_utf8(corpus)\n",
    "\n",
    "corpus = tf.strings.lower(corpus)\n",
    "\n",
    "\n",
    "n_vocab = 20000\n",
    "\n",
    "tokenizer = TextVectorization(\n",
    "            standardize=None,\n",
    "            output_sequence_length=50,\n",
    "            max_tokens=n_vocab)\n",
    "\n",
    "tokenizer.adapt(corpus)\n",
    "\n",
    "vocab_list = tokenizer.get_vocabulary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_list[:10]\n",
    "\n",
    "# vocab_list.index('Öfen')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text = ['Keine befreiende Novelle für Tymoshenko durch das Parlament']\n",
    "\n",
    "text = ['Keine befreiende Novelle für Tymoshenko durch das Parlament']\n",
    "\n",
    "text = tf.strings.lower(text)\n",
    "\n",
    "tokenizer(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_dataset = Dataset.from_tensor_slices(corpus).batch(2)\n",
    "\n",
    "\n",
    "corpus_vector = corpus_dataset.map(lambda batch_text: tokenizer.call(batch_text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(corpus_vector)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class SpaceTokenizer(tf.keras.Model):\n",
    "    \"\"\"\n",
    "    使用 TextVectorization 构建的空格分词器\n",
    "\n",
    "    Author: xrh\n",
    "    Date: 2021-12-15\n",
    "\n",
    "    ref:\n",
    "    https://www.tensorflow.org/text/guide/subwords_tokenizer\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, corpus):\n",
    "        \"\"\"\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        super(SpaceTokenizer, self).__init__(self)\n",
    "        \n",
    "        self.tokenizer = TextVectorization(\n",
    "            standardize=None,\n",
    "            output_sequence_length=50,\n",
    "            max_tokens=20000)\n",
    "\n",
    "        self.tokenizer.adapt(corpus)\n",
    "\n",
    "        self.vocab_list = self.tokenizer.get_vocabulary()\n",
    "        \n",
    "\n",
    "    # @tf.function\n",
    "    @tf.function(input_signature=[tf.TensorSpec(dtype=tf.string, shape=[None])])\n",
    "    def tokenize(self, strings):\n",
    "        \"\"\"\n",
    "        返回序列长度(self.max_seq_length)为固定的 tensor\n",
    "\n",
    "        :param strings:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        enc = self.tokenizer.call(strings)\n",
    "        \n",
    "        ragged = tf.RaggedTensor.from_tensor(enc, padding=0)\n",
    "\n",
    "        return ragged\n",
    "\n",
    "model_tokenizer = SpaceTokenizer(corpus) \n",
    "\n",
    "corpus_vector = corpus_dataset.map(lambda batch_text: model_tokenizer.tokenize(batch_text))\n",
    "\n",
    "list(corpus_vector)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_path = os.path.join('dataset', 'tokenizer_source_model')\n",
    "\n",
    "tf.saved_model.save(model_tokenizer, tokenizer_path)\n",
    "\n",
    "model_tokenizer = tf.saved_model.load(tokenizer_path)\n",
    "\n",
    "corpus_vector = corpus_dataset.map(lambda batch_text: model_tokenizer.tokenize(batch_text))\n",
    "\n",
    "list(corpus_vector)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  StringLookup (字典)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "token_string_from_index = StringLookup(\n",
    "            vocabulary=vocab_list,\n",
    "            mask_token='',\n",
    "            invert=True)\n",
    "\n",
    "index_seq = [\n",
    "    [115, 12805,  4027,    30,  6743,    79,    14,  1013],\n",
    "    [115, 12805,  4027,    30,  6743,    79,    14]\n",
    "]\n",
    "\n",
    "result_text_tokens = token_string_from_index(index_seq)\n",
    "\n",
    "\n",
    "\n",
    "result_text = tf.strings.reduce_join(result_text_tokens, axis=1,\n",
    "                                    separator=' ')\n",
    "\n",
    "[text.numpy().decode('utf-8') for text in result_text]\n",
    "\n",
    "\n",
    "# print(result_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_from_token_string = StringLookup(\n",
    "            vocabulary=vocab_list,\n",
    "            mask_token='',\n",
    "            )\n",
    "\n",
    "token_string_seq = ['keine', 'befreiende','novelle', 'für','tymoshenko', 'durch', 'das', 'parlament']\n",
    "\n",
    "index_from_token_string(token_string_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_index = index_from_token_string(tf.constant('[start]'))\n",
    "\n",
    "start_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices(corpus).batch(256)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(dataset.as_numpy_iterator())[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "seq = list(dataset.as_numpy_iterator())[0][0]\n",
    "\n",
    "seq_vector = index_from_token_string(seq.decode('utf-8').split(' '))\n",
    "\n",
    "\n",
    "seq_vector\n",
    "\n",
    "# seq_vector_pad = tf.keras.preprocessing.sequence.pad_sequences(seq_vector, padding='post')\n",
    "\n",
    "# seq_vector_pad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Subword tokenizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### WordPiece( by TF_text )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_corpus_data( corpus_file_dir):\n",
    "    \"\"\"\n",
    "    读取 图片描述文本, 并将它们和对应的图片进行映射\n",
    "\n",
    "    :param corpus_file_dir: 语料文本\n",
    "\n",
    "    :return:\n",
    "\n",
    "    text_data: 所有句子的列表\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    with open(corpus_file_dir, encoding='utf-8') as corpus_file:\n",
    "\n",
    "        lines = corpus_file.readlines()\n",
    "        text_data = []\n",
    "\n",
    "        for line in lines:\n",
    "\n",
    "            sentence = line.strip()\n",
    "\n",
    "#             sentence = '[START] ' + sentence + ' [END]'\n",
    "\n",
    "            text_data.append(sentence)\n",
    "\n",
    "        return text_data\n",
    "    \n",
    "target_corpus = load_corpus_data('dataset/newstest2012.de')\n",
    "\n",
    "target_dataset = Dataset.from_tensor_slices(target_corpus).batch(16)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_corpus[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow_text.tools.wordpiece_vocab import bert_vocab_from_dataset as bert_vocab\n",
    "\n",
    "\n",
    "bert_tokenizer_params=dict(lower_case=False)\n",
    "reserved_tokens=[\"\", \"[UNK]\", \"[START]\", \"[END]\", \"##AT##-##AT##\", \"&quot;\", \"&apos;\"]\n",
    "\n",
    "bert_vocab_args = dict(\n",
    "    # The target vocabulary size\n",
    "    vocab_size = 5000,\n",
    "    # Reserved tokens that must be included in the vocabulary\n",
    "    reserved_tokens=reserved_tokens,\n",
    "    # Arguments for `text.BertTokenizer`\n",
    "    bert_tokenizer_params=bert_tokenizer_params,\n",
    "    # Arguments for `wordpiece_vocab.wordpiece_tokenizer_learner_lib.learn`\n",
    "    learn_params={},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "target_vocab = bert_vocab.bert_vocab_from_dataset(\n",
    "    target_dataset.prefetch(buffer_size=tf.data.AUTOTUNE),\n",
    "    **bert_vocab_args\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(target_vocab[:10])\n",
    "print(target_vocab[100:110])\n",
    "print(target_vocab[1000:1010])\n",
    "print(target_vocab[-10:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_vocab_file(filepath, vocab):\n",
    "  with open(filepath, 'w', encoding='utf-8') as f:\n",
    "    for token in vocab:\n",
    "      print(token, file=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_vocab_file('target_vocab.txt', target_vocab)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# target_tokenizer = tf_text.BertTokenizer('target_vocab.txt', **bert_tokenizer_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch_seq in target_dataset.unbatch().batch(2).take(1):\n",
    "    \n",
    "    print(batch_seq.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the examples -> (batch, word, word-piece)\n",
    "token_batch = target_tokenizer.tokenize(batch_seq)\n",
    "\n",
    "# for ex in token_batch.to_list():\n",
    "#   print(ex)\n",
    "\n",
    "# Merge the word and word-piece axes -> (batch, tokens)\n",
    "token_merge_batch = token_batch.merge_dims(-2,-1)\n",
    "\n",
    "print('token_merge_batch:')\n",
    "\n",
    "print(token_merge_batch)\n",
    "\n",
    "# for ex in token_merge_batch.to_list():\n",
    "#   print(ex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lookup each token id in the vocabulary.\n",
    "txt_tokens = tf.gather(target_vocab, token_merge_batch)\n",
    "# Join with spaces.\n",
    "tf.strings.reduce_join(txt_tokens, separator=' ', axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = target_tokenizer.detokenize(token_merge_batch)\n",
    "tf.strings.reduce_join(words, separator=' ', axis=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Tokenizer 的持久化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SubwordTokenizer(tf.keras.Model):\n",
    "    \"\"\"\n",
    "    使用 tensorflow_text.BertTokenizer 构建的基于 subword 的分词器\n",
    "\n",
    "    Author: xrh\n",
    "    Date: 2021-12-15\n",
    "\n",
    "    ref:\n",
    "    https://www.tensorflow.org/text/guide/subwords_tokenizer\n",
    "    https://www.tensorflow.org/text/api_docs/python/text/BertTokenizer\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, bert_tokenizer_params, fixed_seq_length, reserved_tokens, vocab_list, _start_str, _end_str):\n",
    "        \"\"\"\n",
    "\n",
    "        :param bert_tokenizer_params: bert_tokenizer 的参数(字典形式)\n",
    "        :param fixed_seq_length: 指定的序列长度\n",
    "        :param reserved_tokens: 保留的控制字符,\n",
    "                [\"[NULL]\", \"[UNK]\", \"[START]\", \"[END]\"]\n",
    "        :param vocab_list: 词典\n",
    "        :param _start_str: 句子的开始\n",
    "        :param _end_str: 句子的结束\n",
    "        \"\"\"\n",
    "\n",
    "        super(SubwordTokenizer, self).__init__(self)\n",
    "\n",
    "        self._reserved_tokens = reserved_tokens\n",
    "        self.fixed_seq_length = fixed_seq_length\n",
    "        self.start = tf.constant(reserved_tokens.index(_start_str), dtype=tf.int64)\n",
    "        self.end = tf.constant(reserved_tokens.index(_end_str), dtype=tf.int64)\n",
    "\n",
    "        lookup = tf.lookup.StaticVocabularyTable(\n",
    "            num_oov_buckets=1,\n",
    "            initializer=tf.lookup.KeyValueTensorInitializer(\n",
    "                keys=vocab_list,\n",
    "                values=tf.range(len(vocab_list), dtype=tf.int64)))\n",
    "\n",
    "        # bert_tokenizer_params = dict(lower_case=False)\n",
    "\n",
    "        self.tokenizer = tf_text.BertTokenizer(lookup, **bert_tokenizer_params)\n",
    "\n",
    "        self.vocab = tf.Variable(vocab_list)\n",
    "\n",
    "        # Include a tokenize_fixed signature for a batch of strings.\n",
    "        self.tokenize_fixed.get_concrete_function(\n",
    "            tf.TensorSpec(shape=[None], dtype=tf.string))\n",
    "\n",
    "        # Include a tokenize signature for a batch of strings.\n",
    "        self.tokenize.get_concrete_function(\n",
    "            tf.TensorSpec(shape=[None], dtype=tf.string))\n",
    "\n",
    "        # Include `detokenize` and `lookup` signatures for:\n",
    "        #   * `Tensors` with shapes [tokens] and [batch, tokens]\n",
    "        #   * `RaggedTensors` with shape [batch, tokens]\n",
    "        self.detokenize.get_concrete_function(\n",
    "            tf.TensorSpec(shape=[None, None], dtype=tf.int64))\n",
    "\n",
    "        self.detokenize.get_concrete_function(\n",
    "            tf.RaggedTensorSpec(shape=[None, None], dtype=tf.int64))\n",
    "\n",
    "        self.lookup.get_concrete_function(\n",
    "            tf.TensorSpec(shape=[None, None], dtype=tf.int64))\n",
    "\n",
    "        self.lookup.get_concrete_function(\n",
    "            tf.RaggedTensorSpec(shape=[None, None], dtype=tf.int64))\n",
    "\n",
    "    def add_start_end(self, batch_ragged):\n",
    "        \"\"\"\n",
    "        在句子的首尾添加控制字符\n",
    "\n",
    "        :param batch_ragged:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        N_batch = batch_ragged.bounding_shape()[0]\n",
    "        starts = tf.fill([N_batch, 1], self.start)\n",
    "        ends = tf.fill([N_batch, 1], self.end)\n",
    "\n",
    "        return tf.concat([starts, batch_ragged, ends], axis=1)\n",
    "\n",
    "    # @tf.function(input_signature=[tf.TensorSpec(dtype=tf.string, shape=[None]), tf.TensorSpec(dtype=tf.int64, shape=None)])\n",
    "    # TODO: 报错 TypeError: Dimension value must be integer or None\n",
    "    @tf.function\n",
    "    def tokenize_fixed(self, strings):\n",
    "        \"\"\"\n",
    "        返回固定序列长度(fixed_seq_length)的 tensor\n",
    "\n",
    "        :param strings:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "\n",
    "        enc = self.tokenizer.tokenize(strings)\n",
    "\n",
    "        # Merge the `word` and `word-piece` axes.\n",
    "        enc = enc.merge_dims(-2, -1)\n",
    "        enc = self.add_start_end(enc)\n",
    "\n",
    "        return enc.to_tensor(shape=[None, self.fixed_seq_length])\n",
    "\n",
    "    @tf.function\n",
    "    def tokenize(self, strings):\n",
    "        \"\"\"\n",
    "        返回不规则的 tensor(RaggedTensor)\n",
    "\n",
    "        :param strings:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "\n",
    "        enc = self.tokenizer.tokenize(strings)\n",
    "\n",
    "        # Merge the `word` and `word-piece` axes.\n",
    "        enc = enc.merge_dims(-2, -1)\n",
    "        enc = self.add_start_end(enc)\n",
    "\n",
    "        return enc\n",
    "\n",
    "    @tf.function\n",
    "    def detokenize(self, tokenized):\n",
    "        \"\"\"\n",
    "        合并所有的 subword 并返回单词列表\n",
    "\n",
    "        :param tokenized:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        words = self.tokenizer.detokenize(tokenized)\n",
    "        return words\n",
    "\n",
    "    @tf.function\n",
    "    def lookup(self, token_ids):\n",
    "        \"\"\"\n",
    "        返回 subword 的 token 列表\n",
    "\n",
    "        :param token_ids:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        return tf.gather(self.vocab, token_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reserved_tokens=[\"\", \"[UNK]\", \"[START]\", \"[END]\", \"##AT##-##AT##\", \"&quot;\", \"&apos;\"]\n",
    "\n",
    "fixed_seq_length = 50\n",
    "\n",
    "tokenizer_obj = SubwordTokenizer(bert_tokenizer_params, fixed_seq_length, reserved_tokens, target_vocab,\n",
    "                             _start_str=\"[START]\", _end_str=\"[END]\")\n",
    "\n",
    "model_name = 'target_tokenizer.bin'\n",
    "\n",
    "tf.saved_model.save(tokenizer_obj, model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tf.constant(reserved_tokens.index(\"[START]\"), dtype=tf.int64)\n",
    "\n",
    "tf.argmax(tf.constant(reserved_tokens) == \"[START]\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'target_tokenizer.bin'\n",
    "tokenizer_obj = tf.saved_model.load(model_name)\n",
    "tokenizer_obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch_seq in target_dataset.unbatch().batch(2).take(1):\n",
    "    \n",
    "    print(batch_seq.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_length = tf.constant(50)\n",
    "\n",
    "print(max_seq_length)\n",
    "\n",
    "token_batch = tokenizer_obj.tokenize_fixed(batch_seq)\n",
    "\n",
    "token_batch\n",
    "\n",
    "# token_batch.to_tensor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_seq=['Die CDU schlägt vor , den EU ##AT##-##AT## Präsidenten aufgrund allgemeiner Stimmabgabe zu wählen',\n",
    "          '&quot; Ethisches Gebot &quot;'\n",
    "          ]\n",
    "\n",
    "token_batch = tokenizer_obj.tokenize(batch_seq)\n",
    "\n",
    "token_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = tokenizer_obj.detokenize(token_batch)\n",
    "\n",
    "# words\n",
    "\n",
    "texts = tf.strings.reduce_join(words, separator=' ', axis=-1).numpy()\n",
    "\n",
    "texts\n",
    "\n",
    "texts_utf8 = [sentence.decode('utf-8') for sentence in texts]\n",
    "\n",
    "texts_utf8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = tokenizer_obj.lookup(token_batch)\n",
    "\n",
    "words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### bug: reserved token  不 work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_list = [\n",
    "    # reserved tokens\n",
    "    b\"[UNK]\", \"[MASK]\", b\"[RANDOM]\", b\"[CLS]\", b\"[SEP]\",\n",
    "    # Suffixes\n",
    "    b\"Sponge\", b\"&quot;\",\n",
    "    b\"##ack\", b\"##ama\", b\"##ger\", b\"##gers\", b\"##onge\", b\"##pants\",  b\"##uare\",\n",
    "    b\"##vel\", b\"##ven\", b\"an\", b\"A\", b\"Bar\", b\"Hates\", b\"Mar\", b\"Ob\",\n",
    "    b\"Patrick\", b\"President\", b\"Sp\", b\"Sq\", b\"bob\", b\"box\", b\"has\", b\"highest\",\n",
    "    b\"is\", b\"office\", b\"the\",\n",
    "]\n",
    "# TODO: vocab_list 中, 若 token 的前后含有符号则在 tokenize 时无法识别( eg. &quot; , [MASK]), 原因未知 \n",
    "\n",
    "lookup_table = tf.lookup.StaticVocabularyTable(\n",
    "    tf.lookup.KeyValueTensorInitializer(\n",
    "      keys=vocab_list,\n",
    "      key_dtype=tf.string,\n",
    "      values=tf.range(\n",
    "          tf.size(vocab_list, out_type=tf.int64), dtype=tf.int64),\n",
    "      value_dtype=tf.int64),\n",
    "      num_oov_buckets=1\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\"[MASK] Sponge &quot; bob Squarepants [CLS] is an Avenger\"]\n",
    "\n",
    "bert_tokenizer_params=dict(lower_case=False)\n",
    "\n",
    "bert_tokenizer = tf_text.BertTokenizer(vocab_lookup_table=lookup_table, \n",
    "                                    token_out_type=tf.int64, **bert_tokenizer_params\n",
    "                                   )\n",
    "\n",
    "segments = bert_tokenizer.tokenize(sentences)\n",
    "\n",
    "# print(segments)\n",
    "\n",
    "segments = segments.merge_dims(-2, -1)\n",
    "\n",
    "print(segments)\n",
    "\n",
    "\n",
    "detoken = bert_tokenizer.detokenize(segments)\n",
    "sentence = tf.strings.reduce_join(detoken, separator=' ', axis=-1)\n",
    "\n",
    "print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "bert_tokenizer_string = tf_text.BertTokenizer(\n",
    "                             vocab_lookup_table=lookup_table, \n",
    "                             token_out_type=tf.string,\n",
    "                             preserve_unused_token= True\n",
    "                            )\n",
    "\n",
    "segments = bert_tokenizer_string.tokenize(sentences)\n",
    "\n",
    "print(segments)\n",
    "\n",
    "segments = segments.merge_dims(-2, -1)\n",
    "\n",
    "print(segments)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BPE ( by Hugging)\n",
    "\n",
    " Byte Pair Encoding\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer\n",
    "\n",
    "from tokenizers import ByteLevelBPETokenizer\n",
    "\n",
    "from tokenizers.models import BPE\n",
    "\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "\n",
    "from tokenizers import decoders\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### BPE\n",
    "\n",
    " **ref:**\n",
    " \n",
    " https://huggingface.co/docs/tokenizers/python/v0.10.0/quicktour.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tokenizer = Tokenizer(BPE(unk_token=\"[UNK]\"))\n",
    "\n",
    "reserved_tokens=[\"\", \"[UNK]\", \"[START]\", \"[END]\", \"##AT##-##AT##\", \"&quot;\", \"&apos;\"]\n",
    "\n",
    "trainer = BpeTrainer(special_tokens=reserved_tokens, vocab_size=5000)\n",
    "\n",
    "tokenizer.pre_tokenizer = Whitespace()\n",
    "\n",
    "files = ['dataset/newstest2012.de']\n",
    "\n",
    "tokenizer.train(files, trainer)\n",
    "\n",
    "tokenizer.save(\"dataset/cache data/tokenizer-de.json\")\n",
    "\n",
    "tokenizer = Tokenizer.from_file(\"dataset/cache data/tokenizer-de.json\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ByteLevelBPETokenizer\n",
    "\n",
    "字节级 BPE 将所有 Unicode 码位转换为多个字节级字符：\n",
    "\n",
    "1.每个 Unicode 码位分解为字节（ASCII 字符为 1 个字节，UTF-8 字符最多为 4 个字节）\n",
    "\n",
    "2.每个字节的值都会从 Unicode 表的开头获取 一个\"可见\"字符。这一点尤其重要，因为有很多控制字符，因此我们不能只使用简单的映射： ASCII 字符<->字节值。\n",
    "因此，某些字符看起来就不一样了，例如空格 U+0020 变为 Ġ。\n",
    "\n",
    "目的是，通过这样做，你最终会得到一个包含256个 token 的初始字母表。\n",
    "然后，这 256 个标记可以合并在一起，以表示词汇表中的任何其他token。这么做使得词汇表较小，并且不需要标记unk的 token。\n",
    "\n",
    "**ref:**\n",
    "\n",
    "[1] https://github.com/huggingface/tokenizers/issues/203\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = ['dataset/newstest2012.de']\n",
    "\n",
    "reserved_tokens=[\"\", \"[UNK]\", \"[START]\", \"[END]\", \"##AT##-##AT##\", \"&quot;\", \"&apos;\"]\n",
    "\n",
    "tokenizer = ByteLevelBPETokenizer(lowercase=False)\n",
    "\n",
    "tokenizer.train(files=files, vocab_size=5000, special_tokens=reserved_tokens)\n",
    "\n",
    "tokenizer.save(\"dataset/cache data/tokenizer-de.json\")\n",
    "\n",
    "tokenizer = Tokenizer.from_file(\"dataset/cache data/tokenizer-de.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "batch_seq=['Die CDU schlägt vor , den EU ##AT##-##AT## Präsidenten aufgrund allgemeiner Stimmabgabe zu wählen',\n",
    "          ' [START] &quot; Ethisches Gebot &quot; [END]'\n",
    "          ]\n",
    "\n",
    "for seq in batch_seq:\n",
    "\n",
    "    output = tokenizer.encode(seq)\n",
    "\n",
    "    print(output.tokens)\n",
    "    print(output.ids)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "encode_list = tokenizer.encode_batch(batch_seq)\n",
    "\n",
    "for ele in encode_list:\n",
    "    \n",
    "    print(ele.tokens)\n",
    "    print(ele.ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "decode1=tokenizer.decode(encode_list[0].ids, skip_special_tokens=False)\n",
    "\n",
    "print(decode1)\n",
    "\n",
    "decode2=tokenizer.decode(encode_list[1].ids, skip_special_tokens=False)\n",
    "\n",
    "print(decode2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized = [\n",
    "    [414, 424, 42, 59, 417, 870, 626, 452, 269, 340, 1511, 227, 4, 2351, 2319, 3782, 276, 264, 3429, 443, 1641, 327, 278, 4208],\n",
    "    [227, 2, 227, 5, 2570, 78, 1855, 1048, 294, 227, 5, 227, 3]\n",
    "]\n",
    "\n",
    "\n",
    "tokenizer.decode_batch(tokenized, skip_special_tokens=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 与 tf.dataset 结合"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_dataset = target_dataset.unbatch().batch(2)\n",
    "\n",
    "for i,element in enumerate(target_dataset.take(1)):\n",
    "    \n",
    "    print(element.numpy())\n",
    "\n",
    "    \n",
    "# target_dataset.map(lambda target: tokenizer.encode_batch(target) )\n",
    "                                      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BPETokenizer(tf.keras.Model):\n",
    "    \"\"\"\n",
    "    使用  ByteLevelBPETokenizer 构建的基于 subword 的分词器\n",
    "\n",
    "    Author: xrh\n",
    "    Date: 2021-12-15\n",
    "\n",
    "    ref:\n",
    "    https://huggingface.co/docs/tokenizers/python/v0.10.0/pipeline.html\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, corups_file, reserved_tokens, vocab_size, _start_str, _end_str):\n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "        :param reserved_tokens: 保留的控制字符,\n",
    "                [\"[NULL]\", \"[UNK]\", \"[START]\", \"[END]\"]\n",
    "        :param vocab_size: 词典大小\n",
    "        :param _start_str: 句子的开始\n",
    "        :param _end_str: 句子的结束\n",
    "        \"\"\"\n",
    "\n",
    "        super(BPETokenizer, self).__init__(self)\n",
    "\n",
    "        self._reserved_tokens = reserved_tokens\n",
    "        \n",
    "        self.start = tf.constant(reserved_tokens.index(_start_str), dtype=tf.int64)\n",
    "        self.end = tf.constant(reserved_tokens.index(_end_str), dtype=tf.int64)\n",
    "\n",
    "        self.tokenizer = ByteLevelBPETokenizer(lowercase=False)\n",
    "\n",
    "        self.tokenizer.train(files=corups_file, vocab_size=vocab_size, special_tokens=reserved_tokens)\n",
    "\n",
    "\n",
    "#         self.tokenize.get_concrete_function(\n",
    "#             tf.TensorSpec(shape=[None], dtype=tf.string))\n",
    "\n",
    "\n",
    "#         self.detokenize.get_concrete_function(\n",
    "#             tf.TensorSpec(shape=[None, None], dtype=tf.int64))\n",
    "\n",
    "#         self.detokenize.get_concrete_function(\n",
    "#             tf.RaggedTensorSpec(shape=[None, None], dtype=tf.int64))\n",
    "\n",
    "\n",
    "    def add_start_end(self, batch_ragged):\n",
    "        \"\"\"\n",
    "        在句子的首尾添加控制字符\n",
    "\n",
    "        :param batch_ragged:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        N_batch = batch_ragged.bounding_shape()[0]\n",
    "        starts = tf.fill([N_batch, 1], self.start)\n",
    "        ends = tf.fill([N_batch, 1], self.end)\n",
    "\n",
    "        return tf.concat([starts, batch_ragged, ends], axis=1)\n",
    "\n",
    "\n",
    "#     @tf.function\n",
    "    def tokenize(self, strings):\n",
    "        \"\"\"\n",
    "        返回不规则的 tensor(RaggedTensor)\n",
    "\n",
    "        :param strings:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        strings = [ sentence.decode('utf-8') for sentence in strings.numpy() ] \n",
    "        \n",
    "        print(strings)\n",
    "        \n",
    "        encode_list = self.tokenizer.encode_batch(strings)\n",
    "        \n",
    "        ragged = tf.ragged.constant([ enc.ids for enc in encode_list])\n",
    "#         enc = self.add_start_end(enc)\n",
    "\n",
    "        return ragged\n",
    "\n",
    "#     @tf.function\n",
    "    def detokenize(self, tokenized):\n",
    "        \"\"\"\n",
    "        合并所有的 subword 并返回单词列表\n",
    "\n",
    "        :param tokenized:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        \n",
    "        tokenized = tokenized.to_list()\n",
    "        \n",
    "        sentence_list = self.tokenizer.decode_batch(tokenized, skip_special_tokens=False)\n",
    "        \n",
    "        words = [sentence.split() for sentence in sentence_list]\n",
    "        \n",
    "        return words\n",
    "\n",
    "\n",
    "\n",
    "files = ['dataset/newstest2012.de']\n",
    "\n",
    "reserved_tokens=[\"\", \"[UNK]\", \"[START]\", \"[END]\", \"##AT##-##AT##\", \"&quot;\", \"&apos;\"]\n",
    "\n",
    "    \n",
    "tokenizer_obj = BPETokenizer(corups_file=files, reserved_tokens=reserved_tokens, vocab_size=5000, _start_str=\"[START]\", _end_str=\"[END]\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_seq=tf.constant(\n",
    "         ['Die CDU schlägt vor , den EU ##AT##-##AT## Präsidenten aufgrund allgemeiner Stimmabgabe zu wählen',\n",
    "          ' [START] &quot; Ethisches Gebot &quot; [END]'\n",
    "          ])\n",
    "\n",
    "vectors = tokenizer_obj.tokenize(batch_seq)\n",
    "\n",
    "vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = tokenizer_obj.detokenize(vectors)\n",
    "\n",
    "words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 原理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import re, collections\n",
    "\n",
    "def get_stats(vocab):\n",
    "    pairs = collections.defaultdict(int)\n",
    "    for word, freq in vocab.items():\n",
    "        symbols = word.split()\n",
    "        for i in range(len(symbols)-1):\n",
    "            pairs[symbols[i],symbols[i+1]] += freq\n",
    "    return pairs\n",
    "\n",
    "def merge_vocab(pair, v_in):\n",
    "    v_out = {}\n",
    "    bigram = re.escape(' '.join(pair))\n",
    "    p = re.compile(r'(?<!\\S)' + bigram + r'(?!\\S)')\n",
    "    for word in v_in:\n",
    "        w_out = p.sub(''.join(pair), word)\n",
    "        v_out[w_out] = v_in[word]\n",
    "    return v_out\n",
    "\n",
    "vocab = {'l o w </w>': 5, 'l o w e r </w>': 2, 'n e w e s t </w>': 6, 'w i d e s t </w>': 3}\n",
    "num_merges = 1000\n",
    "for i in range(num_merges):\n",
    "    pairs = get_stats(vocab)\n",
    "    if not pairs:\n",
    "        break\n",
    "    best = max(pairs, key=pairs.get)\n",
    "    vocab = merge_vocab(best, vocab)\n",
    "    print(best)\n",
    "    \n",
    "    print(vocab)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### WordPiece(by Hugging)\n",
    "\n",
    "**ref:**\n",
    "\n",
    "[1] https://huggingface.co/docs/tokenizers/python/v0.10.0/pipeline.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import WordPiece\n",
    "from tokenizers import normalizers\n",
    "from tokenizers.normalizers import Lowercase, NFD, StripAccents\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "from tokenizers.trainers import WordPieceTrainer\n",
    "from tokenizers import decoders\n",
    "\n",
    "wordpiece_tokenizer = Tokenizer(WordPiece(unk_token=\"[UNK]\"))\n",
    "\n",
    "wordpiece_tokenizer.normalizer = normalizers.Sequence([NFD(), Lowercase(), StripAccents()])\n",
    "\n",
    "wordpiece_tokenizer.pre_tokenizer = Whitespace()\n",
    "\n",
    "reserved_tokens=[\"\", \"[UNK]\", \"[START]\", \"[END]\", \"##AT##-##AT##\", \"&quot;\", \"&apos;\"]\n",
    "\n",
    "\n",
    "trainer = WordPieceTrainer(\n",
    "    vocab_size=5000, special_tokens=reserved_tokens\n",
    ")\n",
    "\n",
    "files = ['dataset/newstest2012.de']\n",
    "\n",
    "wordpiece_tokenizer.train(files, trainer)\n",
    "\n",
    "wordpiece_tokenizer.save(\"dataset/cache data/tokenizer-de.json\")\n",
    "\n",
    "wordpiece_tokenizer = Tokenizer.from_file(\"dataset/cache data/tokenizer-de.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "batch_seq=['Die CDU schlägt vor , den EU ##AT##-##AT## Präsidenten aufgrund allgemeiner Stimmabgabe zu wählen',\n",
    "          ' [START] &quot; Ethisches Gebot &quot; [END]'\n",
    "          ]\n",
    "\n",
    "for seq in batch_seq:\n",
    "\n",
    "    res_encode = wordpiece_tokenizer.encode(seq)\n",
    "\n",
    "    print(res_encode.tokens)\n",
    "    print(res_encode.ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "encode_list = wordpiece_tokenizer.encode_batch(batch_seq)\n",
    "\n",
    "for ele in encode_list:\n",
    "    \n",
    "    print(ele.tokens)\n",
    "    print(ele.ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "wordpiece_tokenizer.decoder = decoders.WordPiece()\n",
    "\n",
    "wordpiece_tokenizer.decode(ele.ids, skip_special_tokens=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_vec = [\n",
    "    [114, 36, 562, 1620, 76, 207, 15, 159, 1078, 4, 1992, 1669, 2418, 107, 1956, 233, 1412, 146, 2104],\n",
    "    [2, 5, 1815, 83, 1541, 567, 128, 5, 3]\n",
    "]\n",
    "\n",
    "wordpiece_tokenizer.decode_batch(batch_vec, skip_special_tokens=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 不规则 Tensor(RaggedTensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# b=[[1, 1, 1, 1, 1],\n",
    "#    [2, 2, 2, 2]\n",
    "# ]\n",
    "\n",
    "# b_tensor = tf.constant(b)\n",
    "\n",
    "# print(b_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 和 tensor 的转换"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "batch_text = tf.RaggedTensor.from_row_splits(\n",
    "      values=[3, 1, 4, 1, 5, 9, 2, 6],\n",
    "      row_splits=[0, 4, 4, 7, 8, 8])\n",
    "\n",
    "batch_text\n",
    "\n",
    "batch_text.to_tensor()\n",
    "\n",
    "# target_out = batch_text.map(lambda text: text[1:])\n",
    "\n",
    "# target_in = batch_text.map(lambda text: text[ :-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "digits = tf.ragged.constant([[3, 1, 4, 1], [], [5, 9, 2], [6], []])\n",
    "\n",
    "tensor = digits.to_tensor(shape=[None, None])\n",
    "\n",
    "tensor\n",
    "\n",
    "tensor = digits.to_tensor(shape=[None, 10])\n",
    "\n",
    "tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.RaggedTensor.from_tensor(tensor, padding=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 维度合并 (merge_dims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rt = tf.ragged.constant([[[1, 2], [3]], \n",
    "                         [[4, 5, 6]]]\n",
    "                       )\n",
    "print(rt.merge_dims(0, 1))\n",
    "\n",
    "print(rt.merge_dims(1, 2))\n",
    "\n",
    "print(rt.merge_dims(0, 2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 操作每一个元素"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rt = tf.ragged.constant([[1, 2, 3], [], [4, 5], [6]])\n",
    "\n",
    "tf.ragged.map_flat_values(tf.ones_like, rt)\n",
    "\n",
    "tf.ragged.map_flat_values(tf.multiply, rt, rt)\n",
    "\n",
    "tf.ragged.map_flat_values(tf.add, rt, 5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.map_fn(fn=lambda t: tf.range(t, t + 3), elems=tf.constant([3, 5, 2]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 建立模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BRsjcOnGHjl2"
   },
   "source": [
    "## 序列（Sequential）建模"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Amyc6YZlo9fT"
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add( Flatten(input_shape=[28,28]) )\n",
    "model.add( Dense(100, activation='relu') )\n",
    "model.add( Dense(10, activation='softmax') )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 269
    },
    "colab_type": "code",
    "id": "4DOLKEcTpYaI",
    "outputId": "5de9a825-f54e-4831-dd9f-da05944c958c"
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "h0ZEb_u_PgoP"
   },
   "outputs": [],
   "source": [
    "model = Sequential( [Flatten( input_shape=[28, 28] ),\n",
    "                     Dense( 100, activation='relu' ),\n",
    "                     Dense( 10, activation='softmax' )\n",
    "                    ] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 269
    },
    "colab_type": "code",
    "id": "LK6gW-GaTzTJ",
    "outputId": "af00aa82-c022-4d60-bab5-b019e97531d0"
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "B4KWvGAQEnvX",
    "outputId": "5e5758e0-ffa0-4fdf-e5b8-79fecfb06ac5"
   },
   "outputs": [],
   "source": [
    "model.layers[1].name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CJO98w7SEu4O"
   },
   "outputs": [],
   "source": [
    "weights, biases = model.layers[1].get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 251
    },
    "colab_type": "code",
    "id": "zdEslfWsE38t",
    "outputId": "c896d775-4757-4b4c-8b05-736c989845f9"
   },
   "outputs": [],
   "source": [
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 143
    },
    "colab_type": "code",
    "id": "djMrDkQ-E721",
    "outputId": "ec128471-f164-43e0-a592-8312252d0634"
   },
   "outputs": [],
   "source": [
    "biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "colab_type": "code",
    "id": "ZqrUFJs_E8hW",
    "outputId": "afe8f8b7-7845-4356-80ea-0219e0bb4c24"
   },
   "outputs": [],
   "source": [
    "print( weights.shape )\n",
    "print( biases.shape )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JaD0S8knHthX"
   },
   "source": [
    "## 函数（Functional）建模"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 例子1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2kaO1T12Ihvi"
   },
   "outputs": [],
   "source": [
    "inputs = Input( shape=[28,28] )\n",
    "x = Flatten()( inputs )\n",
    "x = Dense(100, activation='relu')(x)\n",
    "output = Dense(10, activation='softmax')(x)\n",
    "\n",
    "basic_model = Model( inputs=[inputs], outputs=[output] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 305
    },
    "colab_type": "code",
    "id": "QzjyDf0AJC7c",
    "outputId": "f9c43df1-f049-4b28-e9a9-9c629a44f0a2"
   },
   "outputs": [],
   "source": [
    "basic_model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 例子2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def base_model(m):\n",
    "\n",
    "    # 建立模型第一层的 keras Tensor，只能是Input 或者从 别的地方传来的 tensor from keras.layer\n",
    "    X = Input(shape=(32,), name='X') # 此时 shape(None,32) ；真正训练时为 shape(m,32)\n",
    "\n",
    "    init_state = Input(shape=(32,), name='init_state')\n",
    "\n",
    "    #1.\n",
    "    # b= K.zeros((100,32))\n",
    "    # b = Input( name='input_b',tensor=K.zeros((100,32)))  #<tf.Variable 'Variable:0' shape=(100, 32) dtype=float32>\n",
    "    # 喂入模型的一个 batch的样本的数目 m 此时是未知的，因此为None ，\n",
    "    # 所以模型的第一层中，所有和 m 相关的tensor 都要使用 Input，相当于tensorflow的一个占位符，其他方法不work:\n",
    "    #\n",
    "    # 反例1:\n",
    "    # b=K.zeros((init_state.shape[0],32)) # init_state.shape[0]=None，K.zeros 的shape参数 中不能出现None\n",
    "    #\n",
    "    # 反例2:\n",
    "    # b= K.zeros((m,32)) # m 由参数传入，m=100\n",
    "    # b = Input( name='input_b',tensor=K.zeros((m,32))) # <tf.Variable 'Variable:0' shape=(100, 32) dtype=float32>\n",
    "    # 虽然成功建立的 tensor 但是和 来源于 keras.layer 的a (shape: (None,32)) 无法进行 axis=1的拼接\n",
    "    # 即报错： ValueError: A `Concatenate` layer requires inputs with matching shapes except for the concat axis. Got inputs shapes: [(None, 32), (100, 32)]\n",
    "\n",
    "    #2.\n",
    "    # 连接的所有tensor 必须来源于 keras.layer（血统纯正），否则无法生成计算图\n",
    "    # 下面两种方法都报 AttributeError: 'NoneType' object has no attribute '_inbound_nodes'\n",
    "    #M1\n",
    "    # init_state_2 = K.constant(np.zeros((1,64)), dtype='float32') \n",
    "    #M2\n",
    "    # init_state_2=K.ones((1,64))\n",
    "    \n",
    "    #M3 使用 keras.layer.Reshape 转换，但是维度不对,会自动的在全面加上一维\n",
    "    \n",
    "\n",
    "    concat = Concatenate(axis=1)([X, init_state])\n",
    "\n",
    "#     concat2= Concatenate(axis=1)([concat, b])\n",
    "#     concat2 = Concatenate(axis=0)([concat, init_state_2])\n",
    "\n",
    "    dense = Dense(1, name='dense_1')(concat)\n",
    "\n",
    "    print(dense)\n",
    "    \n",
    "    model= Model(inputs=[ X , init_state ], outputs=dense)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "M=100\n",
    "model = base_model(M)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=np.random.randint(0,10,size=[M,32])\n",
    "init_state=np.zeros((M,32))\n",
    "# b_train=K.zeros((M,32)) #ValueError: If your data is in the form of symbolic tensors, you should specify the `steps_per_epoch` argument (instead of the `batch_size` argument, because symbolic tensors are expected to produce batches of input data).\n",
    "\n",
    "x_train = [X, init_state]\n",
    "y_train=np.random.randint(low=0,high=2,size=[M,1])\n",
    "\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='sgd')\n",
    "\n",
    "batch_size=10\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=5\n",
    "          )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4nkFE1F7arq0"
   },
   "source": [
    "## 子类化（Subclassing）建模"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "官方教程\n",
    "\n",
    "https://keras.io/guides/making_new_layers_and_models_via_subclassing/\n",
    "\n",
    "\n",
    "可训练的权重变量(有状态)必须作为类的变量并且被 add_weight(), 如果嫌麻烦也可以直接在 init 中写 Keras layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 例子1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = tf.Variable([[1.], [2.]])\n",
    "x = tf.constant([[3., 4.]])\n",
    "\n",
    "tf.matmul(w, x)  # 矩阵乘法 相当于 np.dot()\n",
    "\n",
    "tf.sigmoid(w + x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(Layer):\n",
    "    \n",
    "    def __init__(self, units=32):\n",
    "        super(Linear, self).__init__()\n",
    "        self.units = units\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.w = self.add_weight(\n",
    "            shape=(input_shape[-1], self.units),\n",
    "            initializer=\"random_normal\",\n",
    "            trainable=True,\n",
    "        ) # 可训练的权重变量(有状态) 作为类的变量并且被 add_weight()\n",
    "        self.b = self.add_weight(\n",
    "            shape=(self.units,), initializer=\"random_normal\", trainable=True\n",
    "        )\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return tf.matmul(inputs, self.w) + self.b\n",
    "\n",
    "N = 2\n",
    "    \n",
    "x = tf.ones((N, 2))\n",
    "\n",
    "linear_layer = Linear(4)\n",
    "\n",
    "y = linear_layer(x)\n",
    "\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 例子2\n",
    "\n",
    "把 Keras Layer 当做类变量, 与函数建模很类似，好处是： 不用写 输入层(Input) 和 在最后定义 model 指明输入输出层\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rI1aDQMb7cso"
   },
   "outputs": [],
   "source": [
    "class Linear(Layer):  \n",
    "  \n",
    "    def __init__( self, units=32, **kwargs ):\n",
    "\n",
    "        super().__init__(**kwargs)\n",
    "        self.dense_layer = Dense(units) \n",
    "\n",
    "    def call(self, inputs):\n",
    "\n",
    "        x = Flatten()(inputs)\n",
    "        x = self.dense_layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZHx1-mvn2IfU"
   },
   "outputs": [],
   "source": [
    "model = Linear(4)\n",
    "\n",
    "model\n",
    "\n",
    "# model.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.ones((2, 2))\n",
    "linear_layer = Linear(4)\n",
    "y = linear_layer(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Layer 和 Model 的区别**\n",
    "\n",
    "\n",
    "通常，您会使用 Layer 类来定义内部计算块，并使用 Model 类来定义外部模型，即您将训练的对象。\n",
    "\n",
    "例如，在 ResNet50 模型中，您会有几个子类化 Layer 的 ResNet 块，以及一个包含整个 ResNet50 网络的 Model。\n",
    "\n",
    "Model 类具有与 Layer 相同的 API，但有如下区别：\n",
    "\n",
    "* 它会公开内置训练、评估和预测循环（model.fit()、model.evaluate()、model.predict()）。\n",
    "* 它会通过 model.layers 属性公开其内部层的列表。\n",
    "* 它会公开保存和序列化 API（save()、save_weights()…）\n",
    "\n",
    "实际上，Layer 类对应于我们在文献中所称的“层”（如“卷积层”或“循环层”）或“块”（如“ResNet 块”或“Inception 块”）。\n",
    "\n",
    "同时，Model 类对应于文献中所称的“模型”（如“深度学习模型”）或“网络”（如“深度神经网络”）。\n",
    "\n",
    "因此，如果您想知道“我应该用 Layer 类还是 Model 类？”，请问自己：我是否需要在它上面调用 fit()？我是否需要在它上面调用 save()？如果是，则使用 Model。如果不是（要么因为您的类只是更大系统中的一个块，要么因为您正在自己编写训练和保存代码），则使用 Layer。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 例子3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#   By XRH in 2019.12.28\n",
    "# 使用 keras.layer 搭建神经网络 不太灵活：\n",
    "# （1）keras.layer 提供的结构太少\n",
    "# （2）要使用 后端提供的丰富的函数 还需要使用 Lambda层进行包装，较繁琐\n",
    "# 因此，我们 使用 后端的函数来 定义属于自己的层\n",
    "\n",
    "\n",
    "class ConcatDense(Layer):\n",
    "\n",
    "    def __init__(self, output_dim, **kwargs):\n",
    "        self.output_dim = output_dim\n",
    "        super(ConcatDense, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert isinstance(input_shape, list)\n",
    "        # 为该层创建一个可训练的权重\n",
    "        self.w = self.add_weight(name='w',\n",
    "                                      shape=(input_shape[0][1]+input_shape[1][1], self.output_dim),\n",
    "                                      initializer='uniform',\n",
    "                                      trainable=True)\n",
    "        \n",
    "        self.b = self.add_weight( name='b',\n",
    "                                    shape=(self.output_dim,),\n",
    "                                        initializer='zeros',\n",
    "                                        trainable=False)\n",
    "        \n",
    "        self.input_shapes=input_shape\n",
    "        \n",
    "        super(ConcatDense, self).build(input_shape)  # 一定要在最后调用它\n",
    "\n",
    "    def call(self, inputs):\n",
    "        assert isinstance(inputs, list)\n",
    "        \n",
    "        print('input_shapes:', self.input_shapes)\n",
    "        \n",
    "        X, init_state = inputs # X shape: (None,32) , init_state shape: (None,32)\n",
    "        \n",
    "        print(X)\n",
    "        print(init_state)\n",
    "        \n",
    "        concat=K.concatenate([X,init_state],axis=1) # shape(None, 64)\n",
    "        print(concat) \n",
    "        \n",
    "        z = K.dot(concat, self.w) + self.b\n",
    "        # concat shape (None, 64), self.w shape(64, 10) -> shape (None, 10)\n",
    "        print(z)\n",
    "        \n",
    "        a = relu(z) # 激活函数\n",
    "        print(a)\n",
    "        \n",
    "        return a\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        assert isinstance(input_shape, list)\n",
    "        shape_X, shape_init_state = input_shape\n",
    "        return (shape_X[0], self.output_dim)\n",
    "\n",
    "\n",
    "    \n",
    "def test_model():\n",
    "\n",
    "    # 建立模型第一层的 keras Tensor，只能是Input 或者从 别的地方传来的 tensor from keras.layer\n",
    "    X = Input(shape=(32,), name='X') # 此时 shape(None,32) ；真正训练时为 shape(m,32)\n",
    "\n",
    "    init_state = Input(shape=(32,), name='init_state')\n",
    "\n",
    "    layer1 = ConcatDense(10, name='layer1')([ X , init_state] )\n",
    "    \n",
    "    layer2 = Dense(1, name='layer2')(layer1)\n",
    "    \n",
    "    model= Model(inputs=[ X , init_state ], outputs=layer2)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "model = test_model()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "N=10 # 10 个样本\n",
    "\n",
    "X=np.random.randint(0,10,size=[N,32])\n",
    "init_state=np.zeros((N,32))\n",
    "\n",
    "x_train = [X, init_state]\n",
    "y_train=np.random.randint(low=0,high=2,size=[N,1]) # 取值范围 [0, 2)\n",
    "\n",
    "# y_train\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='sgd')\n",
    "\n",
    "batch_size=10\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=5\n",
    "          )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 模型调试"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tensor 的直接计算"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Session\n",
    "\n",
    "tensoflow 1.x  采用静态图的方法，需要 使用会话来 run 计算图，其才会执行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.compat.v1 as tf1\n",
    "import numpy as np \n",
    "\n",
    "tf1.disable_v2_behavior()\n",
    " \n",
    " \n",
    "a=np.array(\n",
    "[[1,2,3,4,5],\n",
    "[1,2,3,4,5],\n",
    "[1,2,3,4,5]]    \n",
    ")\n",
    "# a=np.array([[2.29982214e-10,1.05035841e-03,1.04089566e-04,9.98845458e-01,\n",
    "#   5.10124494e-08,5.03688757e-10,2.52189380e-10,1.18713073e-09,\n",
    "#   2.30988277e-08,2.21948682e-08,1.48340121e-07]])\n",
    "\n",
    "\n",
    "\n",
    "input = tf1.constant(a)\n",
    "k = 3\n",
    "output = tf1.nn.top_k(input, k).indices\n",
    "\n",
    "\n",
    "# one_hot=one_hot_tensor(output,11)\n",
    "# one_hot=one_hot[0]\n",
    "# # one_hot[0].shape\n",
    "# one_hot=K.reshape(one_hot,(1,one_hot.shape[0],one_hot.shape[1]))\n",
    "# # one_hot.shape\n",
    "# one_hot_permute=K.permute_dimensions(one_hot,(1,0,2))\n",
    "# one_hot_permute.shape\n",
    "\n",
    "with tf1.Session() as sess:\n",
    "    \n",
    "    print(sess.run(input))\n",
    "    print(sess.run(output))\n",
    "#     print(sess.run(one_hot))\n",
    "#     print(sess.run(one_hot_permute))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eager\n",
    "\n",
    "TensorFlow 2.X 的Eager模式，也可以看做是动态图模型。该模型下不需要先构造图，然后再使用Session.run()，而是可以得到即时的反馈。这样在研究和开发时会更加符合直觉。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "print(tf.__version__)\n",
    "\n",
    "#检查是否开启 eager 模式, 在使用了tensorflow.compat.v1 后就会将 eager 模式退出\n",
    "tf.executing_eagerly() \n",
    "\n",
    "tf.config.experimental_run_functions_eagerly(True) # 没有起到作用\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "rank_2_tensor = tf.constant([[1, 2],\n",
    "                             [3, 4],\n",
    "                             [5, 6]], dtype=tf.float16)\n",
    "print(rank_2_tensor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "a = tf.constant([[10,10],\n",
    "                 [11.,1.]])\n",
    "\n",
    "x = tf.constant([[1.,0.],\n",
    "                 [0.,1.]])\n",
    "\n",
    "b = tf.Variable(12.)\n",
    "\n",
    "c = np.ones((2))\n",
    "\n",
    "y = tf.matmul(a, x) + b \n",
    "\n",
    "print(y)\n",
    "\n",
    "y2 = y+c\n",
    "\n",
    "print(y2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_k_tensor(input, k):\n",
    "    \"\"\"\n",
    "    返回张量 input, 第0个维度的 topk 个元素的标号\n",
    "\n",
    "    :param input:\n",
    "    :param k:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "\n",
    "    return tf.nn.top_k(input,k).indices\n",
    "\n",
    "a = np.array(\n",
    "    [[1, 2, 3, 4, 5],\n",
    "     [1, 2, 2, 2, 2],\n",
    "     [1, 3, 3, 3, 6]]\n",
    ")\n",
    "k = 3\n",
    "\n",
    "# 静态图下要使用 eval 才能触发计算\n",
    "print('top_k_tensor: \\n', K.eval(top_k_tensor(a,k)))\n",
    "\n",
    "# 动态图下 直接打印即可\n",
    "print(top_k_tensor(a,k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a= np.array([[1,2,3,4]])\n",
    "layer1 = Dense(1)\n",
    "\n",
    "out = layer1(a)\n",
    "out \n",
    "print(out) # 直接执行"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  查看所有层(layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 查看所有的 layer \n",
    "\n",
    "model.layers\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import plot_model \n",
    "\n",
    "plot_model(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 输出指定层的参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.layers[2]\n",
    "\n",
    "model.layers[2].name\n",
    "\n",
    "model.get_layer('layer1')\n",
    "\n",
    "W , b = model.layers[2].get_weights()\n",
    "\n",
    "np.shape(W)\n",
    "np.shape(b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 输出该层的所有参数\n",
    "for weight in model.get_layer('layer1').weights:\n",
    "    print(weight.name, weight.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 输出所有层的所有参数\n",
    "\n",
    "for i in range(len(model.layers)):\n",
    "    \n",
    "    for weight in model.layers[i].weights:\n",
    "        \n",
    "        print(weight.name, weight.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 查看所有层 和 所有层的输出\n",
    "\n",
    "for index in range(len(model.layers)):\n",
    "    print(model.get_layer(index=index).name, model.get_layer(index=index).output_shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型中间层的计算结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.layers[2].name\n",
    "\n",
    "model.layers[2].input\n",
    "\n",
    "model.layers[2].output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 使用 K.function()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 例子1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "mid_func = K.function( inputs=[ model.layers[0].input, model.layers[1].input ] ,outputs=[model.layers[2].output] )\n",
    "\n",
    "N=5 # 10 个样本\n",
    "\n",
    "X=np.random.randint(0,10,size=[N,32])\n",
    "init_state=np.zeros((N,32))\n",
    "\n",
    "x_train = [X, init_state]\n",
    "\n",
    "res = mid_func([X, init_state])\n",
    "\n",
    "np.shape(res)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**使用K.function()可以快速检查层的正确性**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 例子2 LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "N = 2\n",
    "max_length = 4\n",
    "n_embedding=20\n",
    "n_vocab=100\n",
    "n_a = 10\n",
    "\n",
    "\n",
    "word_embedding_layer = Embedding(n_vocab, n_embedding, input_length=max_length  , name='word_embedding')\n",
    "lstm_layer = LSTM(units=n_a, return_sequences=True, return_state=True)\n",
    "\n",
    "\n",
    "word_list = Input(shape=(max_length))  \n",
    "\n",
    "mask = (word_list != 0) # shape(N,max_length)\n",
    "\n",
    "word_embedding = word_embedding_layer(inputs=word_list)\n",
    "\n",
    "out, state_h, state_c = lstm_layer(inputs=word_embedding, mask=mask)\n",
    "    \n",
    "\n",
    "f_out = K.function(inputs=[word_list], outputs=out)\n",
    "\n",
    "f_mask = K.function(inputs=[word_list], outputs=mask)\n",
    "\n",
    "f_state_h = K.function(inputs=[word_list], outputs=state_h)\n",
    "\n",
    "f_state_c = K.function(inputs=[word_list], outputs=state_c)\n",
    "\n",
    "\n",
    "seq_data = np.ones((N, max_length))\n",
    "\n",
    "\n",
    "# seq_data =np.random.randint(n_vocab,size=(N, max_length))  \n",
    "\n",
    "res_out = f_out([seq_data])\n",
    "res_state_h = f_state_h([seq_data])\n",
    "res_state_c = f_state_c([seq_data])\n",
    "\n",
    "\n",
    "res_mask = f_mask([seq_data])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(res_out)\n",
    "res_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_data\n",
    "\n",
    "np.shape(res_out)\n",
    "res_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 把 LSTM 的时间步拆出来, 一步一步执行\n",
    "\n",
    "# 1.执行第1个时间步\n",
    "\n",
    "word_list_t = Input(shape=(max_length))  \n",
    "mask = (word_list_t != 0) \n",
    "word_embedding_t = word_embedding_layer(inputs=word_list_t)\n",
    "out_t, state_h_t, state_c_t = lstm_layer(inputs=word_embedding_t, mask=mask)\n",
    "\n",
    "f_out_t = K.function(inputs=[word_list_t], outputs=out_t)\n",
    "\n",
    "# 测试数据\n",
    "seq_data_0 = np.expand_dims(seq_data[:,0], axis=1)\n",
    "# np.shape(seq_data_0)\n",
    "\n",
    "# 推理\n",
    "res_out_0 = f_out_t([seq_data_0])\n",
    "res_out_0\n",
    "\n",
    "res_out[:,0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.执行第1个时间步，给 LSTM 一个初始隐藏层状态 h0\n",
    "\n",
    "word_list_t = Input(shape=(max_length))  \n",
    "h0 = Input(shape=(n_a))\n",
    "c0 = Input(shape=(n_a))\n",
    "\n",
    "mask = (word_list_t != 0) \n",
    "word_embedding_t = word_embedding_layer(inputs=word_list_t)\n",
    "h=h0\n",
    "c=c0\n",
    "out_t, h_t, c_t = lstm_layer(inputs=word_embedding_t, mask=mask, initial_state=[h, c])\n",
    "\n",
    "f_out_t = K.function(inputs=[word_list_t, h0, c0], outputs=out_t)\n",
    "f_h_t = K.function(inputs=[word_list_t, h0, c0], outputs=h_t)\n",
    "f_c_t = K.function(inputs=[word_list_t, h0, c0], outputs=c_t)\n",
    "\n",
    "# 测试数据\n",
    "seq_data_0 = np.expand_dims(seq_data[:,0], axis=1)\n",
    "# np.shape(seq_data_0)\n",
    "\n",
    "data_h = np.ones((N, n_a))\n",
    "# data_h = np.zeros((N, n_a))\n",
    "data_c = np.zeros((N, n_a))\n",
    "\n",
    "# 推理\n",
    "res_out_0 = f_out_t([seq_data_0, data_h, data_c])\n",
    "res_out_0\n",
    "\n",
    "# res_out[:,0,:]\n",
    "\n",
    "\n",
    "res_h_t = f_h_t([seq_data_0, data_h, data_c])\n",
    "res_h_t\n",
    "\n",
    "\n",
    "res_c_t = f_c_t([seq_data_0, data_h, data_c])\n",
    "res_c_t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 建立新的 model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用原来的 layer 建立新的计算图"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "mid_model = Model( inputs=[ model.layers[0].input, model.layers[1].input ] ,outputs=[model.layers[2].output] )\n",
    "\n",
    "N=5 # 10 个样本\n",
    "\n",
    "X=np.random.randint(0,10,size=[N,32])\n",
    "init_state=np.zeros((N,32))\n",
    "\n",
    "x_train = [X, init_state]\n",
    "\n",
    "res = mid_model.predict([X, init_state])\n",
    "\n",
    "np.shape(res)\n",
    "\n",
    "# res\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Graph  execution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 与 Eager execution 的区别"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def get_MSE(y_true, y_pred):\n",
    "  sq_diff = tf.pow(y_true - y_pred, 2)\n",
    "  return tf.reduce_mean(sq_diff)\n",
    "\n",
    "y_true = tf.random.uniform([5], maxval=10, dtype=tf.int32)\n",
    "y_pred = tf.random.uniform([5], maxval=10, dtype=tf.int32)\n",
    "\n",
    "get_MSE(y_true, y_pred)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 关闭构建计算图\n",
    "tf.config.run_functions_eagerly(True)\n",
    "\n",
    "get_MSE(y_true, y_pred)\n",
    "\n",
    "# Don't forget to set it back when you are done.\n",
    "tf.config.run_functions_eagerly(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@tf.function\n",
    "def get_MSE(y_true, y_pred):\n",
    "  print(\"Calculating MSE!\")\n",
    "  sq_diff = tf.pow(y_true - y_pred, 2)\n",
    "  return tf.reduce_mean(sq_diff)\n",
    "\n",
    "error = get_MSE(y_true, y_pred)\n",
    "error = get_MSE(y_true, y_pred)\n",
    "error = get_MSE(y_true, y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, globally set everything to run eagerly to force eager execution.\n",
    "tf.config.run_functions_eagerly(True)\n",
    "\n",
    "error = get_MSE(y_true, y_pred)\n",
    "error = get_MSE(y_true, y_pred)\n",
    "error = get_MSE(y_true, y_pred)\n",
    "\n",
    "tf.config.run_functions_eagerly(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 非严格执行\n",
    "\n",
    "计算图中不是所有的操作都被执行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.gather(1, [1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unused_return_eager(x):\n",
    "  # Get index 1 will fail when `len(x) == 1`\n",
    "  tf.gather(x, [1]) # unused \n",
    "  return x\n",
    "\n",
    "try:\n",
    "  print(unused_return_eager(tf.constant([0.0])))\n",
    "except tf.errors.InvalidArgumentError as e:\n",
    "  # All operations are run during eager execution so an error is raised.\n",
    "  print(f'{type(e).__name__}: {e}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def unused_return_graph(x):\n",
    "  tf.gather(x, [1]) # unused\n",
    "  return x\n",
    "\n",
    "# Only needed operations are run during graph exection. The error is not raised.\n",
    "print(unused_return_graph(tf.constant([0.0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 计算图的多态\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.计算图的构建**\n",
    "\n",
    "第一阶段为跟踪（tracing），Function 创建一个新的 tf.Graph，在此阶段 Python代码正常运行，但所有TensorFlow操作（比如两个Tensors 相加）都被推迟执行(它们被 tf.Graph捕获 并用来建立计算图，但是不执行计算图)   \n",
    "\n",
    "在第二阶段，执行在第一步构建好的计算图（tf.Graph）, 此阶段比跟踪阶段快得多\n",
    "\n",
    "使用 get_concrete_function 函数可以只执行第一阶段\n",
    "\n",
    "在跟踪阶段会创建一个tf.Graph 并将其包装在新建立的 concrete function中\n",
    "\n",
    "Function manages 管理所有的 concrete function，并根据输入的 tensor 来选择一个合适的计算图进行计算\n",
    "\n",
    "\n",
    "\n",
    "**2.input_signature**\n",
    "\n",
    "在输入 tensor 的 维度 或 数据类型 改变后，框架会通过 Tracing 代码中的所有计算节点来建立新的计算图，我们可以使用 input_signature 来避免重复建立计算图 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function()\n",
    "def g(x):\n",
    "  print('Tracing with', x)\n",
    "  return x\n",
    "\n",
    "\n",
    "print(g(tf.constant([1, 2, 3])))\n",
    "\n",
    "print(g(tf.constant([1, 2, 3, 4, 5])))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(g.pretty_printed_concrete_signatures())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function(input_signature=(tf.TensorSpec(shape=[None], dtype=tf.int32),))\n",
    "def g(x):\n",
    "  print('Tracing with', x)\n",
    "  return x\n",
    "\n",
    "# No retrace!\n",
    "print(g(tf.constant([1, 2, 3])))\n",
    "\n",
    "print(g(tf.constant([1, 2, 3, 4, 5])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def double_func(a):\n",
    "  print(\"Tracing with\", a)\n",
    "  return a + a\n",
    "\n",
    "print(\"Obtaining concrete trace\")\n",
    "double_strings = double_func.get_concrete_function(tf.constant(\"a\"))\n",
    "\n",
    "print(\"Executing traced function\")\n",
    "print(double_strings(tf.constant(\"a\")))\n",
    "print(double_strings(a=tf.constant(\"b\")))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(double_strings)\n",
    "\n",
    "print(double_strings.structured_input_signature)\n",
    "\n",
    "print(double_strings.structured_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def pow(a, b):\n",
    "    \n",
    "    print(a) # Caution! Will only happen once when tracing\n",
    "    tf.print(a)\n",
    "    \n",
    "    return a**b\n",
    "\n",
    "square = pow.get_concrete_function(a=tf.TensorSpec(None, tf.int32), b=tf.TensorSpec(None, tf.int32))\n",
    "print(square)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "square(tf.constant(10), tf.constant(3))\n",
    "\n",
    "# square(10, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Python 变量作为参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_step():\n",
    "  pass\n",
    "\n",
    "@tf.function\n",
    "def train(num_steps):\n",
    "  print(\"Tracing with num_steps = \", num_steps)\n",
    "  tf.print(\"Executing with num_steps = \", num_steps)\n",
    "  for _ in tf.range(num_steps):\n",
    "    train_one_step()\n",
    "\n",
    "print(\"Retracing occurs for different Python arguments.\")\n",
    "train(num_steps=10)\n",
    "train(num_steps=20)\n",
    "\n",
    "print()\n",
    "print(\"Traces are reused for Tensor arguments.\")\n",
    "train(num_steps=tf.constant(10))\n",
    "train(num_steps=tf.constant(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@tf.function\n",
    "def change_to_tensor(ragged, max_seq_length):\n",
    "    \"\"\"\n",
    "    返回序列长度为固定的 tensor\n",
    "\n",
    "    :param strings:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    tf.print(max_seq_length)\n",
    "    return ragged.to_tensor(shape=[None, max_seq_length])\n",
    "\n",
    "ragged = tf.ragged.constant([[3, 1, 4, 1], [], [5, 9, 2], [6], []])\n",
    "\n",
    "# tensor = ragged.to_tensor(shape=[None, 5])\n",
    "\n",
    "tensor = change_to_tensor(ragged, 4)\n",
    "\n",
    "tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**一个奇怪的bug**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = tf.constant(4)\n",
    "a.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function(input_signature=[tf.RaggedTensorSpec(shape=[None, None], dtype=tf.int32), tf.TensorSpec(dtype=tf.int32, shape=None)])\n",
    "def change_to_tensor(ragged, max_seq_length):\n",
    "    \"\"\"\n",
    "    返回序列长度为固定的 tensor\n",
    "\n",
    "    :param strings:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    print(ragged)\n",
    "\n",
    "    print(max_seq_length)\n",
    "\n",
    "#     tf.print(ragged)\n",
    "#     tf.print(max_seq_length.numpy())\n",
    "    tf.print(max_seq_length)\n",
    "    \n",
    "    return ragged.to_tensor(shape=[None, max_seq_length])\n",
    "\n",
    "ragged = tf.ragged.constant([[3, 1, 4, 1], [], [5, 9, 2], [6], []])\n",
    "\n",
    "# tf.print(ragged)\n",
    "\n",
    "tensor = change_to_tensor(ragged, tf.constant(4))\n",
    "\n",
    "tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 流程控制"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A simple loop\n",
    "\n",
    "@tf.function\n",
    "def f(x):\n",
    "  while tf.reduce_sum(x) > 1:\n",
    "    tf.print(x)\n",
    "    x = tf.tanh(x)\n",
    "  return x\n",
    "\n",
    "# f(tf.random.uniform([5]))\n",
    "\n",
    "# print(tf.autograph.to_code(f.python_function))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def fizzbuzz(n):\n",
    "  for i in tf.range(1, n + 1):\n",
    "    print('Tracing for loop')\n",
    "    if i % 15 == 0:\n",
    "      print('Tracing fizzbuzz branch')\n",
    "      tf.print('fizzbuzz')\n",
    "    elif i % 3 == 0:\n",
    "      print('Tracing fizz branch')\n",
    "      tf.print('fizz')\n",
    "    elif i % 5 == 0:\n",
    "      print('Tracing buzz branch')\n",
    "      tf.print('buzz')\n",
    "    else:\n",
    "      print('Tracing default branch')\n",
    "      tf.print(i)\n",
    "\n",
    "fizzbuzz(tf.constant(5))\n",
    "# fizzbuzz(tf.constant(20))\n",
    "\n",
    "fizzbuzz(tf.constant(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DynamicRNN(tf.keras.Model):\n",
    "\n",
    "      def __init__(self, rnn_cell):\n",
    "            \n",
    "            super(DynamicRNN, self).__init__(self)\n",
    "            self.cell = rnn_cell\n",
    "\n",
    "      @tf.function(input_signature=[tf.TensorSpec(dtype=tf.float32, shape=[None, None, 3])])\n",
    "      def call(self, input_data):\n",
    "            \n",
    "            print('Tracing with',input_data)\n",
    "            \n",
    "            # [batch, time, features] -> [time, batch, features]\n",
    "            input_data = tf.transpose(input_data, [1, 0, 2])\n",
    "            timesteps =  tf.shape(input_data)[0]\n",
    "            batch_size = tf.shape(input_data)[1]\n",
    "            outputs = tf.TensorArray(tf.float32, timesteps)\n",
    "            state = self.cell.get_initial_state(batch_size = batch_size, dtype=tf.float32)\n",
    "\n",
    "            for i in tf.range(timesteps):\n",
    "                output, state = self.cell(input_data[i], state)\n",
    "                outputs = outputs.write(i, output)\n",
    "            \n",
    "            tf.print(outputs.read(0))\n",
    "            \n",
    "            return tf.transpose(outputs.stack(), [1, 0, 2]), state\n",
    "\n",
    "lstm_cell = tf.keras.layers.LSTMCell(units = 6)\n",
    "\n",
    "rnn = DynamicRNN(lstm_cell)\n",
    "\n",
    "outputs, state = rnn(tf.random.normal(shape=[2,5,3]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs, state = rnn(tf.random.normal(shape=[10,10,3]))\n",
    "\n",
    "outputs, state = rnn(tf.random.normal(shape=[20,10,3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### python 对象的副作用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def f(x):\n",
    "  print(\"Traced with\", x)\n",
    "  tf.print(\"Executed with\", x)\n",
    "\n",
    "f(1)\n",
    "f(1)\n",
    "f(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### python 变量和 tf.Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(tf.Module):\n",
    "  def __init__(self):\n",
    "    self.v = tf.Variable(0)\n",
    "    self.counter = 0\n",
    "\n",
    "  @tf.function\n",
    "  def __call__(self):\n",
    "    \n",
    "    print('in Trace step')\n",
    "    if self.counter == 0:\n",
    "      \n",
    "      tf.print('counter:', self.counter)\n",
    "      # A python side-effect\n",
    "      self.counter += 1\n",
    "      self.v.assign_add(1)\n",
    "\n",
    "    return self.v\n",
    "\n",
    "m = Model()\n",
    "\n",
    "print(m().numpy()) # prints 1, 2, 3\n",
    "\n",
    "print(m().numpy()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(tf.Module):\n",
    "  def __init__(self):\n",
    "    self.v = tf.Variable(0)\n",
    "    self.counter = tf.Variable(0)\n",
    "\n",
    "  @tf.function\n",
    "  def __call__(self):\n",
    "    \n",
    "    print('in Trace step')\n",
    "    if self.counter == 0:\n",
    "      \n",
    "      tf.print('counter:', self.counter)\n",
    "      # A python side-effect\n",
    "      self.counter.assign_add(1)\n",
    "        \n",
    "      self.v.assign_add(1)\n",
    "\n",
    "    return self.v\n",
    "\n",
    "m = Model()\n",
    "\n",
    "print(m().numpy()) # prints 1, 2, 3\n",
    "\n",
    "print(m().numpy()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(tf.Module):\n",
    "  def __init__(self):\n",
    "    self.v = tf.Variable(0)\n",
    "    self.counter = 0\n",
    "\n",
    "  @tf.function\n",
    "  def __call__(self):\n",
    "    if self.counter == 0:\n",
    "      # Lifts ops out of function-building graphs\n",
    "      with tf.init_scope():\n",
    "        self.counter += 1\n",
    "        self.v.assign_add(1)\n",
    "\n",
    "    return self.v\n",
    "\n",
    "m = Model()\n",
    "\n",
    "print(m().numpy()) \n",
    "print(m().numpy()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def buggy_add():\n",
    "  return 1 + foo\n",
    "\n",
    "@tf.function\n",
    "def recommended_add(foo):\n",
    "  return 1 + foo\n",
    "\n",
    "foo = 1\n",
    "print(\"Buggy:\", buggy_add())\n",
    "print(\"Correct:\", recommended_add(foo))\n",
    "\n",
    "print(\"Updating the value of `foo` to 100!\")\n",
    "foo = 100\n",
    "print(\"Buggy:\", buggy_add())  # Did not change!\n",
    "print(\"Correct:\", recommended_add(foo))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def variable_add():\n",
    "  return 1 + foo\n",
    "\n",
    "foo = tf.Variable(1)\n",
    "print(\"Variable:\", variable_add())\n",
    "\n",
    "print(\"Updating the value of `foo` to 100!\")\n",
    "foo.assign(100)\n",
    "print(\"Variable:\", variable_add())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BetterModel:\n",
    "\n",
    "  def __init__(self):\n",
    "    self.bias = tf.Variable(0.)\n",
    "    self.weight = tf.Variable(2.)\n",
    "\n",
    "@tf.function\n",
    "def evaluate(model, x):\n",
    "  return model.weight * x + model.bias\n",
    "\n",
    "better_model = BetterModel()\n",
    "x = tf.constant(10.)\n",
    "print(evaluate(better_model, x))\n",
    "\n",
    "print(\"Adding bias!\")\n",
    "better_model.bias.assign_add(5.0)  # Note: instead of better_model.bias += 5\n",
    "print(evaluate(better_model, x))  # This works!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def f(x):\n",
    "  v = tf.Variable(1.0)\n",
    "  return v\n",
    "\n",
    "f(1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Count(tf.Module):\n",
    "  def __init__(self):\n",
    "    self.count = None\n",
    "\n",
    "  @tf.function\n",
    "  def __call__(self):\n",
    "    if self.count is None:\n",
    "      self.count = tf.Variable(0)\n",
    "    return self.count.assign_add(1)\n",
    "\n",
    "c = Count()\n",
    "print(c())\n",
    "print(c())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def buggy_consume_next(iterator):\n",
    "  \n",
    "  print('in Trace step')\n",
    "  tf.print(\"Value:\", next(iterator))\n",
    "\n",
    "iterator = iter([1, 2, 3])\n",
    "buggy_consume_next(iterator)\n",
    "# This reuses the first value from the iterator, rather than consuming the next value.\n",
    "buggy_consume_next(iterator)\n",
    "buggy_consume_next(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def good_consume_next(iterator):\n",
    "  # This is ok, iterator is a tf.data.Iterator\n",
    "  tf.print(\"Value:\", next(iterator))\n",
    "\n",
    "ds = tf.data.Dataset.from_tensor_slices([1, 2, 3])\n",
    "iterator = iter(ds)\n",
    "good_consume_next(iterator)\n",
    "good_consume_next(iterator)\n",
    "good_consume_next(iterator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "external_list = []\n",
    "\n",
    "@tf.function\n",
    "def side_effect(x):\n",
    "  print('in Trace step')\n",
    "  external_list.append(x)\n",
    "\n",
    "side_effect(1)\n",
    "side_effect(1)\n",
    "side_effect(1)\n",
    "# The list append only happened once!\n",
    "print(external_list)\n",
    "assert len(external_list) == 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "side_effect(2)\n",
    "print(external_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@tf.function\n",
    "def f(x):\n",
    "    l = [] # python list 对象\n",
    "    for i in x:\n",
    "        l.append(i + 1)    # Caution! Will only happen once when tracing\n",
    "    return l\n",
    "\n",
    "\n",
    "# f(tf.constant([1, 2, 3]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = []\n",
    "@tf.function\n",
    "def f(x):\n",
    "  for i in x:\n",
    "    l.append(i + 1)    # Caution! Will only happen once when tracing\n",
    "f(tf.constant([1, 2, 3]))\n",
    "# l\n",
    "l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def f(x):\n",
    "  ta = tf.TensorArray(dtype=tf.int32, size=0, dynamic_size=True)\n",
    "  for i in range(len(x)):\n",
    "    ta = ta.write(i, x[i] + 1)\n",
    "  return ta.stack()\n",
    "f(tf.constant([1, 2, 3]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tensor 泄露"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = None\n",
    "\n",
    "@tf.function\n",
    "def leaky_function(a):\n",
    "  global x\n",
    "  x = a + 1  # Bad - leaks local tensor\n",
    "  return a + 2\n",
    "\n",
    "correct_a = leaky_function(tf.constant(1))\n",
    "\n",
    "print(correct_a.numpy())  # Good - value obtained from function's returns\n",
    "\n",
    "try:\n",
    "  x.numpy()  # Bad - tensor leaked from inside the function, cannot be used here\n",
    "except AttributeError as expected:\n",
    "  print(expected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyClass:\n",
    "\n",
    "  def __init__(self):\n",
    "    self.field = None\n",
    "\n",
    "external_list = []\n",
    "external_object = MyClass()\n",
    "\n",
    "def leaky_function():\n",
    "  a = tf.constant(1)\n",
    "  external_list.append(a)  # Bad - leaks tensor\n",
    "  external_object.field = a  # Bad - leaks tensor\n",
    "\n",
    "leaky_function()\n",
    "\n",
    "print(external_list)\n",
    "\n",
    "leaky_function()\n",
    "\n",
    "print(external_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5R5S3CnAjA_u"
   },
   "source": [
    "# 编译和拟合模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用内置的方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zZegr331jXOY"
   },
   "outputs": [],
   "source": [
    "basic_model.compile( loss='sparse_categorical_crossentropy',\n",
    "               optimizer='adam',\n",
    "               metrics=['accuracy'] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 755
    },
    "colab_type": "code",
    "id": "E1iXQ1unj3po",
    "outputId": "d7e70ef0-b325-4178-c159-e01484b57ec3"
   },
   "outputs": [],
   "source": [
    "basic_model.fit( x_train, y_train, epochs=10, validation_split=0.2 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Dl2q7XZgkBGx"
   },
   "outputs": [],
   "source": [
    "class myCallback( tf.keras.callbacks.Callback ):\n",
    "  def on_epoch_end(self, epoch, logs={}):\n",
    "    if(logs.get('acc')>0.9):\n",
    "      print(\"\\nReached 90% accuracy so cancelling training!\")\n",
    "      self.model.stop_training = True\n",
    "\n",
    "callbacks = myCallback()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 359
    },
    "colab_type": "code",
    "id": "eHs6QHCxpTRl",
    "outputId": "569b8427-5bbe-45a2-b5ca-1de146b47a77"
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add( Flatten(input_shape=[28,28]) )\n",
    "model.add( Dense(100, activation='relu') )\n",
    "model.add( Dense(10, activation='softmax') )\n",
    "\n",
    "model.compile( loss='sparse_categorical_crossentropy',\n",
    "               optimizer='adam',\n",
    "               metrics=['accuracy'] )\n",
    "\n",
    "model.fit( x_train, y_train, epochs=20, callbacks=[callbacks] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 自定义 fit "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomModel(keras.Model):\n",
    "    \n",
    "\n",
    "#     @tf.function(input_signature=[\n",
    "#         tf.TensorSpec(shape=(None, None), dtype=tf.int64),\n",
    "#         tf.TensorSpec(shape=(None, None), dtype=tf.int64),\n",
    "#     ])\n",
    "    def train_step(self, data):\n",
    "        # Unpack the data. Its structure depends on your model and\n",
    "        # on what you pass to `fit()`.\n",
    "        x, y = data\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            y_pred = self(x, training=True)  # Forward pass\n",
    "            # Compute the loss value\n",
    "            # (the loss function is configured in `compile()`)\n",
    "            loss = self.compiled_loss(y, y_pred, regularization_losses=self.losses)\n",
    "\n",
    "        # Compute gradients\n",
    "        trainable_vars = self.trainable_variables\n",
    "        gradients = tape.gradient(loss, trainable_vars)\n",
    "        # Update weights\n",
    "        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n",
    "        # Update metrics (includes the metric that tracks the loss)\n",
    "        self.compiled_metrics.update_state(y, y_pred)\n",
    "        # Return a dict mapping metric names to current value\n",
    "        return {m.name: m.result() for m in self.metrics}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Construct and compile an instance of CustomModel\n",
    "inputs = keras.Input(shape=(32,))\n",
    "outputs = keras.layers.Dense(1)(inputs)\n",
    "model = CustomModel(inputs, outputs)\n",
    "model.compile(optimizer=\"adam\", loss=\"mse\", metrics=[\"mae\"])\n",
    "\n",
    "# Just use `fit` as usual\n",
    "x = np.random.random((1000, 32))\n",
    "y = np.random.random((1000, 1))\n",
    "model.fit(x, y, epochs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_tracker = keras.metrics.Mean(name=\"loss\")\n",
    "mae_metric = keras.metrics.MeanAbsoluteError(name=\"mae\")\n",
    "\n",
    "\n",
    "class CustomModel(keras.Model):\n",
    "    def train_step(self, data):\n",
    "        x, y = data\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            y_pred = self(x, training=True)  # Forward pass\n",
    "            # Compute our own loss\n",
    "            loss = keras.losses.mean_squared_error(y, y_pred)\n",
    "\n",
    "        # Compute gradients\n",
    "        trainable_vars = self.trainable_variables\n",
    "        gradients = tape.gradient(loss, trainable_vars)\n",
    "\n",
    "        # Update weights\n",
    "        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n",
    "\n",
    "        # Compute our own metrics\n",
    "        loss_tracker(loss)\n",
    "        mae_metric(y, y_pred)\n",
    "        return {\"loss\": loss_tracker.result(), \"mae\": mae_metric.result()}\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        # We list our `Metric` objects here so that `reset_states()` can be\n",
    "        # called automatically at the start of each epoch\n",
    "        # or at the start of `evaluate()`.\n",
    "        # If you don't implement this property, you have to call\n",
    "        # `reset_states()` yourself at the time of your choosing.\n",
    "        return [loss_tracker, mae_metric]\n",
    "\n",
    "\n",
    "# Construct an instance of CustomModel\n",
    "inputs = keras.Input(shape=(32,))\n",
    "outputs = keras.layers.Dense(1)(inputs)\n",
    "model = CustomModel(inputs, outputs)\n",
    "\n",
    "# We don't passs a loss or metrics here.\n",
    "model.compile(optimizer=\"adam\")\n",
    "\n",
    "# Just use `fit` as usual -- you can use callbacks, etc.\n",
    "x = np.random.random((1000, 32))\n",
    "y = np.random.random((1000, 1))\n",
    "model.fit(x, y, epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 从头编写训练循环"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "\n",
    "# Prepare the training dataset.\n",
    "batch_size = 64\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "x_train = np.reshape(x_train, (-1, 784))\n",
    "x_test = np.reshape(x_test, (-1, 784))\n",
    "\n",
    "# Reserve 10,000 samples for validation.\n",
    "x_val = x_train[-10000:]\n",
    "y_val = y_train[-10000:]\n",
    "x_train = x_train[:-10000]\n",
    "y_train = y_train[:-10000]\n",
    "\n",
    "# Prepare the training dataset.\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "train_dataset = train_dataset.shuffle(buffer_size=1024).batch(batch_size)\n",
    "\n",
    "# Prepare the validation dataset.\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((x_val, y_val))\n",
    "val_dataset = val_dataset.batch(batch_size)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get model\n",
    "inputs = keras.Input(shape=(784,), name=\"digits\")\n",
    "x = layers.Dense(64, activation=\"relu\", name=\"dense_1\")(inputs)\n",
    "x = layers.Dense(64, activation=\"relu\", name=\"dense_2\")(x)\n",
    "outputs = layers.Dense(10, name=\"predictions\")(x)\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate an optimizer to train the model.\n",
    "optimizer = keras.optimizers.SGD(learning_rate=1e-3)\n",
    "# Instantiate a loss function.\n",
    "loss_fn = keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "# Prepare the metrics.\n",
    "train_acc_metric = keras.metrics.SparseCategoricalAccuracy()\n",
    "val_acc_metric = keras.metrics.SparseCategoricalAccuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "epochs = 1\n",
    "for epoch in range(epochs):\n",
    "    print(\"\\nStart of epoch %d\" % (epoch,))\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Iterate over the batches of the dataset.\n",
    "    for step, (x_batch_train, y_batch_train) in enumerate(train_dataset):\n",
    "        with tf.GradientTape() as tape:\n",
    "            logits = model(x_batch_train, training=True)\n",
    "            loss_value = loss_fn(y_batch_train, logits)\n",
    "        grads = tape.gradient(loss_value, model.trainable_weights)\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
    "\n",
    "        # Update training metric.\n",
    "        train_acc_metric.update_state(y_batch_train, logits)\n",
    "\n",
    "        # Log every 200 batches.\n",
    "        if step % 200 == 0:\n",
    "            print(\n",
    "                \"Training loss (for one batch) at step %d: %.4f\"\n",
    "                % (step, float(loss_value))\n",
    "            )\n",
    "            print(\"Seen so far: %d samples\" % ((step + 1) * batch_size))\n",
    "\n",
    "    # Display metrics at the end of each epoch.\n",
    "    train_acc = train_acc_metric.result()\n",
    "    print(\"Training acc over epoch: %.4f\" % (float(train_acc),))\n",
    "\n",
    "    # Reset training metrics at the end of each epoch\n",
    "    train_acc_metric.reset_states()\n",
    "\n",
    "    # Run a validation loop at the end of each epoch.\n",
    "    for x_batch_val, y_batch_val in val_dataset:\n",
    "        val_logits = model(x_batch_val, training=False)\n",
    "        # Update val metrics\n",
    "        val_acc_metric.update_state(y_batch_val, val_logits)\n",
    "    val_acc = val_acc_metric.result()\n",
    "    val_acc_metric.reset_states()\n",
    "    print(\"Validation acc: %.4f\" % (float(val_acc),))\n",
    "    print(\"Time taken: %.2fs\" % (time.time() - start_time))\n",
    "\n",
    "# TODO: 在 pycharm 中执行不会 打印 <tf.Variable 'UnreadVariable' shape=() dtype=int64, numpy=1>\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 混合精度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tensorflow.keras.mixed_precision import experimental as mixed_precision\n",
    "\n",
    "\n",
    "# os.environ['CUDA_VISIBLE_DEVICES'] = '0,1'    #使用GPU0,1\n",
    "\n",
    "# os.environ['CUDA_VISIBLE_DEVICES'] = '-1'      #使用CPU\n",
    "\n",
    "\n",
    "tf.config.list_physical_devices('GPU')\n",
    "\n",
    "# policy = mixed_precision.Policy('mixed_float16')\n",
    "\n",
    "policy = mixed_precision.Policy('float32')\n",
    "\n",
    "mixed_precision.set_policy(policy)\n",
    "\n",
    "inputs = keras.Input(shape=(784,), name='digits')\n",
    "\n",
    "num_units = 4096\n",
    "\n",
    "dense1 = layers.Dense(num_units, activation='relu', name='dense_1')\n",
    "x = dense1(inputs)\n",
    "dense2 = layers.Dense(num_units, activation='relu', name='dense_2')\n",
    "x = dense2(x)\n",
    "\n",
    "print('x.dtype: %s' % x.dtype.name)\n",
    "# 'kernel' is dense1's variable\n",
    "print('dense1.kernel.dtype: %s' % dense1.kernel.dtype.name)\n",
    "\n",
    "# CORRECT: softmax and model output are float32\n",
    "x = layers.Dense(10, name='dense_logits')(x)\n",
    "outputs = layers.Activation('softmax', dtype='float32', name='predictions')(x)\n",
    "print('Outputs dtype: %s' % outputs.dtype.name)\n",
    "\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "model.compile(loss='sparse_categorical_crossentropy',\n",
    "              optimizer=keras.optimizers.RMSprop(),\n",
    "              metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "x_train = x_train.reshape(60000, 784).astype('float32') / 255\n",
    "x_test = x_test.reshape(10000, 784).astype('float32') / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_weights = model.get_weights()\n",
    "\n",
    "# initial_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(x_train, y_train,\n",
    "                    batch_size=8192,\n",
    "                    epochs=5,\n",
    "                    validation_split=0.2)\n",
    "test_scores = model.evaluate(x_test, y_test, verbose=2)\n",
    "print('Test loss:', test_scores[0])\n",
    "print('Test accuracy:', test_scores[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 损失放大\n",
    "\n",
    "float16 也极少出现下溢的情况。此外，在正向传递中出现下溢的情形更是十分罕见。但是，在反向传递中，梯度可能因下溢而变为零。损失放大就是一个防止出现下溢的技巧。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.constant(256, dtype='float16')\n",
    "(x ** 2).numpy()  # Overflow\n",
    "\n",
    "x = tf.constant(1e-5, dtype='float16')\n",
    "(x ** 2).numpy()  # Underflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = mixed_precision.Policy('mixed_float16')\n",
    "mixed_precision.set_policy(policy)\n",
    "\n",
    "\n",
    "loss_scale = policy.loss_scale\n",
    "print('Loss scale: %s' % loss_scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_policy = mixed_precision.Policy('mixed_float16', loss_scale=1024)\n",
    "print(new_policy.loss_scale)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 检查点"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FBtaK8QIqvgm"
   },
   "source": [
    "# 预测模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "colab_type": "code",
    "id": "zFmJDUHnpaHP",
    "outputId": "7467e053-8bb8-43e5-876b-25c1d02eef16"
   },
   "outputs": [],
   "source": [
    "basic_model.evaluate( x_test, y_test )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "id": "YLmeFPKhq9pa",
    "outputId": "95251b5d-6cb6-4ed0-c3e2-6cc0e149b3c7"
   },
   "outputs": [],
   "source": [
    "prob = model.predict( x_test[0:1] )\n",
    "prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "6zWKDawxtS4i",
    "outputId": "b1ec0d04-cca6-46f8-e2f9-200b7cde65ac"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.argmax(prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "Df1S7zmNrg6i",
    "outputId": "3461ed1f-31cc-43d0-d811-bbc7aaa75085"
   },
   "outputs": [],
   "source": [
    "print( y_test[0] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 287
    },
    "colab_type": "code",
    "id": "kRAuDP45sAuD",
    "outputId": "074d6fde-4a9f-43ce-d1f9-a95a680dbdaa"
   },
   "outputs": [],
   "source": [
    "plt.imshow(x_test[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vfwfZIk0r4Nn"
   },
   "outputs": [],
   "source": [
    "# 训练精度 90% 但是测试精度 87.7%，有过拟合的征兆"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "a9ScZEUHvhgq"
   },
   "source": [
    "## 引进验证集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wdeqxXZFvlSr"
   },
   "outputs": [],
   "source": [
    "data = keras.datasets.fashion_mnist\n",
    "\n",
    "(x_train_full, y_train_full),(x_test, y_test) = data.load_data()\n",
    "\n",
    "x_valid, x_train = x_train_full[:5000] / 255.0, x_train_full[5000:] / 255.0\n",
    "y_valid, y_train = y_train_full[:5000], y_train_full[5000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eoWS6Qd3wuKq"
   },
   "outputs": [],
   "source": [
    "class myCallback( tf.keras.callbacks.Callback ):\n",
    "  def on_epoch_end(self, epoch, logs={}):\n",
    "    if(logs.get('val_acc')>0.9):\n",
    "      print(\"\\nReached 90% validation accuracy so cancelling training!\")\n",
    "      self.model.stop_training = True\n",
    "\n",
    "callbacks = myCallback()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 755
    },
    "colab_type": "code",
    "id": "OdDYrGK5wBgi",
    "outputId": "4ec8d58b-e17e-444b-b360-3de8bcc5b3e7"
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add( Flatten(input_shape=[28,28]) )\n",
    "model.add( Dense(100, activation='relu') )\n",
    "model.add( Dense(10, activation='softmax') )\n",
    "\n",
    "model.compile( loss='sparse_categorical_crossentropy',\n",
    "               optimizer='adam',\n",
    "               metrics=['accuracy'] )\n",
    "\n",
    "history = model.fit( x_train,\n",
    "                     y_train,\n",
    "                     epochs=20, \n",
    "                     validation_data=(x_valid, y_valid), \n",
    "                     callbacks=[callbacks] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 755
    },
    "colab_type": "code",
    "id": "QAtj4x8mxIUM",
    "outputId": "f8ba99a6-c094-4d82-d99e-6351dfd1b320"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
    "\n",
    "data = keras.datasets.fashion_mnist\n",
    "\n",
    "(x_train_full, y_train_full),(x_test, y_test) = data.load_data()\n",
    "\n",
    "x_train_full = x_train_full.reshape(60000, 28, 28, 1)\n",
    "x_test = x_test.reshape(10000, 28, 28, 1)\n",
    "\n",
    "x_valid, x_train = x_train_full[:5000] / 255.0, x_train_full[5000:] / 255.0\n",
    "y_valid, y_train = y_train_full[:5000], y_train_full[5000:]\n",
    "\n",
    "\n",
    "model = Sequential([Conv2D(64, (3,3), activation='relu', input_shape=(28,28,1)),\n",
    "                    MaxPooling2D(2,2),\n",
    "                    Conv2D(64, (3,3), activation='relu'),\n",
    "                    MaxPooling2D(2,2),\n",
    "                    Flatten(),\n",
    "                    Dense(128, activation='relu'),\n",
    "                    Dense(10, activation='softmax') ])\n",
    "\n",
    "model.compile( loss='sparse_categorical_crossentropy',\n",
    "               optimizer='adam',\n",
    "               metrics=['accuracy'] )\n",
    "\n",
    "history = model.fit( x_train,\n",
    "                     y_train,\n",
    "                     epochs=20, \n",
    "                     validation_data=(x_valid, y_valid) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 385
    },
    "colab_type": "code",
    "id": "O2GrT5mSA1Mu",
    "outputId": "279d5a97-0581-4f0c-f1f8-07362ab8d236"
   },
   "outputs": [],
   "source": [
    "acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs = range(1, len(acc) + 1)\n",
    "\n",
    "plt.figure( figsize=(8,4), dpi=100 )\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 755
    },
    "colab_type": "code",
    "id": "qnSTby4DEY6H",
    "outputId": "630bcbce-a9d0-40f6-f969-d96607e64403"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dropout\n",
    "\n",
    "model = Sequential([Conv2D(64, (3,3), activation='relu', input_shape=(28,28,1)),\n",
    "                    MaxPooling2D(2,2),\n",
    "                    Conv2D(64, (3,3), activation='relu'),\n",
    "                    MaxPooling2D(2,2),\n",
    "                    Flatten(),\n",
    "                    Dropout(0.5),\n",
    "                    Dense(128, activation='relu'),\n",
    "                    Dense(10, activation='softmax') ])\n",
    "\n",
    "model.compile( loss='sparse_categorical_crossentropy',\n",
    "               optimizer='adam',\n",
    "               metrics=['accuracy'] )\n",
    "\n",
    "history = model.fit( x_train,\n",
    "                     y_train,\n",
    "                     epochs=20, \n",
    "                     validation_data=(x_valid, y_valid) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 385
    },
    "colab_type": "code",
    "id": "mU6_w3fSIYY-",
    "outputId": "10fa9234-18b2-4229-caa2-c00174c2a664"
   },
   "outputs": [],
   "source": [
    "acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs = range(1, len(acc) + 1)\n",
    "\n",
    "plt.figure( figsize=(8,4), dpi=100 )\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 自定义 评估函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomModel(keras.Model):\n",
    "    def test_step(self, data):\n",
    "        # Unpack the data\n",
    "        x, y = data\n",
    "        # Compute predictions\n",
    "        y_pred = self(x, training=False)\n",
    "        # Updates the metrics tracking the loss\n",
    "        self.compiled_loss(y, y_pred, regularization_losses=self.losses)\n",
    "        # Update the metrics.\n",
    "        self.compiled_metrics.update_state(y, y_pred)\n",
    "        # Return a dict mapping metric names to current value.\n",
    "        # Note that it will include the loss (tracked in self.metrics).\n",
    "        return {m.name: m.result() for m in self.metrics}\n",
    "\n",
    "\n",
    "# Construct an instance of CustomModel\n",
    "inputs = keras.Input(shape=(32,))\n",
    "outputs = keras.layers.Dense(1)(inputs)\n",
    "model = CustomModel(inputs, outputs)\n",
    "model.compile(loss=\"mse\", metrics=[\"mae\"])\n",
    "\n",
    "# Evaluate with our custom test_step\n",
    "x = np.random.random((1000, 32))\n",
    "y = np.random.random((1000, 1))\n",
    "model.evaluate(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nAeD73T7Tn_6"
   },
   "source": [
    "# 超参数调优 Keras Tuner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ref**\n",
    "\n",
    "[1] https://zhuanlan.zhihu.com/p/156139224\n",
    "\n",
    "[2] https://www.tensorflow.org/tutorials/keras/keras_tuner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 例子1 使用序列建模"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "WMAB0HIlIcHz",
    "outputId": "c4973189-de2b-4f02-dbd8-5137f0225fd9"
   },
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "# from kerastuner.tuners import RandomSearch\n",
    "\n",
    "import keras_tuner as kt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print( x_train.shape )\n",
    "print( x_test.shape )\n",
    "print( y_train.shape )\n",
    "print( y_test.shape )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(hp):\n",
    "    model = keras.Sequential()\n",
    "    model.add( Flatten(input_shape=[28,28]) )\n",
    "    model.add(layers.Dense(units=hp.Int('units',\n",
    "                                        min_value=32,\n",
    "                                        max_value=512,\n",
    "                                        step=32),\n",
    "                           activation='relu')) \n",
    "    model.add(layers.Dense(10, activation='softmax')) \n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(\n",
    "            hp.Choice('learning_rate',\n",
    "                      values=[1e-2, 1e-3, 1e-4])),\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner = kt.Hyperband(\n",
    "    build_model,\n",
    "    objective='val_accuracy', #优化目标\n",
    "    max_epochs=20, \n",
    "    directory='E:\\python package\\python-project\\DeepLearningApp\\LearnKeras\\logs',\n",
    "    project_name='image_classify')\n",
    "\n",
    "# directory 要使用绝对路径, 否则报 UnicodeDecodeError 真TM坑"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner.results_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)\n",
    "\n",
    "tuner.search(x_train,\n",
    "             y_train, \n",
    "             epochs=5, \n",
    "             validation_split=0.2, callbacks=[stop_early]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 例子2 使用函数建模"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def build_model2(hp):\n",
    "\n",
    "    inputs = Input(shape=[28,28])\n",
    "    x = Flatten(input_shape=[28,28])(inputs)\n",
    "    x = Dense(units=hp.Int('units',\n",
    "                            min_value=50,\n",
    "                            max_value=512,\n",
    "                            step=50), activation='relu')(x)\n",
    "    output = Dense(10, activation='softmax')(x)\n",
    "\n",
    "    model = Model( inputs=[inputs], outputs=[output] )\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(\n",
    "            hp.Choice('learning_rate',\n",
    "                      values=[1e-2, 1e-3, 1e-4])),\n",
    "        \n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "tuner = kt.Hyperband(\n",
    "    build_model2,\n",
    "    objective='val_accuracy', #优化目标\n",
    "    max_epochs=10, \n",
    "    directory='E:\\python package\\python-project\\DeepLearningApp\\LearnKeras\\logs', # directory 要使用绝对路径, 否则报 UnicodeDecodeError \n",
    "    project_name='imageclassify') # 每次运行之前, 将 imageclassify 目录删除"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)\n",
    "\n",
    "tuner.search(x_train,\n",
    "             y_train, \n",
    "             epochs=10, \n",
    "             validation_split=0.2, callbacks=[stop_early]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner.results_summary(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = tuner.get_best_models(num_models=1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner.get_best_hyperparameters()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model.evaluate( x_test, y_test )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basic_model.evaluate( x_test, y_test )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basic_model.summary()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Keras Basics",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "242.8px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
